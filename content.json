[{"title":"11 正则解析html","date":"2019-10-27T12:10:00.000Z","path":"2019/10/27/11-正则解析html/","text":"简介：正则解析html例子（待补充） 使用正则，比使用其他模块的优点,对一段html 能够提取多个指定的数据，而非定位单个数据或多个相同类型的数据,使用beautfulsoup和xpath 定位标签或指定id或类的数据更加的简单。 使用正则常使用的方法compile(匹配的规则)，正则常使用.?，和（.\\?） 分组，结尾的re.S 添加后能识别\\n 换行符。 12345678910111213141516# -*- coding:utf-8 -*-import rehtml = &apos;&apos;&apos;&lt;div class=&quot;ProductDetails&quot;&gt; &lt;span class=&quot;ProductName&quot;&gt;&lt;a href=&quot;http://www.photoequipmentstore.com.au/pixel-dl-913-video-led-light-x2-kit/&quot;&gt;Pixel DL-913 Video Led Light X2 Kit&lt;/a&gt;&lt;/span&gt; &lt;span class=&quot;ProductPrice&quot;&gt;&lt;strike class=&quot;RetailPriceValue&quot;&gt;$575.00&lt;/strike&gt; $445.00&lt;/span&gt;&apos;&apos;&apos;pattern = re.compile(&apos;.*?ProductName.*?/&quot;&gt;(.*?)&lt;/a&gt;&apos;+&apos;.*?ProductPrice.*?&lt;/strike&gt;(.*?)&lt;/span&gt;&apos;,re.S)items = re.findall(pattern, html)for item in items: print(&#123; &apos;ProductName&apos;: item[0], &apos;ProductPrice&apos;: item[1] &#125;) 1&#123;&apos;ProductName&apos;: &apos;Pixel DL-913 Video Led Light X2 Kit&apos;, &apos;ProductPrice&apos;: &apos; $445.00&apos;&#125;","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"13 Scrapy 爬取知乎用户信息","date":"2019-10-27T12:07:00.000Z","path":"2019/10/27/13-Scrapy-爬取知乎用户信息/","text":"简介：13 crapy 爬取知乎用户信息 思路 选定起始人：选定一位关注数和粉丝数多的大V为爬取起始点 获取粉丝列表和关注列表：通过知乎接口获得大V的粉丝列表和关注列表 获取列表用户信息：通过知乎接口获得列表中每位用户的详细信息 获取每位用户粉丝和关注 123456一个知乎的站点： https://www.zhihu.com/people/mu-rong-chen-xi/activities选择 更多》关注 查看该用户的他关注的人在谷歌浏览器上打开开发者工具，选择Network,刷新页面，在左下侧的网页Name 进行排序（方便）网络连接 有这个一个地址 1https://www.zhihu.com/api/v4/members/mu-rong-chen-xi/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20 12345打开Headers ，通过地址可以看出这是一个api，是一个接口:path: /api/v4/members/mu-rong-chen-xi/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20Request URL: https://www.zhihu.com/api/v4/members/mu-rong-chen-xi/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20 1234567891011121314151617181920212223242526272829303132333435363738394041打开Prview (返回数据)，显示是一个json 结构数据一层：data: [&#123;id: &quot;64feacb96c62f7b9e62dfd3a941ba00a&quot;, url_token: &quot;quartz&quot;, name: &quot;Quartz&quot;,…&#125;,…]paging: &#123;is_end: false, is_start: true,…&#125;二层： 打开datadata: [&#123;id: &quot;64feacb96c62f7b9e62dfd3a941ba00a&quot;, url_token: &quot;quartz&quot;, name: &quot;Quartz&quot;,…&#125;,…]0: &#123;id: &quot;64feacb96c62f7b9e62dfd3a941ba00a&quot;, url_token: &quot;quartz&quot;, name: &quot;Quartz&quot;,…&#125;1: &#123;id: &quot;ce959f6243a1120e281ebba923faedb1&quot;, url_token: &quot;chen-lei-93-78&quot;, name: &quot;陈磊&quot;,…&#125;.....三层： 打开data下的 0 数据 0: &#123;id: &quot;64feacb96c62f7b9e62dfd3a941ba00a&quot;, url_token: &quot;quartz&quot;, name: &quot;Quartz&quot;,…&#125;answer_count: 528articles_count: 116avatar_url: &quot;https://pic1.zhimg.com/v2-77899d61448ff747c292f2de6453cfa6_is.jpg&quot;avatar_url_template: &quot;https://pic1.zhimg.com/v2-77899d61448ff747c292f2de6453cfa6_&#123;size&#125;.jpg&quot;badge: []follower_count: 119161gender: 1headline: &quot;公众号是：人界爆款&quot; # 关注者副标题id: &quot;64feacb96c62f7b9e62dfd3a941ba00a&quot;is_advertiser: falseis_followed: falseis_following: falseis_org: falsename: &quot;Quartz&quot; # 关注者名称type: &quot;people&quot;url: &quot;https://www.zhihu.com/people/quartz&quot; # 关注者知乎地址url_token: &quot;quartz&quot; # 标识每个用户，use_default_avatar: falseuser_type: &quot;people&quot;vip_info: &#123;is_vip: true, rename_days: &quot;60&quot;,…&#125;上面的用户民Quartz,其url_token: quartz如果是中文用户： 陈磊，url 中标识变成拼音加数字url: &quot;https://www.zhihu.com/people/chen-lei-93-78&quot;url_token: &quot;chen-lei-93-78&quot; 项目创建项目和蜘蛛123scrapy startproject zhihuusercd zhihuuser scrapy genspider zhihu www.zhihu.com 蜘蛛文件 zhihuuser/zhihuuser/spiders/zhihu.py 1scrapy crawl zhihu 修改默认的配置文件seettings.py 1234567891011121314# Obey robots.txt rules # robots.txt 存在服务器端，告诉爬虫用户那些能爬，那些不能爬# ROBOTSTXT_OBEY = True # 将True 改为Flase ROBOTSTXT_OBEY = False # 将user-agent 添加到request 的头部去,#user-agent 默认没有写在request-head中， 是我从自己浏览器访问时，从request 中获取的# Override the default request headers: DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'&#125; 第一轮测试 : 初步使用12345678910111213141516# -*- coding: utf-8 -*-import scrapyfrom scrapy import Spider, Requestclass ZhihuSpider(scrapy.Spider): name = 'zhihu' allowed_domains = ['www.zhihu.com'] start_urls = ['http://www.zhihu.com/'] def start_requests(self): url = 'https://www.zhihu.com/api/v4/members/mu-rong-chen-xi/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20' yield Request(url,callback=self.parse) def parse(self, response): print(response.text) 1scrapy crawl zhihu 结果爬出用户信息，URL是Head的Request URL 进行优化,分析URL 123456789101112关注者URL的分析Head 下 Query String Parametesview source 版本include: data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topicsoffset: 0limit: 20view parsed 版本 ，该参数就是Request URl的后半部分include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20Head 下 GenealRequest URL: https://www.zhihu.com/api/v4/members/mu-rong-chen-xi/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset=0&amp;limit=20 12345678910当前用户的分析，每次鼠标放在关注着图标上，后台返回一个链接,该链接是用户链接地址https://www.zhihu.com/api/v4/members/chen-lei-93-78?include=allow_message%2Cis_followed%2Cis_following%2Cis_org%2Cis_blocking%2Cemployments%2Canswer_count%2Cfollower_count%2Carticles_count%2Cgender%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topicshead:Request URL: https://www.zhihu.com/api/v4/members/chen-lei-93-78?include=allow_message%2Cis_followed%2Cis_following%2Cis_org%2Cis_blocking%2Cemployments%2Canswer_count%2Cfollower_count%2Carticles_count%2Cgender%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topicsHead 下 Query String Parametes 其请求参数include: allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics 将URL 进行拆分 1234567891011user = &apos;mu-rong-chen-xi&apos;offset = 0limit = 20# 用户的url 和请求参数user_url = &apos;https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;&apos;user_query = &apos;allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics&apos; # 关注着url和请求参数follows_url = &apos;https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;office&#125;&amp;limit=&#123;limit&#125;&apos;follows_query=&apos;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&apos; 第二轮测试 ： 将URL 进行拆分12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-import scrapyfrom scrapy import Spider, Requestclass ZhihuSpider(scrapy.Spider): name = 'zhihu' allowed_domains = ['www.zhihu.com'] start_urls = ['http://www.zhihu.com/'] start_user = 'mu-rong-chen-xi' offset = 0 limit = 20 # 用户的URL和请求参数 user_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;' user_query = 'allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics' # 关注该用户的URL和请求参数 follows_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' follows_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' # def start_requests(self): yield Request(self.user_url.format(user=self.start_user, include=self.user_query), self.parse_user) yield Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=self.limit, offset=self.offset), callback=self.parse_followers) def parse_user(self, response): print(response.text) def parse_followers(self, response): print(response.text) def parse(self, response): print(response.text) 请求的URL 12342019-10-24 10:22:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.zhihu.com/api/v4/members/mu-rong-chen-xi?include=allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics&gt; (referer: None)2019-10-24 10:22:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.zhihu.com/api/v4/members/mu-rong-chen-xi/followees?include=data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&amp;offset=0&amp;limit=20&gt; (referer: None) 用户的返回信息 1234567891011121314151617181920212223242526272829303132333435363738394041&#123;&quot;id&quot;:&quot;dd851a5ba9de70dcf4ce6b9604fd01c2&quot;,&quot;url_token&quot;:&quot;mu-rong-chen-xi&quot;,&quot;name&quot;:&quot;狐狸晨曦&quot;,&quot;use_default_avatar&quot;:false,&quot;avatar_url&quot;:&quot;https://pic1.zhimg.com/v2-47a4f7a0f78f4e2b4abfc456d6090346_is.jpg&quot;,&quot;avatar_url_template&quot;:&quot;https://pic1.zhimg.com/v2-47a4f7a0f78f4e2b4abfc456d6090346_&#123;size&#125;.jpg&quot;,&quot;is_org&quot;:false,&quot;type&quot;:&quot;people&quot;,&quot;url&quot;:&quot;https://www.zhihu.com/people/mu-rong-chen-xi&quot;,&quot;user_type&quot;:&quot;people&quot;,&quot;headline&quot;:&quot;历史/影视撰稿人。公众号：狐言论史（huyanls1012）&quot;,&quot;gender&quot;:1,&quot;is_advertiser&quot;:false,&quot;vip_info&quot;:&#123; &quot;is_vip&quot;:true, &quot;rename_days&quot;:&quot;60&quot;, &quot;vip_icon&quot;:&#123;&quot;url&quot;:&quot;https://pic3.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a_r.png&quot;, &quot;night_mode_url&quot;:&quot;https://pic3.zhimg.com/v2-c9686ff064ea3579730756ac6c289978_r.png&quot;&#125;&#125;, &quot;badge&quot;:[], &quot;allow_message&quot;:true, &quot;is_following&quot;:false, &quot;is_followed&quot;:false, &quot;is_blocking&quot;:false, &quot;follower_count&quot;:99674, &quot;answer_count&quot;:859, &quot;articles_count&quot;:150, &quot;employments&quot;:[&#123; &quot;job&quot;:&#123; &quot;id&quot;:&quot;&quot;, &quot;type&quot;:&quot;topic&quot;, &quot;url&quot;:&quot;&quot;, &quot;name&quot;:&quot;文学影视评论&quot;, &quot;avatar_url&quot;:&quot;https://pic4.zhimg.com/e82bab09c_is.jpg&quot;&#125;, &quot;company&quot;:&#123; &quot;id&quot;:&quot;19586269&quot;, &quot;type&quot;:&quot;topic&quot;, &quot;url&quot;:&quot;https://www.zhihu.com/topics/19586269&quot;, &quot;name&quot;:&quot;中国古代历史&quot;, &quot;avatar_url&quot;:&quot;https://pic2.zhimg.com/3ceadc585_is.jpg&quot;&#125;&#125;]&#125; 关注者信息，西面是2个用户的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&#123; &quot;paging&quot;: &#123; &quot;is_end&quot;: false, &quot;is_start&quot;: true, &quot;next&quot;: &quot;https://www.zhihu.com/members/mu-rong-chen-xi/followees?include=data%5B%2A%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F%28typ e % 3 Dbest_answerer % 29 % 5 D.topics\\ u0026limit = 20\\ u0026offset = 20 &quot;, &quot;previous&quot;: &quot; https: //www.zhihu.com/members/mu-rong-chen-xi/followees?include=data%5B%2A%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_follow ing % 2 Cbadge % 5 B % 3 F % 28 type % 3 Dbest_answerer % 29 % 5 D.topics\\ u0026limit = 20\\ u0026offset = 0 &quot;, &quot;totals &quot;:265 &#125;, &quot;data &quot;:[ &#123; &quot;id&quot; : &quot;64feacb96c62f7b9e62dfd3a941ba00a&quot;, &quot;url_token&quot; : &quot;quartz&quot;, &quot;name&quot; : &quot;Quartz&quot;, &quot;use_default_avatar&quot;:false, &quot;avatar_url&quot; : &quot;https://pic1.zhimg.com/v2-77899d61448ff747c292f2de6453cfa6_is.jpg&quot;, &quot;avatar_url_template&quot; : &quot;https://pic1.zhimg.com/v2-77899d61448ff747c292f2de6453cfa6_&#123;size&#125;.jpg&quot;, &quot;is_org&quot;:false, &quot;type&quot;:&quot;people&quot;, &quot;url&quot;:&quot;https://www.zhihu.com/people/quartz&quot;, &quot;user_type &quot;:&quot;people &quot;, &quot;headline &quot;:&quot;公众号是： 人界爆款 &quot;, &quot;gender &quot;:1, &quot;is_advertiser &quot;:false, &quot;vip_info &quot;:&#123; &quot;is_vip &quot;:true, &quot;rename_days &quot;:&quot;60 &quot;, &quot;vip_icon &quot;:&#123; &quot;url &quot;:&quot;https://pic3.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a_r.png&quot;, &quot;night_mode_url&quot;:&quot;https: //pic3.zhimg.com/v2-c9686ff064ea3579730756ac6c289978_r.png&quot; &#125; &#125;, &quot;badge&quot;:[], &quot;is_following&quot;:false, &quot;is_followed&quot;:false, &quot;follower_count&quot;:119181, &quot;answer_count&quot;:528, &quot;articles_count&quot;:116&#125;, &#123; &quot;id&quot;:&quot;ce959f6243a1120e281ebba923faedb1&quot;, &quot;url_token &quot;:&quot;chen-lei-93-78 &quot;, &quot;name &quot;:&quot;陈磊 &quot;, &quot;use_default_avatar &quot;:false, &quot;avatar_url &quot;:&quot;https: //pic2.zhimg.com/v2-87df13062314f311f15b5eab38f4b40b_is.jpg&quot;, &quot;avatar_url_template&quot;:&quot;https://pic2.zhimg.com/v2-87df13062314f311f15b5eab38f4b40b_&#123;size&#125;.jpg &quot;, &quot;is_org &quot;:false, &quot;type &quot;:&quot;people &quot;, &quot;url &quot;:&quot;https: //www.zhihu.com/people/chen-lei-93-78&quot;, &quot;user_type&quot;:&quot;people&quot;, &quot;headline&quot;:&quot;君子惠而不费、劳而不怨、欲而不贪、泰而不骄、威而不猛。&quot;, &quot;gender&quot;:1, &quot;is_advertiser&quot;:false, &quot;vip_info&quot;:&#123; &quot;is_vip&quot;:false, &quot;rename_days&quot;: &quot;60&quot; &#125;, &quot;badge&quot;: [], &quot;is_following&quot;: false, &quot;is_followed&quot;: false, &quot;follower_count&quot;: 25688, &quot;answer_count&quot;: 916, &quot;articles_count&quot;: 12&#125;, ......]&#125; 使用itemItem是爬取到的数据的容器，在这里我们可以将自己爬取到的内容定义不同的字段名。再通过parse的方法解析并读取出来放到这里 items.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445from scrapy import Item, Fieldclass UserItem(Item): # define the fields for your item here like: # Field() 是 scrapy.item下的类 id = Field() name = Field() avatar_url = Field() headline = Field() description = Field() url = Field() url_token = Field() gender = Field() cover_url = Field() type = Field() badge = Field() answer_count = Field() articles_count = Field() commercial_question_count = Field() favorite_count = Field() favorited_count = Field() follower_count = Field() following_columns_count = Field() following_count = Field() pins_count = Field() question_count = Field() thank_from_count = Field() thank_to_count = Field() thanked_count = Field() vote_from_count = Field() vote_to_count = Field() voteup_count = Field() following_favlists_count = Field() following_question_count = Field() following_topic_count = Field() marked_answers_count = Field() mutual_followees_count = Field() hosted_live_count = Field() participated_live_count = Field() locations = Field() educations = Field() employments = Field() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# -*- coding: utf-8 -*-import scrapyimport jsonfrom scrapy import Spider, Request# 导入UserItem类from zhihuuser.items import UserItem# 要进行分页，所以讲offset设置为全局变量offset = 0class ZhihuSpider(scrapy.Spider): name = 'zhihu' allowed_domains = ['www.zhihu.com'] start_urls = ['http://www.zhihu.com/'] start_user = 'mu-rong-chen-xi' user_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;' user_query = 'allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics' follows_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' follows_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' def start_requests(self): yield Request(self.user_url.format(user=self.start_user, include=self.user_query), self.parse_user) yield Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=20, offset=offset), callback=self.parse_follows) def parse_user(self, response): \"\"\" :param response: :return: # 对单个知乎用户的url请求处理数据，进行反序列化 # 如果是项目内的字段，则进行赋值 \"\"\" result = json.loads(response.text) item = UserItem() for field in item.fields: # field是result 的键名之一，对item 进行赋值 if field in result.keys(): item[field] = result.get(field) yield item def parse_follows(self, response): \"\"\" :param response: :return: # rsult_v 是一个具体的关注者用户，是包含在data 内的数据 # 通过获取具体的关注者的信息，提取url_token字段，使用user_url 进行格式化生成单个用户的url，再使用Request 进行请求 # paging 数据代表分页数据，提取数据， 判断下一页的url 地址（next）， # is_end = True 代表已经到了结尾 # 如果有下一页数据，进行请求，获取下一页的关注者信息 \"\"\" results = json.loads(response.text) global offset # 有data 键名说明是关注者数据 if 'data' in results.keys(): for result_v in results.get('data'): yield Request(self.user_url.format(user=result_v.get('url_token'),include=self.user_query),self.parse_user) # 进行分页分页不成功，next 连接无法访问，使用见offset 加20的方法 # if 'paging' in results.keys() and results.get('paging').get('is_end') == False: # next_page = results.get('paging').get('next') # print(\"next_page:\"+next_page) # yield Request(next_page,self.parse_follows) if 'paging' in results.keys() and results.get('paging').get('is_end') == False: offset += 20 yield Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=20, offset=offset), callback=self.parse_follows) def parse(self, response): print(response.text) 使用pipiine 保存数据到MongoDBpipelines.py 下面是官网提供链接数据的方法 123456789101112131415161718192021222324252627282930import pymongoclass MongoPipeline(object): collection_name = 'users' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): # item 是parse_user 的 return返回结果，是 &lt;class 'zhihuuser.items.UserItem'&gt;, 类似字典的形式 # 定义操作， self.collection_name ： 表名， # True 是进行去重，前面是条件&#123;'url_token': item['url_token']&#125;，没有则插入，有则更新 # dict(item) 是插入的值 self.db[self.collection_name].update(&#123;'url_token': item['url_token']&#125;, dict(item), True) return item settings.py 12345678# 激活项目管道 ，MongoPipeline 是自己定义类的名称ITEM_PIPELINES = &#123; 'zhihuuser.pipelines.MongoPipeline': 300, &#125;MONGO_URI = 'localhost'MONGO_DATABASE = 'zhihu'","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":" 12 scrapy 框架基础知识","date":"2019-10-27T11:59:00.000Z","path":"2019/10/27/12-scrapy-框架基础知识/","text":"简介：scrapy 框架基础知识 12.1 scrapy框架简介12.1.1 介绍Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。 整体架构大致如下： 123456789101112131415161718192021222324252627Components：1、引擎(EGINE)引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。2、调度器(SCHEDULER)用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址3、下载器(DOWLOADER)用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的4、爬虫(SPIDERS)SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求5、项目管道(ITEM PIPLINES)在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作下载器中间件(Downloader Middlewares)位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，你可用该中间件做以下几件事： (1) process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website); (2) change received response before passing it to a spider; (3) send a new Request instead of passing received response to a spider; (4) pass response to a spider without fetching a web page; (5) silently drop some requests.6、爬虫中间件(Spider Middlewares)位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests） 执行顺序： 123456781. Spider的yield将requests发送给Engine1. Engine对requests不做任何的处理就发送给Scheduler1. Scheduler（url调度器），生成requests交给Engine1. Engine拿到requests，通过middleware进行层层过滤发送给Downloader1. downloader在网上获取到response数据之后，又经过middleware进行层层过滤发送给Engine1. Engine获取到response之后，返回给Spider，Spider的parse（）方法对获取到的response数据进行处理解析出items或者requests1. 将解析出来的items或者requests发送给Engine1. Engine获取到items或者requests，将items发送给ITEMPIPELINES，将requests发送给Scheduler 官网链接 12.1.2 安装Twisted是用Python实现的基于事件驱动的网络引擎框架，Twisted支持许多常见的传输及应用层协议，包括TCP、UDP、SSL/TLS、HTTP、IMAP、SSH、IRC以及FTP。安装scrapy 之前必须安装这个框架。 安装Twisted 注意: cp 后面跟的是python的版本，cp36 代表python3.6 注意系统版本，36位还是64位 Pywin32是一个Python库，为python提供访问Windows API的扩展，提供了齐全的windows常量、接口、线程以及COM机制等等。 123456789101112#Windows平台 1、pip3 install wheel #安装后，便支持通过wheel文件安装软件， #wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs 3、pip3 install lxml 4、pip3 install pyopenssl 5、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/ 6、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 7、执行pip3 install 下载目录\\Twisted-17.9.0-cp36-cp36m-win_amd64.whl 8、pip3 install scrapy #Linux平台 1、pip3 install scrapy 12.1.3 命令行工具 查看帮助12scrapy -h # 查看所有可用命令scrapy &lt;command&gt; -h 123456789101112131415161718192021# 有两种命令：其中Project-only必须切到项目文件夹下才能执行，而Global的命令则不需要 Global commands: startproject #创建项目 genspider #创建爬虫程序 settings #如果是在项目目录下，则得到的是该项目的配置 runspider #运行一个独立的python文件，不必创建项目 shell #scrapy shell url地址 在交互式调试，如选择器规则正确与否 fetch #独立于程单纯地爬取一个页面，可以拿到请求头 view #下载完毕后直接弹出浏览器，以此可以分辨出哪些数据是ajax请求 version #scrapy version 查看scrapy的版本，scrapy version -v查看scrapy依赖库的版本 Project-only commands: crawl #运行爬虫，必须创建项目才行，确保配置文件中ROBOTSTXT_OBEY = False check #检测项目中有无语法错误 list #列出项目中所包含的爬虫名 edit #编辑器，一般不用 parse #scrapy parse url地址 --callback 回调函数 #以此可以验证我们的回调函数是否正确 bench #scrapy bentch压力测试# 官网链接 https://docs.scrapy.org/en/latest/topics/commands.html startproject : 创建项目 1234语法 ： scrapy startproject &lt;project_name&gt; [project_dir]需要项目：否说明： roject_name在project_dir 目录下创建一个名为的新Scrapy项目。如果project_dir未指定，project_dir则与相同project_name 示例 1scrapy startproject myproject genspider ：创建蜘蛛123456语法: scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;需要项目：是spiders如果从项目内部调用，则在当前文件夹或当前项目的文件夹中创建一个新的蜘蛛。&lt;name&gt;参数设置为蜘蛛的name&lt;domain&gt;用于生成allowed_domains和start_urls蜘蛛的属性 123456789101112$ scrapy genspider -l # 创建时生成的模板Available templates: basic crawl csvfeed xmlfeed$ scrapy genspider example example.com # 创建蜘蛛Created spider &apos;example&apos; using template &apos;basic&apos;$ scrapy genspider -t crawl scrapyorg scrapy.org # 使用特定的模板Created spider &apos;scrapyorg&apos; using template &apos;crawl&apos; crawl : 起动蜘蛛爬网12语法: scrapy crawl &lt;spider&gt;需要项目：是 123$ scrapy crawl myspider$ scrapy crawl example # 上面创建爬虫名为example check : 检查蜘蛛的完整性1234语法： scrapy check [-l] &lt;spider&gt;需要项目：是检查爬虫是否有错误，检查一些语法、import和warning等错误 1scrapy check list : 显示蜘蛛名称12语法： scrapy list需要项目：是 123$ scrapy listexamplequotes edit : 编辑指定蜘蛛12345语法： scrapy edit &lt;spider&gt;需要项目：是使用EDITOR环境变量或（如果未设置）定义的编辑器编辑给定的蜘蛛EDITOR。windows上未设置相关变量，所有不能执行 1$ scrapy edit spider1 fetch : 下载网页12345678910语法： scrapy fetch &lt;url&gt;需要项目：否使用Scrapy下载器下载给定的URL，并将内容写入标准输出。支持的选项： --nolog: 不打印日志 --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛 --headers：打印响应的HTTP标头，而不是响应的正文 --no-redirect：不遵循HTTP 3xx重定向（默认为遵循它们） 1scrapy fetch --nolog --headers &quot;http://www.baidu.com&quot; view : 请求一个url,Document下载下来，在本地保存文件，并在浏览器中打开123456语法： scrapy view &lt;url&gt;需要项目：否支持的选项： --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛 --no-redirect：不遵循HTTP 3xx重定向（默认为遵循它们） 1234567scrapy view &quot;https://www.bilibili.com/&quot;# 访问B站成功，并下载B站的主页面到本地，并在浏览器中打开# file:///C:/Users/wutai/AppData/Local/Temp/tmpgcmddg5j.html# 网页显示不完全，是由于一部分数据是使用ajax 进行加载的，是看不出完全的效果的scrapy view &quot;https://www.baidu.com/&quot;# 打开百度失败，从robots.txt 上禁止 &lt;GET https://www.baidu.com&gt; shell : 命令行交互模式12345678910语法： scrapy shell [url]需要项目：否为给定的URL（如果给定）启动Scrapy shell，如果没有给定的URL，则为空。支持的选项： --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛 -c code：评估shell中的代码，打印结果并退出 --no-redirect：不遵循HTTP 3xx重定向（默认为遵循重定向）；这只会影响您可能在命令行中作为参数传递的URL； 一旦进入shell，fetch(url)默认情况下仍将遵循HTTP重定向。 1$ scrapy shell http://www.example.com/some/page.html shell 使用见官网 1https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell parse : 解析url123456789101112131415161718语法： scrapy parse &lt;url&gt; [options]需要项目：是使用随--callback选项传递的方法（parse如果未提供），获取给定的URL并与处理它的蜘蛛解析。支持的选项： --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛 --a NAME=VALUE：设置蜘蛛参数（可以重复） --callback或-c：蜘蛛方法用作解析响应的回调 --meta或-m：将传递给回调请求的其他请求元。这必须是有效的json字符串。例如：–meta =&apos;&#123;“ foo”：“ bar”&#125;&apos; --cbkwargs：将传递给回调的其他关键字参数。这必须是有效的json字符串。示例：–cbkwargs =&apos;&#123;“ foo”：“ bar”&#125;&apos; --pipelines：通过管道处理项目 --rules或-r：使用CrawlSpider 规则发现用于解析响应的回调（即蜘蛛方法） --noitems：不显示刮擦的物品 --nolinks：不显示提取的链接 --nocolour：避免使用pygments给输出着色 --depth或-d：应递归遵循请求的深度级别（默认值：1） --verbose或-v：显示每个深度级别的信息 1$ scrapy parse http://www.example.com/ -c parse_item settings : 获取Scrapy设置的值。12语法： scrapy settings [options]需要项目：否 1234$ scrapy settings --get BOT_NAMEscrapybot$ scrapy settings --get DOWNLOAD_DELAY0 runspider : 无需创建项目即可运行包含在Python文件中的蜘蛛程序12句法： scrapy runspider &lt;spider_file.py&gt;需要项目：否 12$ scrapy runspider myspider.py[ ... spider starts crawling ... ] version : 查看scrapy 版本 123句法： scrapy version [-v]需要项目：否打印Scrapy版本。如果与-v它一起使用，还可以打印Python，Twisted和Platform信息，这对于错误报告很有用。 bench ：运行快速基准测试 12语法： scrapy bench需要项目：否 12.1.4 创建、启动项目 创建项目 1scrapy startproject quotetutorial # 初始化创建quotetutorial 项目，会在该目录下自动创建目录quotetutorial； 进入项目后，创建一个新的蜘蛛 12345cd quotetutorial scrapy genspider quotes quotes.toscrape.com# 会自动创建一个爬虫项目文件quotes.py，quotes 是别名，别名不能与项目名相同；# 文件目录 quotetutorial/quotetutorial/spiders/quotes.py， # genspider 后面第一个个是别名，第二个是域名 启动蜘蛛爬行 12scrapy crawl quotes # 启动quotes.py项目 ，抓取目标网站数据，默认没有进行网页处理，所以速度很快 生成的quotes.py 12345678910import scrapyclass QuotesSpider(scrapy.Spider): name = &apos;quotes&apos; allowed_domains = [&apos;quotes.toscrape.com&apos;] start_urls = [&apos;http://quotes.toscrape.com/&apos;] def parse(self, response): pass 12.1.5 目录结构创建项目后，内部有一个同项目名称的文件夹和.cfg 文件 123456789101112project_name/ scrapy.cfg project_name/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py 爬虫1.py 爬虫2.py 爬虫3.py 文件说明：scrapy.cfg 项目的主配置信息，用来部署scrapy时使用，爬虫相关的配置信息在settings.py文件中。items.py 设置数据存储模板，用于结构化数据，如：Django的Modelpipelines 数据处理行为，如：一般结构化的数据持久化settings.py 配置文件，如：递归的层数、并发数，延迟下载等。强调:配置文件的选项必须大写否则视为无效，正确写法USER_AGENT=’xxxx’spiders 爬虫目录，如：创建文件，编写爬虫规则 注意：1、一般创建爬虫文件时，以网站域名命名 2、默认只能在终端执行命令，为了更便捷操作： 1234#在项目根目录下新建：entrypoint.pyfrom scrapy.cmdline import executeexecute([&apos;scrapy&apos;, &apos;crawl&apos;, &apos;xiaohua&apos;])框架基础：spider类，选择器， 12.2 Spider类Spiders是定义如何抓取某个站点（或一组站点）的类，包括如何执行爬行（即跟随链接）以及如何从其页面中提取结构化数据（即抓取项目）。换句话说，Spiders是您为特定站点（或者在某些情况下，一组站点）爬网和解析页面定义自定义行为的地方。 1234567891011121314151617181、 生成初始的Requests来爬取第一个URLS，并且标识一个回调函数 第一个请求定义在start_requests()方法内默认从start_urls列表中获得url地址来生成Request请求， 默认的回调函数是parse方法。回调函数在下载完成返回response时自动触发2、 在回调函数中，解析response并且返回值 返回值可以4种： 包含解析数据的字典 Item对象 新的Request对象（新的Requests也需要指定一个回调函数） 或者是可迭代对象（包含Items或Request）3、在回调函数中解析页面内容 通常使用Scrapy自带的Selectors，但很明显你也可以使用Beutifulsoup，lxml或其他你爱用啥用啥。4、最后，针对返回的Items对象将会被持久化到数据库 通过Item Pipeline组件存到数据库：https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline） 或者导出到不同的文件（通过Feed exports：https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports） 1234567891011121314class QuotesSpider(scrapy.Spider): name = &apos;quotes&apos; allowed_domains = [&apos;quotes.toscrape.com&apos;] start_urls = [&apos;http://quotes.toscrape.com/&apos;] custom_settings = &#123; # 覆盖全局配置 DEFAULT_REQUEST_HEADERS : &#123; &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;, &apos;Accept-Language&apos;: &apos;en&apos;, &#125; &#125; def parse(self, response): pass 1234567891011scrapy.spiders.Spider name : 用于定义此蜘蛛的名称 allowed_domains : 包含此蜘蛛可以爬网的域的字符串的可选列表。 如果OffsiteMiddleware启用，则不遵循对不属于此列表中指定的域名（或其子域）的URL的请求 。 start_urls : 起始url 列表，拿到每个链接，进行get请求 custom_settings ： 特定的字典配置，会覆盖项目的配置settings.py crawler ： 封装了许多组件以供其单项访问（例如扩展，中间件，信号管理器等） settings ： 控制核心插件、组件等配置 from_crawler ： 这是Scrapy用于创建蜘蛛的类方法。 start_requests : 此方法必须返回带有第一个Request的Iterable，以对该蜘蛛进行爬网。 .... 如果您需要通过使用POST请求登录来开始，则可以执行以下操作： 123456789101112class MySpider(scrapy.Spider): name = &apos;myspider&apos; def start_requests(self): return [scrapy.FormRequest(&quot;http://www.example.com/login&quot;, formdata=&#123;&apos;user&apos;: &apos;john&apos;, &apos;pass&apos;: &apos;secret&apos;&#125;, callback=self.logged_in)] def logged_in(self, response): # here you would extract links to follow and return Requests for # each of them, with another callback pass 从单个回调返回多个请求和项目： 1234567891011121314151617import scrapyclass MySpider(scrapy.Spider): name = &apos;example.com&apos; allowed_domains = [&apos;example.com&apos;] start_urls = [ &apos;http://www.example.com/1.html&apos;, &apos;http://www.example.com/2.html&apos;, &apos;http://www.example.com/3.html&apos;, ] def parse(self, response): for h3 in response.xpath(&apos;//h3&apos;).getall(): yield &#123;&quot;title&quot;: h3&#125; for href in response.xpath(&apos;//a/@href&apos;).getall(): yield scrapy.Request(response.urljoin(href), self.parse) 123456789import scrapyclass MySpider(scrapy.Spider): name = &apos;myspider&apos; def __init__(self, category=None, *args, **kwargs): super(MySpider, self).__init__(*args, **kwargs) self.start_urls = [&apos;http://www.example.com/categories/%s&apos; % category] # ... 默认的init方法将使用任何蜘蛛参数，并将其作为属性复制到蜘蛛。上面的示例也可以编写如下： 1234567import scrapyclass MySpider(scrapy.Spider): name = &apos;myspider&apos; def start_requests(self): yield scrapy.Request(&apos;http://www.example.com/categories/%s&apos; % self.category) 12.3 选择器为了解释如何使用选择器，我们将使用Scrapy shell（提供交互式测试）和Scrapy文档服务器中的示例页面， 这是它的HTML代码： 123456789101112131415&lt;html&gt; &lt;head&gt; &lt;base href=&apos;http://example.com/&apos; /&gt; &lt;title&gt;Example website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&apos;images&apos;&gt; &lt;a href=&apos;image1.html&apos;&gt;Name: My image 1 &lt;br /&gt;&lt;img src=&apos;image1_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image2.html&apos;&gt;Name: My image 2 &lt;br /&gt;&lt;img src=&apos;image2_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image3.html&apos;&gt;Name: My image 3 &lt;br /&gt;&lt;img src=&apos;image3_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image4.html&apos;&gt;Name: My image 4 &lt;br /&gt;&lt;img src=&apos;image4_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image5.html&apos;&gt;Name: My image 5 &lt;br /&gt;&lt;img src=&apos;image5_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 进入shell12345首先，让我们打开shell：1 scrapy shell https://doc.scrapy.org/en/latest/_static/selectors-sample1.html# 这个网址是官网提供进行选择器测试的网页，# 然后，在shell加载之后，您将获得响应作为response shell变量，并在response.selector属性中附加选择器，selectctor是一个类。 初步使用原始方式response.selector.xpath()和response.selector.css()便捷方式 response.xpath()和response.css() 123456789101112131415161718192021222324让我们构建一个XPath来选择title标签内的文本：In [1]: response.selector.xpath(&apos;//title/text()&apos;)Out[1]: [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]# 返回的是selector的列表，使用extract_first() 方法提取内容# 使用XPath和CSS查询响应非常常见，响应包括两个便捷快捷方式：response.xpath()和response.css()：# 返回的都是SelectorList，这是新的选择列表。# 方式一： 使用response.xpath() 获取内容In [2]: response.selector.xpath(&apos;//title/text()&apos;).extract_first() Out[2]: &apos;Example website&apos;# 使用extract_first 方法获取文本内容（extract:提取）# 方式二：使用response.css 获取内容In [3]: response.selector.css(&apos;title::text&apos;).extract_first()Out[3]: &apos;Example website&apos;# 为了方便使用，可以不加selector 方法，直接使用In [4]: response.xpath(&apos;//title/text()&apos;)Out[4]: [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]In [5]: response.css(&apos;title::text&apos;)In [5]:[&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]正如你所看到的，.xpath()并且.css()方法返回一个 SelectorList实例，这是新的选择列表。此API可用于快速选择嵌套数据： 获取img 标签中src 属性内容，生成一个文本列表 1234567891011121314151617181920212223In [9]: response.css(&apos;img&apos;) # 通过img 标签，获取SelectorListOut[9]: [&lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&apos;&gt;]In [10]: response.css(&apos;img&apos;).xpath(&apos;@src&apos;) # 获取标签内src属性内容，生成的是SelectorListOut[10]: [&lt;Selector xpath=&apos;@src&apos; data=&apos;image1_thumb.jpg&apos;&gt;, &lt;Selector xpath=&apos;@src&apos; data=&apos;image2_thumb.jpg&apos;&gt;, &lt;Selector xpath=&apos;@src&apos; data=&apos;image3_thumb.jpg&apos;&gt;, &lt;Selector xpath=&apos;@src&apos; data=&apos;image4_thumb.jpg&apos;&gt;, &lt;Selector xpath=&apos;@src&apos; data=&apos;image5_thumb.jpg&apos;&gt;]In [11]: response.css(&apos;img&apos;).xpath(&apos;@src&apos;).extract() # extract() 提取SelectorList中的文本Out[11]: [&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;] xpath和css结合使用css(‘img::attr(src)’) ： ::attr 是获取标签属性内容 12345678# 结合xpath和css 提取数据，xpath 定位到块的内容，css 定位到具体标签内容In [21]: response.xpath(&apos;//div[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(src)&apos;).extract()Out[21]: [&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;] 提取SelectorList 的data数据，转为文本extract()： 是获取所有extract_first() : 获取第一个元素extract_first(default=’’) : 获取第一个元素,如果没有则显示default 内的内容 提取id=”images”的div下的a标签的文本内容 12345678910In [16]: response.xpath(&apos;//div[@id=&quot;images&quot;]/a/text()&apos;).extract()Out[16]: [&apos;Name: My image 1 &apos;, &apos;Name: My image 2 &apos;, &apos;Name: My image 3 &apos;, &apos;Name: My image 4 &apos;, &apos;Name: My image 5 &apos;]In [17]: response.xpath(&apos;//div[@id=&quot;images&quot;]/a/text()&apos;).extract_first()Out[17]: &apos;Name: My image 1 &apos; 提取属性使用css和xpath 分别提取a标签下的href属性内容 12345In [23]: response.xpath(&apos;//a/@href&apos;).extract()Out[23]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]In [24]: response.css(&apos;a::attr(href)&apos;).extract()Out[24]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;] 获取标签文本获取a标签下的文本 123456789101112131415In [26]: response.css(&apos;a::text&apos;).extract()Out[26]: [&apos;Name: My image 1 &apos;, &apos;Name: My image 2 &apos;, &apos;Name: My image 3 &apos;, &apos;Name: My image 4 &apos;, &apos;Name: My image 5 &apos;]In [28]: response.xpath(&apos;//a/text()&apos;).extract()Out[28]: [&apos;Name: My image 1 &apos;, &apos;Name: My image 2 &apos;, &apos;Name: My image 3 &apos;, &apos;Name: My image 4 &apos;, &apos;Name: My image 5 &apos;] 标签属性包含匹配a标签的href属性名称包含image的所有超链接 1234567# href*=image ： href属性内包含image数据In [33]: response.css(&apos;a[href*=image]::attr(href)&apos;).extract()Out[33]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]# contains(@属性名,值) In [34]: response.xpath(&apos;//a[contains(@href,&quot;image&quot;)]/@href&apos;).extract()Out[34]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;] 属性href = image 的a 标签下的img标签的src属性 12345678910111213141516# 注意中间有个空格In [37]: response.css(&apos;a[href*=image] img::attr(src)&apos;).extract()Out[37]: [&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;]In [38]: response.xpath(&apos;//a[contains(@href,&quot;image&quot;)]/img/@src&apos;).extract()Out[38]: [&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;] 使用正则提取字符串对SelectorList 使用正则，非extract()后使用正则 对a标签下内容进行提取字符串 123456789101112131415161718In [43]: response.xpath(&apos;//a/text()&apos;).re(&apos;Name\\:(.*)&apos;)Out[43]: [&apos; My image 1 &apos;, &apos; My image 2 &apos;, &apos; My image 3 &apos;, &apos; My image 4 &apos;, &apos; My image 5 &apos;]In [44]: response.css(&apos;a::text&apos;).re(&apos;Name\\:(.*)&apos;)Out[44]: [&apos; My image 1 &apos;, &apos; My image 2 &apos;, &apos; My image 3 &apos;, &apos; My image 4 &apos;, &apos; My image 5 &apos;]In [45]: response.css(&apos;a::text&apos;).re_first(&apos;Name\\:(.*)&apos;) # 提取第一个Out[45]: &apos; My image 1 &apos; 12.4 Item(项目)抓取的主要目标是从非结构化源（通常是网页）中提取结构化数据。Scrapy蜘蛛可以像Python一样返回提取的数据。虽然方便和熟悉，但P很容易在字段名称中输入拼写错误或返回不一致的数据，尤其是在具有许多蜘蛛的较大项目中。 为了定义通用输出数据格式，Scrapy提供了Item类。 Item对象是用于收集抓取数据的简单容器。它们提供类似字典的 API，并具有用于声明其可用字段的方便语法。 12.4.1 声明项目使用简单的类定义语法和Field 对象声明项。这是一个例子： 1234567import scrapy class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) 注意那些熟悉Django的人会注意到Scrapy Items被宣告类似于Django Models，除了Scrapy Items更简单，因为没有不同字段类型的概念。 12.4.2 项目字段Field对象用于指定每个字段的元数据。例如，last_updated上面示例中说明的字段的序列化函数。 您可以为每个字段指定任何类型的元数据。Field对象接受的值没有限制。出于同样的原因，没有所有可用元数据键的参考列表。 Field对象中定义的每个键可以由不同的组件使用，只有那些组件知道它。您也可以根据Field自己的需要定义和使用项目中的任何其他键。 Field对象的主要目标是提供一种在一个地方定义所有字段元数据的方法。通常，行为取决于每个字段的那些组件使用某些字段键来配置该行为。 12.4.3 使用项目以下是使用上面声明的Product项目对项目执行的常见任务的一些示例 。您会注意到API与dict API非常相似。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&gt;&gt;&gt; product = Product(name=&apos;Desktop PC&apos;, price=1000)&gt;&gt;&gt; print productProduct(name=&apos;Desktop PC&apos;, price=1000)获取字段值&gt;&gt;&gt; product[&apos;name&apos;]Desktop PC&gt;&gt;&gt; product.get(&apos;name&apos;)Desktop PC &gt;&gt;&gt; product[&apos;price&apos;]1000 &gt;&gt;&gt; product[&apos;last_updated&apos;]Traceback (most recent call last):...KeyError: &apos;last_updated&apos; &gt;&gt;&gt; product.get(&apos;last_updated&apos;, &apos;not set&apos;)not set &gt;&gt;&gt; product[&apos;lala&apos;] # getting unknown fieldTraceback (most recent call last):...KeyError: &apos;lala&apos; &gt;&gt;&gt; product.get(&apos;lala&apos;, &apos;unknown field&apos;)&apos;unknown field&apos; &gt;&gt;&gt; &apos;name&apos; in product # is name field populated?True &gt;&gt;&gt; &apos;last_updated&apos; in product # is last_updated populated?False &gt;&gt;&gt; &apos;last_updated&apos; in product.fields # is last_updated a declared field?True &gt;&gt;&gt; &apos;lala&apos; in product.fields # is lala a declared field?False设定字段值&gt;&gt;&gt; product[&apos;last_updated&apos;] = &apos;today&apos;&gt;&gt;&gt; product[&apos;last_updated&apos;]today &gt;&gt;&gt; product[&apos;lala&apos;] = &apos;test&apos; # setting unknown fieldTraceback (most recent call last):...KeyError: &apos;Product does not support field: lala&apos;访问所有填充值要访问所有填充值，只需使用典型的dict API： &gt;&gt;&gt; product.keys()[&apos;price&apos;, &apos;name&apos;] &gt;&gt;&gt; product.items()[(&apos;price&apos;, 1000), (&apos;name&apos;, &apos;Desktop PC&apos;)]其他常见任务复制项目： &gt;&gt;&gt; product2 = Product(product)&gt;&gt;&gt; print product2Product(name=&apos;Desktop PC&apos;, price=1000) &gt;&gt;&gt; product3 = product2.copy()&gt;&gt;&gt; print product3Product(name=&apos;Desktop PC&apos;, price=1000)从项目创建dicts： &gt;&gt;&gt; dict(product) # create a dict from all populated values&#123;&apos;price&apos;: 1000, &apos;name&apos;: &apos;Desktop PC&apos;&#125;从dicts创建项目： &gt;&gt;&gt; Product(&#123;&apos;name&apos;: &apos;Laptop PC&apos;, &apos;price&apos;: 1500&#125;)Product(price=1500, name=&apos;Laptop PC&apos;) &gt;&gt;&gt; Product(&#123;&apos;name&apos;: &apos;Laptop PC&apos;, &apos;lala&apos;: 1500&#125;) # warning: unknown field in dictTraceback (most recent call last):...KeyError: &apos;Product does not support field: lala&apos; 12.4.4 扩展项目您可以通过声明原始Item的子类来扩展Items（以添加更多字段或更改某些字段的某些元数据）。 例如： 123class DiscountedProduct(Product): discount_percent = scrapy.Field(serializer=str) discount_expiration_date = scrapy.Field() 12.5 Item PipeLine 项目下的piplines.py文件 在一个项目被蜘蛛抓取之后，它被发送到项目管道，该项目管道通过顺序执行的几个组件处理它。 每个项目管道组件（有时简称为“项目管道”）是一个实现简单方法的Python类。他们收到一个项目并对其执行操作，同时决定该项目是否应该继续通过管道或被丢弃并且不再处理。 项目管道的典型用途是： cleansing HTML data validating scraped data (checking that the items contain certain fields) checking for duplicates (and dropping them) storing the scraped item in a database 清理HTML数据 验证抓取的数据（检查项目是否包含某些字段） 检查重复项（并将其删除） 将刮擦的物品存储在数据库中 12.5.1 编写自己的项目管道123456789101112131415161718每个项管道组件都是一个必须实现以下方法的Python类：process_item（self，项目，蜘蛛）为每个项目管道组件调用此方法。process_item() 必须要么：返回带数据的dict，返回一个Item （或任何后代类）对象，返回Twisted Deferred或引发 DropItem异常。丢弃的项目不再由其他管道组件处理。此外，他们还可以实现以下方法：open_spider（self，蜘蛛）打开蜘蛛时会调用此方法。close_spider（self，蜘蛛）当蜘蛛关闭时调用此方法。from_crawler（cls，crawler ）如果存在，则调用此类方法以从a创建管道实例Crawler。它必须返回管道的新实例。Crawler对象提供对所有Scrapy核心组件的访问，如设置和信号; 它是管道访问它们并将其功能挂钩到Scrapy的一种方式。 12.5.2 项目管道示例(1) 价格验证和丢弃物品没有价格让我们看看下面的假设管道，它调整 price那些不包含增值税（price_excludes_vat属性）的项目的属性，并删除那些不包含价格的项目： 12345678910111213from scrapy.exceptions import DropItem class PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item[&apos;price&apos;]: if item[&apos;price_excludes_vat&apos;]: item[&apos;price&apos;] = item[&apos;price&apos;] * self.vat_factor return item else: raise DropItem(&quot;Missing price in %s&quot; % item) (2) 将项目写入JSON文件以下管道将所有已删除的项目（来自所有蜘蛛）存储到一个items.jl文件中，每行包含一个以JSON格式序列化的项目： 1234567891011121314import json class JsonWriterPipeline(object): def open_spider(self, spider): self.file = open(&apos;items.jl&apos;, &apos;w&apos;) def close_spider(self, spider): self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item)) + &quot;\\n&quot; self.file.write(line) return item 注意JsonWriterPipeline的目的只是介绍如何编写项目管道。如果您确实要将所有已删除的项目存储到JSON文件中，则应使用Feed导出。 (3) 将项目写入数据库在这个例子中，我们将使用pymongo将项目写入MongoDB。MongoDB地址和数据库名称在Scrapy设置中指定; MongoDB集合以item类命名。 这个例子的要点是展示如何使用from_crawler() 方法以及如何正确地清理资源： 123456789101112131415161718192021222324252627import pymongo class MongoPipeline(object): collection_name = &apos;scrapy_items&apos; def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;), mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;) ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert_one(dict(item)) return item (4) 重复过滤一个过滤器，用于查找重复项目，并删除已处理的项目。假设我们的项目具有唯一ID，但我们的蜘蛛会返回具有相同ID的多个项目： 12345678910111213from scrapy.exceptions import DropItem class DuplicatesPipeline(object): def __init__(self): self.ids_seen = set() def process_item(self, item, spider): if item[&apos;id&apos;] in self.ids_seen: raise DropItem(&quot;Duplicate item found: %s&quot; % item) else: self.ids_seen.add(item[&apos;id&apos;]) return item 12.5.3 激活项目管道组件要激活Item Pipeline组件，必须将其类添加到 ITEM_PIPELINES设置中，如下例所示： 1234ITEM_PIPELINES = &#123; &apos;myproject.pipelines.PricePipeline&apos;: 300, &apos;myproject.pipelines.JsonWriterPipeline&apos;: 800,&#125; 您在此设置中为类分配的整数值决定了它们运行的顺序：项目从较低值到较高值类进行。习惯上在0-1000范围内定义这些数字。 12.6 下载中间件 middleware.py 文件 处理request 和response的组件，处理异常，以及其他相关功能 123456789101112131415161718192021222324252627282930313233343536373839404142class MyDownMiddleware(object): def process_request(self, request, spider): &quot;&quot;&quot; 请求需要被下载时，经过所有下载器中间件的process_request调用 :param request: :param spider: :return: None,继续后续中间件去下载； Response对象，停止process_request的执行，开始执行process_response Request对象，停止中间件的执行，将Request重新调度器 raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception &quot;&quot;&quot; pass def process_response(self, request, response, spider): &quot;&quot;&quot; spider处理完成，返回时调用 :param response: :param result: :param spider: :return: Response 对象：转交给其他中间件process_response Request 对象：停止中间件，request会被重新调度下载 raise IgnoreRequest 异常：调用Request.errback &quot;&quot;&quot; print(&apos;response1&apos;) return response def process_exception(self, request, exception, spider): &quot;&quot;&quot; 当下载处理器(download handler)或 process_request() (下载中间件)抛出异常 :param response: :param exception: :param spider: :return: None：继续交给后续中间件处理异常； Response对象：停止后续process_exception方法 Request对象：停止中间件，request将会被重新调用下载 &quot;&quot;&quot; return None 12.7 settings配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392# -*- coding: utf-8 -*-# Scrapy settings for step8_king project## For simplicity, this file contains only settings considered important or# commonly used. You can find more settings consulting the documentation:## http://doc.scrapy.org/en/latest/topics/settings.html# http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html# http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html# 1. 爬虫名称BOT_NAME = &apos;step8_king&apos;# 2. 爬虫应用路径SPIDER_MODULES = [&apos;step8_king.spiders&apos;]NEWSPIDER_MODULE = &apos;step8_king.spiders&apos;# Crawl responsibly by identifying yourself (and your website) on the user-agent# 3. 客户端 user-agent请求头# USER_AGENT = &apos;step8_king (+http://www.yourdomain.com)&apos;# Obey robots.txt rules# 4. 禁止爬虫配置# ROBOTSTXT_OBEY = False# Configure maximum concurrent requests performed by Scrapy (default: 16)# 5. 并发请求数# CONCURRENT_REQUESTS = 4# Configure a delay for requests for the same website (default: 0)# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docs# 6. 延迟下载秒数# DOWNLOAD_DELAY = 2# The download delay setting will honor only one of:# 7. 单域名访问并发数，并且延迟下次秒数也应用在每个域名# CONCURRENT_REQUESTS_PER_DOMAIN = 2# 单IP访问并发数，如果有值则忽略：CONCURRENT_REQUESTS_PER_DOMAIN，并且延迟下次秒数也应用在每个IP# CONCURRENT_REQUESTS_PER_IP = 3# Disable cookies (enabled by default)# 8. 是否支持cookie，cookiejar进行操作cookie# COOKIES_ENABLED = True# COOKIES_DEBUG = True# Disable Telnet Console (enabled by default)# 9. Telnet用于查看当前爬虫的信息，操作爬虫等...# 使用telnet ip port ，然后通过命令操作# TELNETCONSOLE_ENABLED = True# TELNETCONSOLE_HOST = &apos;127.0.0.1&apos;# TELNETCONSOLE_PORT = [6023,]# 10. 默认请求头# Override the default request headers:# DEFAULT_REQUEST_HEADERS = &#123;# &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,# &apos;Accept-Language&apos;: &apos;en&apos;,# &#125;# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html# 11. 定义pipeline处理请求# ITEM_PIPELINES = &#123;# &apos;step8_king.pipelines.JsonPipeline&apos;: 700,# &apos;step8_king.pipelines.FilePipeline&apos;: 500,# &#125;# 12. 自定义扩展，基于信号进行调用# Enable or disable extensions# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html# EXTENSIONS = &#123;# # &apos;step8_king.extensions.MyExtension&apos;: 500,# &#125;# 13. 爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度# DEPTH_LIMIT = 3# 14. 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo# 后进先出，深度优先# DEPTH_PRIORITY = 0# SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleLifoDiskQueue&apos;# SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.LifoMemoryQueue&apos;# 先进先出，广度优先# DEPTH_PRIORITY = 1# SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleFifoDiskQueue&apos;# SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.FifoMemoryQueue&apos;# 15. 调度器队列# SCHEDULER = &apos;scrapy.core.scheduler.Scheduler&apos;# from scrapy.core.scheduler import Scheduler# 16. 访问URL去重# DUPEFILTER_CLASS = &apos;step8_king.duplication.RepeatUrl&apos;# Enable and configure the AutoThrottle extension (disabled by default)# See http://doc.scrapy.org/en/latest/topics/autothrottle.html&quot;&quot;&quot;17. 自动限速算法 from scrapy.contrib.throttle import AutoThrottle 自动限速设置 1. 获取最小延迟 DOWNLOAD_DELAY 2. 获取最大延迟 AUTOTHROTTLE_MAX_DELAY 3. 设置初始下载延迟 AUTOTHROTTLE_START_DELAY 4. 当请求下载完成后，获取其&quot;连接&quot;时间 latency，即：请求连接到接受到响应头之间的时间 5. 用于计算的... AUTOTHROTTLE_TARGET_CONCURRENCY target_delay = latency / self.target_concurrency new_delay = (slot.delay + target_delay) / 2.0 # 表示上一次的延迟时间 new_delay = max(target_delay, new_delay) new_delay = min(max(self.mindelay, new_delay), self.maxdelay) slot.delay = new_delay&quot;&quot;&quot;# 开始自动限速# AUTOTHROTTLE_ENABLED = True# The initial download delay# 初始下载延迟# AUTOTHROTTLE_START_DELAY = 5# The maximum download delay to be set in case of high latencies# 最大下载延迟# AUTOTHROTTLE_MAX_DELAY = 10# The average number of requests Scrapy should be sending in parallel to each remote server# 平均每秒并发数# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# Enable showing throttling stats for every response received:# 是否显示# AUTOTHROTTLE_DEBUG = True# Enable and configure HTTP caching (disabled by default)# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings&quot;&quot;&quot;18. 启用缓存 目的用于将已经发送的请求或相应缓存下来，以便以后使用 from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware from scrapy.extensions.httpcache import DummyPolicy from scrapy.extensions.httpcache import FilesystemCacheStorage&quot;&quot;&quot;# 是否启用缓存策略# HTTPCACHE_ENABLED = True# 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可# HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.DummyPolicy&quot;# 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略# HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.RFC2616Policy&quot;# 缓存超时时间# HTTPCACHE_EXPIRATION_SECS = 0# 缓存保存路径# HTTPCACHE_DIR = &apos;httpcache&apos;# 缓存忽略的Http状态码# HTTPCACHE_IGNORE_HTTP_CODES = []# 缓存存储的插件# HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;&quot;&quot;&quot;19. 代理，需要在环境变量中设置 from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware 方式一：使用默认 os.environ &#123; http_proxy:http://root:woshiniba@192.168.11.11:9999/ https_proxy:http://192.168.11.11:9999/ &#125; 方式二：使用自定义下载中间件 def to_bytes(text, encoding=None, errors=&apos;strict&apos;): if isinstance(text, bytes): return text if not isinstance(text, six.string_types): raise TypeError(&apos;to_bytes must receive a unicode, str or bytes &apos; &apos;object, got %s&apos; % type(text).__name__) if encoding is None: encoding = &apos;utf-8&apos; return text.encode(encoding, errors) class ProxyMiddleware(object): def process_request(self, request, spider): PROXIES = [ &#123;&apos;ip_port&apos;: &apos;111.11.228.75:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;, &#123;&apos;ip_port&apos;: &apos;120.198.243.22:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;, &#123;&apos;ip_port&apos;: &apos;111.8.60.9:8123&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;, &#123;&apos;ip_port&apos;: &apos;101.71.27.120:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;, &#123;&apos;ip_port&apos;: &apos;122.96.59.104:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;, &#123;&apos;ip_port&apos;: &apos;122.224.249.122:8088&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;, ] proxy = random.choice(PROXIES) if proxy[&apos;user_pass&apos;] is not None: request.meta[&apos;proxy&apos;] = to_bytes（&quot;http://%s&quot; % proxy[&apos;ip_port&apos;]） encoded_user_pass = base64.encodestring(to_bytes(proxy[&apos;user_pass&apos;])) request.headers[&apos;Proxy-Authorization&apos;] = to_bytes(&apos;Basic &apos; + encoded_user_pass) print &quot;**************ProxyMiddleware have pass************&quot; + proxy[&apos;ip_port&apos;] else: print &quot;**************ProxyMiddleware no pass************&quot; + proxy[&apos;ip_port&apos;] request.meta[&apos;proxy&apos;] = to_bytes(&quot;http://%s&quot; % proxy[&apos;ip_port&apos;]) DOWNLOADER_MIDDLEWARES = &#123; &apos;step8_king.middlewares.ProxyMiddleware&apos;: 500, &#125; &quot;&quot;&quot;&quot;&quot;&quot;20. Https访问 Https访问时有两种情况： 1. 要爬取网站使用的可信任证书(默认支持) DOWNLOADER_HTTPCLIENTFACTORY = &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot; DOWNLOADER_CLIENTCONTEXTFACTORY = &quot;scrapy.core.downloader.contextfactory.ScrapyClientContextFactory&quot; 2. 要爬取网站使用的自定义证书 DOWNLOADER_HTTPCLIENTFACTORY = &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot; DOWNLOADER_CLIENTCONTEXTFACTORY = &quot;step8_king.https.MySSLFactory&quot; # https.py from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory from twisted.internet.ssl import (optionsForClientTLS, CertificateOptions, PrivateCertificate) class MySSLFactory(ScrapyClientContextFactory): def getCertificateOptions(self): from OpenSSL import crypto v1 = crypto.load_privatekey(crypto.FILETYPE_PEM, open(&apos;/Users/wupeiqi/client.key.unsecure&apos;, mode=&apos;r&apos;).read()) v2 = crypto.load_certificate(crypto.FILETYPE_PEM, open(&apos;/Users/wupeiqi/client.pem&apos;, mode=&apos;r&apos;).read()) return CertificateOptions( privateKey=v1, # pKey对象 certificate=v2, # X509对象 verify=False, method=getattr(self, &apos;method&apos;, getattr(self, &apos;_ssl_method&apos;, None)) ) 其他： 相关类 scrapy.core.downloader.handlers.http.HttpDownloadHandler scrapy.core.downloader.webclient.ScrapyHTTPClientFactory scrapy.core.downloader.contextfactory.ScrapyClientContextFactory 相关配置 DOWNLOADER_HTTPCLIENTFACTORY DOWNLOADER_CLIENTCONTEXTFACTORY&quot;&quot;&quot;&quot;&quot;&quot;21. 爬虫中间件 class SpiderMiddleware(object): def process_spider_input(self,response, spider): &apos;&apos;&apos; 下载完成，执行，然后交给parse处理 :param response: :param spider: :return: &apos;&apos;&apos; pass def process_spider_output(self,response, result, spider): &apos;&apos;&apos; spider处理完成，返回时调用 :param response: :param result: :param spider: :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable) &apos;&apos;&apos; return result def process_spider_exception(self,response, exception, spider): &apos;&apos;&apos; 异常调用 :param response: :param exception: :param spider: :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline &apos;&apos;&apos; return None def process_start_requests(self,start_requests, spider): &apos;&apos;&apos; 爬虫启动时调用 :param start_requests: :param spider: :return: 包含 Request 对象的可迭代对象 &apos;&apos;&apos; return start_requests 内置爬虫中间件： &apos;scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware&apos;: 50, &apos;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&apos;: 500, &apos;scrapy.contrib.spidermiddleware.referer.RefererMiddleware&apos;: 700, &apos;scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware&apos;: 800, &apos;scrapy.contrib.spidermiddleware.depth.DepthMiddleware&apos;: 900,&quot;&quot;&quot;# from scrapy.contrib.spidermiddleware.referer import RefererMiddleware# Enable or disable spider middlewares# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.htmlSPIDER_MIDDLEWARES = &#123; # &apos;step8_king.middlewares.SpiderMiddleware&apos;: 543,&#125;&quot;&quot;&quot;22. 下载中间件 class DownMiddleware1(object): def process_request(self, request, spider): &apos;&apos;&apos; 请求需要被下载时，经过所有下载器中间件的process_request调用 :param request: :param spider: :return: None,继续后续中间件去下载； Response对象，停止process_request的执行，开始执行process_response Request对象，停止中间件的执行，将Request重新调度器 raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception &apos;&apos;&apos; pass def process_response(self, request, response, spider): &apos;&apos;&apos; spider处理完成，返回时调用 :param response: :param result: :param spider: :return: Response 对象：转交给其他中间件process_response Request 对象：停止中间件，request会被重新调度下载 raise IgnoreRequest 异常：调用Request.errback &apos;&apos;&apos; print(&apos;response1&apos;) return response def process_exception(self, request, exception, spider): &apos;&apos;&apos; 当下载处理器(download handler)或 process_request() (下载中间件)抛出异常 :param response: :param exception: :param spider: :return: None：继续交给后续中间件处理异常； Response对象：停止后续process_exception方法 Request对象：停止中间件，request将会被重新调用下载 &apos;&apos;&apos; return None 默认下载中间件 &#123; &apos;scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware&apos;: 100, &apos;scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware&apos;: 300, &apos;scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware&apos;: 350, &apos;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&apos;: 400, &apos;scrapy.contrib.downloadermiddleware.retry.RetryMiddleware&apos;: 500, &apos;scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware&apos;: 550, &apos;scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware&apos;: 580, &apos;scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware&apos;: 590, &apos;scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware&apos;: 600, &apos;scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware&apos;: 700, &apos;scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware&apos;: 750, &apos;scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware&apos;: 830, &apos;scrapy.contrib.downloadermiddleware.stats.DownloaderStats&apos;: 850, &apos;scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware&apos;: 900, &#125;&quot;&quot;&quot;# from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware# Enable or disable downloader middlewares# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html# DOWNLOADER_MIDDLEWARES = &#123;# &apos;step8_king.middlewares.DownMiddleware1&apos;: 100,# &apos;step8_king.middlewares.DownMiddleware2&apos;: 500,# &#125; 八 项目代码下载项目代码","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"10 实战1 Requests+正则表达式抓取猫眼TOP100","date":"2019-10-27T11:45:00.000Z","path":"2019/10/27/实战1-Requests-正则表达式抓取猫眼TOP100/","text":"简介: Requests+正则表达式抓取猫眼TOP100 实战: 抓取猫眼TOP1001234567891011第一步:抓取单页内容 利用requests请求目标站点、得到单个网页html代码，返回结果第二步:正则表达式分析 根据HTML代码分析得到电影的 名称、主演、上映时间、评分、 图⽚片链接等信息。第三步:开启循环及多线程 对多⻚页内容遍历，开启多线程提，⾼高抓取速度。 第四步: 保存至文件 通过文件的形式将结果进行保存，每一步电影一个结果一行Json字符串。 目标站点分析猫眼电影》榜单》TOP100 可以看到猫眼TOP100 文档 1234第一页: https://maoyan.com/board/4?offset=0第二页: https://maoyan.com/board/4?offset=10第三页: https://maoyan.com/board/4?offset=20第四页: https://maoyan.com/board/4?offset=30 第一个霸王别姬的HTML源码，整个被dd标签包围 12345678910111213141516171819202122&lt;dd&gt; &lt;i class=&quot;board-index board-index-1&quot;&gt;1&lt;/i&gt; &lt;a href=&quot;/films/1203&quot; title=&quot;霸王别姬&quot; class=&quot;image-link&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;&#123;movieId:1203&#125;&quot;&gt; &lt;img src=&quot;//s3plus.meituan.net/v1/mss_e2821d7f0cfe4ac1bf9202ecf9590e67/cdn-prod/file:5788b470/image/loading_2.e3d934bf.png&quot; alt=&quot;&quot; class=&quot;poster-default&quot; /&gt; &lt;img data-src=&quot;https://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c&quot; alt=&quot;霸王别姬&quot; class=&quot;board-img&quot; /&gt; &lt;/a&gt; &lt;div class=&quot;board-item-main&quot;&gt; &lt;div class=&quot;board-item-content&quot;&gt; &lt;div class=&quot;movie-item-info&quot;&gt; &lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1203&quot; title=&quot;霸王别姬&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;&#123;movieId:1203&#125;&quot;&gt;霸王别姬&lt;/a&gt;&lt;/p&gt; &lt;p class=&quot;star&quot;&gt; 主演：张国荣,张丰毅,巩俐 &lt;/p&gt; &lt;p class=&quot;releasetime&quot;&gt;上映时间：1993-01-01&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;movie-item-number score-num&quot;&gt; &lt;p class=&quot;score&quot;&gt;&lt;i class=&quot;integer&quot;&gt;9.&lt;/i&gt;&lt;i class=&quot;fraction&quot;&gt;5&lt;/i&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/dd&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# -*- coding:utf-8 -*-import reimport jsonimport requestsfrom multiprocessing import Poolfrom requests.exceptions import RequestException# RequestException是requers 所有报错的父类，使用异常处理的时候，直接使用父类进行接受def get_one_page(url): &quot;&quot;&quot; 传入url ,当状态码为200时返回请求的html文本，否则返回None &quot;&quot;&quot; try: # 猫眼做了简单的反爬措施，get请求时带请求头 response = requests.get(url, headers=&#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36&apos;, &#125; ) if response.status_code == 200: return response.text return None except: return Nonedef parse_one_page(html): &quot;&quot;&quot; 解析页面，使用正则 .*？ 表示匹配任意字符到下一个符合条件的字符 （.*?） 表示分组，显示出来 通过class 名称进行定位，使用分组显示数据，.*?&quot;star&quot;&gt;(.*?)&lt;/p&gt;， 类名为star 的标签后面的数据显示出来 注意，re.compile 结尾加 re.S ， 在字符串中，包含换行符\\n，在这种情况下： 如果不使用re.S参数，则只在每一行内进行匹配，如果一行没有，就换下一行重新开始。 而使用re.S参数以后，正则表达式会将这个字符串作为一个整体，在整体中进行匹配。 :param html: :return: &quot;&quot;&quot; pattern = re.compile(&apos;&lt;dd&gt;.*?board-index.*?&gt;(\\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?&quot;name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;&lt;/p&gt;.*?&quot;star&quot;&gt;(.*?)&lt;/p&gt;&apos; +&apos;.*?&quot;releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?&quot;integer&quot;&gt;(.*?)&lt;/i&gt;.*?&quot;fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;,re.S) items = re.findall(pattern, html) # 使用生成器，提起列表上数据,每个数据是个元组，并存到字典中 for item in items: yield &#123; &quot;index&quot; : item[0], &quot;image&quot;: item[1], &quot;title&quot;: item[2], &quot;actor&quot;: item[3].strip()[3:], # 除去空格并进行切片 &quot;time&quot;: item[4].strip()[5:], &quot;score&quot;: item[5]+item[6] &#125;def write_to_file(content): &quot;&quot;&quot; 数据存到文件中 :param content: 字典数据 :return: &quot;&quot;&quot; with open(&apos;result.txt&apos;,&apos;a&apos;,encoding=&apos;utf-8&apos;) as f: f.write(json.dumps(content) + &quot;\\n&quot;) f.close()def main(offset): url = &quot;https://maoyan.com/board/4?offset=&quot; + str(offset) html = get_one_page(url) for item in parse_one_page(html): write_to_file(item)if __name__ == &apos;__main__&apos;: # for i in range(10): # main(i*10) pool = Pool(5) pool.map(main,[ i*10 for i in range(10)]","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"09  selinux 模块","date":"2019-10-27T11:43:00.000Z","path":"2019/10/27/09-selinux-模块/","text":"摘要： Selenium 模块的使用 9.1 Selenium 简介selenium最初是一个自动化测试工具,而爬虫中使用它主要是为了解决requests无法直接执行JavaScript代码的问题 selenium本质是通过驱动浏览器，完全模拟浏览器的操作，比如跳转、输入、点击、下拉等，来拿到网页渲染之后的结果，可支持多种浏览器 9.1.1 安装1 下载驱动 1http://npm.taobao.org/mirrors/chromedriver/2.35/ if mac系统：然后将解压后的chromedriver移动到/usr/local/bin目录下 if window系统：下载chromdriver.exe放到python安装路径的scripts目录中即可，注意最新版本是2.38，并非2.9 2 安装pip包 1pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple selenium 注意：selenium3默认支持的webdriver是Firfox，而Firefox需要安装geckodriver 下载链接 9.2 Selenium模块9.2.1 基本使用12345678910111213141516171819from selenium import webdriver # webdriver 是浏览器驱动对象from selenium.webdriver.common.by import By #按照什么方式查找，By.ID,By.CSS_SELECTORfrom selenium.webdriver.common.keys import Keys #键盘按键操作from selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWait #等待页面加载某些元素import timebrowser=webdriver.Chrome() # 创建chrome驱动程序的新实例try: browser.get(&apos;https://www.jd.com&apos;) input_tag=browser.find_element_by_id(&apos;key&apos;) input_tag.send_keys(&apos;美女&apos;) input_tag.send_keys(Keys.ENTER) wait=WebDriverWait(browser,10) wait.until(EC.presence_of_element_located((By.ID,&apos;J_goodsList&apos;))) #等到id为J_goodsList的元素加载完毕,最多等10秒 # time.sleep(3)finally: browser.close() # 关闭浏览器访问 9.2.2 声明浏览器对象Selenium支持非常多的浏览器，如Chrome、Firefox、Edge等，还有Android、BlackBerry等手机端的浏览器。另外，也支持无界面浏览器PhantomJS(常用于测试的浏览器)。 12345678from selenium import webdriver# 不同的浏览器调用，生成浏览器对象browser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.Edge()browser = webdriver.PhantomJS()browser = webdriver.Safari() 9.2.3 访问页面123456from selenium import webdriverbrowser=webdriver.Chrome()browser.get(&apos;http://www.baidu.com&apos;)print(browser.page_source)browser.close() 结果 返回了百度页面的html 9.3 元素定位9.3.1 单个元素webdriver 提供了一系列的元素定位方法，常用的有以下几种： 12345678idnameclass nametag namelink textpartial link textxpathcss selector 分别对应python webdriver 中的方法为： 12345678find_element_by_id()find_element_by_name()find_element_by_class_name()find_element_by_tag_name()find_element_by_link_text()find_element_by_partial_link_text()find_element_by_xpath()find_element_by_css_selector() 注意：1、find_element_by_xxx找的是第一个符合条件的标签，find_elements_by_xxx找的是所有符合条件的标签。 2、根据ID、CSS选择器和XPath获取，它们返回的结果完全一致。 3、另外，Selenium还提供了通用方法find_element()，它需要传入两个参数：查找方式By和值。实际上，它就是find_element_by_id()这种方法的通用函数版本，比如find_element_by_id(id)就等价于find_element(By.ID, id)，二者得到的结果完全一致。 CSS 选择器，常见三个 ID 前面加”#“ class 前面加：”.” 标签什么也不加 123456789from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input_first = browser.find_element_by_id(&apos;q&apos;) # 通过id input_second = browser.find_element_by_css_selector(&apos;#q&apos;) # css选择器input_third = browser.find_element_by_xpath(&apos;//*[@id=&quot;q&quot;]&apos;) print(input_first, input_second, input_third,sep=&quot;\\n&quot;)browser.close() 三个相同的结果: 123&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;cd7948e7b8a1d20ce7c548a036ab30f8&quot;, element=&quot;b2eca734-38a4-4ad8-aaec-b575047cccd3&quot;)&gt;&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;cd7948e7b8a1d20ce7c548a036ab30f8&quot;, element=&quot;b2eca734-38a4-4ad8-aaec-b575047cccd3&quot;)&gt;&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;cd7948e7b8a1d20ce7c548a036ab30f8&quot;, element=&quot;b2eca734-38a4-4ad8-aaec-b575047cccd3&quot;)&gt; 使用browser.find_element进行获取元素，获取的结果和上面相同 12345678from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input_first = browser.find_element(By.ID, &apos;q&apos;) # id=qprint(input_first)browser.close() 9.3.2 多个的元素find_elements :查找多个find_element :查找单个 1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)lis = browser.find_elements_by_css_selector(&apos;.service-bd li&apos;)print(lis)browser.close() 12345678from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)lis = browser.find_elements(By.CSS_SELECTOR, &apos;.service-bd li&apos;)print(lis)browser.close() 9.4 元素交互Selenium可以驱动浏览器来执行一些操作，也就是说可以让浏览器模拟执行一些动作。 9.4.1 元素交互常用操作比较常见的用法有： 输入文字时用send_keys()方法， 清空文字时用clear()方法， 点击按钮时用click()方法。示例如下：1234567891011from selenium import webdriverimport timebrowser = webdriver.Chrome() # 调用谷歌浏览器browser.get(&apos;https://www.taobao.com&apos;) # 访问淘宝页面input = browser.find_element_by_id(&apos;q&apos;) # 找到 id = 9 的元素 input.send_keys(&apos;MAC&apos;) # 输入文字mactime.sleep(1)input.clear()input.send_keys(&apos;IPhone&apos;)button = browser.find_element_by_class_name(&apos;btn-search&apos;) button.click() # 点击搜索 结果: 弹出一个淘宝页面，搜索框内先输入MAC ,再输入IPhone并进行搜索 常见节点的操作 9.4.2 动作链在上面的实例中，一些交互动作都是针对某个节点执行的。比如，对于输入框，我们就调用它的输入文字和清空文字方法；对于按钮，就调用它的点击方法。其实，还有另外一些操作，它们没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链。比如，现在实现一个节点的拖曳操作，将某个节点从一处拖曳到另外一处，可以这样实现： 123456789101112131415161718from selenium import webdriverfrom selenium.webdriver import ActionChainsimport timebrowser = webdriver.Chrome()url = &apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;browser.get(url)browser.switch_to.frame(&apos;iframeResult&apos;)source = browser.find_element_by_css_selector(&apos;#draggable&apos;) # 源target = browser.find_element_by_css_selector(&apos;#droppable&apos;) # 目标actions = ActionChains(browser) # # actions.drag_and_drop(source, target)actions.click_and_hold(source) # 点击保持time.sleep(3)for i in range(5): # 每隔0.5s ,x轴移动17像素 actions.move_by_offset(xoffset=17,yoffset=0).perform() time.sleep(0.5)actions.release() 更多的动作链操 9.5 执行JavaScript对于某些操作，Selenium API并没有提供。比如，下拉进度条，它可以直接模拟运行JavaScript，此时使用execute_script()方法即可实现，代码如下： 12345from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.jd.com/&apos;)browser.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight)&apos;)browser.execute_script(&apos;alert(&quot;123&quot;)&apos;) 9.6 获取元素信息通过page_source属性可以获取网页的源代码，接着就可以使用解析库（如正则表达式、Beautiful Soup、pyquery等）来提取信息了。不过，既然Selenium已经提供了选择节点的方法，返回的是WebElement类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等。这样的话，我们就可以不用通过解析源代码来提取信息了，非常方便。 1234567891011121314151617181920212223242526from selenium import webdriverfrom selenium.webdriver.common.by import By #按照什么方式查找，By.ID,By.CSS_SELECTORfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWait #等待页面加载某些元素browser=webdriver.Chrome()browser.get(&apos;https://www.amazon.cn/&apos;)wait=WebDriverWait(browser,10)wait.until(EC.presence_of_element_located((By.ID,&apos;cc-lm-tcgShowImgContainer&apos;)))# 定位获取标签tag=browser.find_element(By.CSS_SELECTOR,&apos;#cc-lm-tcgShowImgContainer img&apos;)#获取标签属性，print(tag.get_attribute(&apos;src&apos;))#获取标签ID，位置，名称，大小（了解）print(tag.id)print(tag.location)print(tag.tag_name)print(tag.size)browser.close() 9.6.1 获取标签属性get_attribute() 方法 通过定位id=zh-top-link-logo的标签，获取该标签的class 属性 123456789from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)logo = browser.find_element_by_id(&apos;zh-top-link-logo&apos;)print(logo)print(logo.get_attribute(&apos;class&apos;)) 9.6.2 获取文本text 属性(该方法加入@property装饰器) 搜索class=Tabs-link 标签的文本 1234567from selenium import webdriverbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)input = browser.find_element_by_class_name(&apos;Tabs-link&apos;)print(input.text) 结果: 1首页 知乎源码 1&lt;a class=&quot;Tabs-link AppHeader-TabsLink&quot; href=&quot;//www.zhihu.com/&quot; data-za-not-track-link=&quot;true&quot;&gt;首页&lt;/a&gt; 9.6.3 获取ID、位置、标签名、大小12345678910from selenium import webdriverbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input.id)print(input.location)print(input.tag_name)print(input.size) 结果: 12340.6822924344980397-1&#123;&apos;y&apos;: 7, &apos;x&apos;: 774&#125;button&#123;&apos;height&apos;: 32, &apos;width&apos;: 66&#125; 9.6.3 Frame元素会创建包含另外一个文档的内联框架（即行内框架） 从父frame切换到子frame ,再从子切换到父frame. 不能从子freme 查找父frame 12345678910111213141516171819202122import timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionbrowser = webdriver.Chrome()url = &apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;browser.get(url)# 切换到子元素framebrowser.switch_to.frame(&apos;iframeResult&apos;)source = browser.find_element_by_css_selector(&apos;#draggable&apos;)print(source)try: # 查父元素frame会报错 logo = browser.find_element_by_class_name(&apos;logo&apos;)except NoSuchElementException: print(&apos;NO LOGO&apos;)# 切换到父元素frame1browser.switch_to.parent_frame()logo = browser.find_element_by_class_name(&apos;logo&apos;)print(logo)print(logo.text) 结果 1234&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.44746093888932004-1&quot;)&gt;NO LOGO&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.13792611320464965-2&quot;)&gt;RUNOOB.COM 9.7 延时等待在Selenium中，get()方法会在网页框架加载结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。这里等待的方式有两种：一种是隐式等待，一种是显式等待。 9.7.2 隐式等待browser.implicitly_wait(10)当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。换句话说，当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找DOM，默认的时间是0。示例如下： 123456789101112131415161718192021from selenium import webdriverfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.common.by import By #按照什么方式查找，By.ID,By.CSS_SELECTORfrom selenium.webdriver.common.keys import Keys #键盘按键操作from selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWait #等待页面加载某些元素browser=webdriver.Chrome()#隐式等待:在查找所有元素时，如果尚未被加载，则等10秒browser.implicitly_wait(10)browser.get(&apos;https://www.baidu.com&apos;)input_tag=browser.find_element_by_id(&apos;kw&apos;)input_tag.send_keys(&apos;美女&apos;)input_tag.send_keys(Keys.ENTER)contents=browser.find_element_by_id(&apos;content_left&apos;) #没有等待环节而直接查找，找不到则会报错print(contents)browser.close() 9.7.2 显示等待wait=WebDriverWait(browser,10)wait.until(EC.presence_of_element_located((By.ID,’content_left’))) 隐式等待的效果其实并没有那么好，因为我们只规定了一个固定时间，而页面的加载时间会受到网络条件的影响。这里还有一种更合适的显式等待方法，它指定要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。 1234567891011121314151617181920212223242526from selenium import webdriverfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.common.by import By #按照什么方式查找，By.ID,By.CSS_SELECTORfrom selenium.webdriver.common.keys import Keys #键盘按键操作from selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWait #等待页面加载某些元素browser=webdriver.Chrome()browser.get(&apos;https://www.baidu.com&apos;)input_tag=browser.find_element_by_id(&apos;kw&apos;)input_tag.send_keys(&apos;美女&apos;)input_tag.send_keys(Keys.ENTER)#显式等待：显式地等待某个元素被加载wait=WebDriverWait(browser,10)# presence_of_element_located ：定位器-用于查找元素，找到后返回WebElementwait.until(EC.presence_of_element_located((By.ID,&apos;content_left&apos;)))contents=browser.find_element(By.CSS_SELECTOR,&apos;#content_left&apos;)print(contents)browser.close() 关于等待条件，其实还有很多，比如判断标题内容，判断某个节点内是否出现了某文字等。更多查看 1http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions 不同的条件，每个条件都是一个类 title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’) visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现Alert 9.8 前进和后退平常使用浏览器时都有前进和后退功能，Selenium也可以完成这个操作，它使用back()方法后退，使用forward()方法前进。示例如下： 12345678910#模拟浏览器的前进后退import time from selenium import webdriver browser=webdriver.Chrome()browser.get(&apos;https://www.baidu.com&apos;)browser.get(&apos;https://www.taobao.com&apos;)browser.get(&apos;http://www.sina.com.cn/&apos;)browser.back()time.sleep(10)browser.forward()browser.close() 9.9 Cookies使用Selenium，还可以方便地对Cookies进行操作，例如获取、添加、删除Cookies等。示例如下： 12345678from selenium import webdriver browser =webdriver.Chrome()browser.get(&apos;https://www.zhihu.com/explore&apos;)print(browser.get_cookies())browser.add_cookie(&#123;&apos;name&apos;:&apos;name&apos;,&apos;domain&apos;:&apos;www.zhihu.com&apos;,&apos;value&apos;:&apos;germey&apos;&#125;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies()) 9.10 选项卡管理同一个浏览器下选项卡操作 123456789101112import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.baidu.com&apos;)browser.execute_script(&apos;window.open()&apos;) # 打开一个新的选项卡print(browser.window_handles) # 返回所有选项卡browser.switch_to_window(browser.window_handles[1]) # 切换到第二个选项卡browser.get(&apos;https://www.taobao.com&apos;)time.sleep(1)browser.switch_to_window(browser.window_handles[0]) # 切换到第一个选项卡browser.get(&apos;https://python.org&apos;) 9.10 异常处理1234567891011121314from selenium import webdriverfrom selenium.common.exceptions import TimeoutException,NoSuchElementException,NoSuchFrameExceptiontry: browser=webdriver.Chrome() browser.get(&apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;) browser.switch_to.frame(&apos;iframssseResult&apos;)except TimeoutException as e: print(e)except NoSuchFrameException as e: print(e)finally: browser.close() 案例讲解滑动验证码的破解通过灰度值判断是否是图片缺口； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWait # 等待元素加载的from selenium.webdriver.common.action_chains import ActionChains #拖拽from selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionfrom selenium.webdriver.common.by import Byfrom PIL import Imageimport requestsimport reimport randomfrom io import BytesIOimport timedef merge_image(image_file,location_list): &quot;&quot;&quot; 拼接图片 &quot;&quot;&quot; im = Image.open(image_file) im.save(&apos;code.jpg&apos;) new_im = Image.new(&apos;RGB&apos;,(260,116)) # 把无序的图片 切成52张小图片 im_list_upper = [] im_list_down = [] # print(location_list) for location in location_list: # print(location[&apos;y&apos;]) if location[&apos;y&apos;] == -58: # 上半边 im_list_upper.append(im.crop((abs(location[&apos;x&apos;]),58,abs(location[&apos;x&apos;])+10,116))) if location[&apos;y&apos;] == 0: # 下半边 im_list_down.append(im.crop((abs(location[&apos;x&apos;]),0,abs(location[&apos;x&apos;])+10,58))) x_offset = 0 for im in im_list_upper: new_im.paste(im,(x_offset,0)) # 把小图片放到 新的空白图片上 x_offset += im.size[0] x_offset = 0 for im in im_list_down: new_im.paste(im,(x_offset,58)) x_offset += im.size[0] #new_im.show() return new_imdef get_image(driver,div_path): &apos;&apos;&apos; 下载无序的图片 然后进行拼接 获得完整的图片 :param driver: :param div_path: :return: &apos;&apos;&apos; background_images = driver.find_elements_by_xpath(div_path) location_list = [] for background_image in background_images: location = &#123;&#125; result = re.findall(&apos;background-image: url\\(&quot;(.*?)&quot;\\); background-position: (.*?)px (.*?)px;&apos;,background_image.get_attribute(&apos;style&apos;)) # print(result) location[&apos;x&apos;] = int(result[0][1]) location[&apos;y&apos;] = int(result[0][2]) image_url = result[0][0] location_list.append(location) image_url = image_url.replace(&apos;webp&apos;,&apos;jpg&apos;) # &apos;替换url http://static.geetest.com/pictures/gt/579066de6/579066de6.webp&apos; image_result = requests.get(image_url).content image_file = BytesIO(image_result) # 是一张无序的图片 image = merge_image(image_file,location_list) return imagedef get_track(distance): # 初速度 v=0 # 单位时间为0.2s来统计轨迹，轨迹即0.2内的位移 t=0.2 # 位移/轨迹列表，列表内的一个元素代表0.2s的位移 tracks=[] tracks_back=[] # 当前的位移 current=0 # 到达mid值开始减速 mid=distance * 7/8 print(&quot;distance&quot;,distance) global random_int random_int=8 distance += random_int # 先滑过一点，最后再反着滑动回来 while current &lt; distance: if current &lt; mid: # 加速度越小，单位时间的位移越小,模拟的轨迹就越多越详细 a = random.randint(2,5) # 加速运动 else: a = -random.randint(2,5) # 减速运动 # 初速度 v0 = v # 0.2秒时间内的位移 s = v0*t+0.5*a*(t**2) # 当前的位置 current += s # 添加到轨迹列表 if round(s)&gt;0: tracks.append(round(s)) else: tracks_back.append(round(s)) # 速度已经达到v,该速度作为下次的初速度 v= v0+a*t print(&quot;tracks:&quot;,tracks) print(&quot;tracks_back:&quot;,tracks_back) print(&quot;current:&quot;,current) # 反着滑动到大概准确位置 tracks_back.append(distance-current) tracks_back.extend([-2,-5,-8,]) return tracks,tracks_backdef get_distance(image1,image2): &apos;&apos;&apos; 拿到滑动验证码需要移动的距离 :param image1:没有缺口的图片对象 :param image2:带缺口的图片对象 :return:需要移动的距离 &apos;&apos;&apos; # print(&apos;size&apos;, image1.size) threshold = 50 for i in range(0,image1.size[0]): # 260 for j in range(0,image1.size[1]): # 160 pixel1 = image1.getpixel((i,j)) pixel2 = image2.getpixel((i,j)) res_R = abs(pixel1[0]-pixel2[0]) # 计算RGB差 res_G = abs(pixel1[1] - pixel2[1]) # 计算RGB差 res_B = abs(pixel1[2] - pixel2[2]) # 计算RGB差 if res_R &gt; threshold and res_G &gt; threshold and res_B &gt; threshold: return i # 需要移动的距离def main_check_code(driver,element): &quot;&quot;&quot; 拖动识别验证码 :param driver: :param element: :return: &quot;&quot;&quot; login_btn = driver.find_element_by_class_name(&apos;js-login&apos;) login_btn.click() element = WebDriverWait(driver, 30, 0.5).until(EC.element_to_be_clickable((By.CLASS_NAME, &apos;gt_guide_tip&apos;))) slide_btn = driver.find_element_by_class_name(&apos;gt_guide_tip&apos;) slide_btn.click() image1 = get_image(driver, &apos;//div[@class=&quot;gt_cut_bg gt_show&quot;]/div&apos;) image2 = get_image(driver, &apos;//div[@class=&quot;gt_cut_fullbg gt_show&quot;]/div&apos;) # 图片上 缺口的位置的x坐标 # 2 对比两张图片的所有RBG像素点，得到不一样像素点的x值，即要移动的距离 l = get_distance(image1, image2) print(&apos;l=&apos;,l) # 3 获得移动轨迹 track_list = get_track(l) print(&apos;第一步,点击滑动按钮&apos;) element = WebDriverWait(driver, 30, 0.5).until(EC.element_to_be_clickable((By.CLASS_NAME, &apos;gt_slider_knob&apos;))) ActionChains(driver).click_and_hold(on_element=element).perform() # 点击鼠标左键，按住不放 import time time.sleep(0.4) print(&apos;第二步,拖动元素&apos;) for track in track_list[0]: ActionChains(driver).move_by_offset(xoffset=track, yoffset=0).perform() # 鼠标移动到距离当前位置（x,y） #time.sleep(0.4) for track in track_list[1]: ActionChains(driver).move_by_offset(xoffset=track, yoffset=0).perform() # 鼠标移动到距离当前位置（x,y） time.sleep(0.1) import time time.sleep(0.6) # ActionChains(driver).move_by_offset(xoffset=2, yoffset=0).perform() # 鼠标移动到距离当前位置（x,y） # ActionChains(driver).move_by_offset(xoffset=8, yoffset=0).perform() # 鼠标移动到距离当前位置（x,y） # ActionChains(driver).move_by_offset(xoffset=2, yoffset=0).perform() # 鼠标移动到距离当前位置（x,y） print(&apos;第三步,释放鼠标&apos;) ActionChains(driver).release(on_element=element).perform() time.sleep(1)def main_check_slider(driver): &quot;&quot;&quot; 检查滑动按钮是否加载 :param driver: :return: &quot;&quot;&quot; while True: try : driver.get(&apos;https://www.huxiu.com/&apos;) element = WebDriverWait(driver, 30, 0.5).until(EC.element_to_be_clickable((By.CLASS_NAME, &apos;js-login&apos;))) if element: return element except TimeoutException as e: print(&apos;超时错误，继续&apos;) time.sleep(5)if __name__ == &apos;__main__&apos;: try: count = 3 # 最多识别3次 driver = webdriver.Chrome() while count &gt; 0: # 等待滑动按钮加载完成 element = main_check_slider(driver) main_check_code(driver,element) try: success_element = (By.CSS_SELECTOR, &apos;.gt_success&apos;) # 得到成功标志 success_images = WebDriverWait(driver,3).until(EC.presence_of_element_located(success_element)) if success_images: print(&apos;成功识别！！！！！！&apos;) count = 0 import sys sys.exit() except Exception as e: print(&apos;识别错误，继续&apos;) count -= 1 time.sleep(1) else: print(&apos;too many attempt check code &apos;) exit(&apos;退出程序&apos;) finally: driver.close()","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"08 解析模块之Xpath 模块","date":"2019-10-27T11:41:00.000Z","path":"2019/10/27/08 解析模块之Xpath 模块/","text":"简介:Xpath 模块 使用 8.1 xpath简介XPath在Python的爬虫学习中，起着举足轻重的地位，对比正则表达式 re两者可以完成同样的工作，实现的功能也差不多，但XPath明显比re具有优势，在网页分析上使re退居二线。 全称为XML Path Language 一种小型的查询语言说道XPath是门语言，不得不说它所具备的优点： 可在XML中查找信息 支持HTML的查找 通过元素和属性进行导航 python开发使用XPath条件： 由于XPath属于lxml库模块，所以首先要安装库lxml。 XPath的简单调用方法： 12345from lxml import etreeselector=etree.HTML(源码) #将html源码转化为能被XPath匹配的格式selector.xpath(表达式) #返回为一列表 8.2 Xpath语法8.2.1 初始化12345678910111213141516171819202122232425262728293031323334html_doc = &quot;&quot;&quot;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;d1&quot;&gt; &lt;div class=&quot;d2&quot;&gt; &lt;p class=&quot;story&quot;&gt; &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; &lt;/p&gt; &lt;/div&gt; &lt;div&gt; &lt;p id=&quot;p1&quot;&gt;ALex is dsb&lt;/p&gt; &lt;p id=&quot;p2&quot;&gt;Egon too&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;d3&quot;&gt; &lt;a href=&quot;http://www.baidu.com&quot;&gt;baidu&lt;/a&gt; &lt;p&gt;百度&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;from lxml import etreeselector=etree.HTML(html_doc) # 将html源码转化为能被XPath匹配的格式 8.2.2 选取节点123456789101112131415&quot;&quot;&quot;nodename 选取nodename节点的所有子节点 xpath(‘//div’) 选取了所有div节点/ 从根节点选取 xpath(‘/div’) 从根节点上选取div节点// 选取所有的当前节点，不考虑他们的位置 xpath(‘//div’) 选取所有的div节点. 选取当前节点 xpath(‘./div’) 选取当前节点下的div节点.. 选取当前节点的父节点 xpath(‘..’) 回到上一个节点@ 选取属性 xpath（’//@calss’） 选取所有的class属性&quot;&quot;&quot;ret=selector.xpath(&quot;//div&quot;)ret=selector.xpath(&quot;/div&quot;)ret=selector.xpath(&quot;./div&quot;)ret=selector.xpath(&quot;//p[@id=&apos;p1&apos;]&quot;)ret=selector.xpath(&quot;//div[@class=&apos;d1&apos;]/div/p[@class=&apos;story&apos;]&quot;) 8.2.3 谓语1234567891011&quot;&quot;&quot;表达式 结果xpath(‘/body/div[1]’) 选取body下的第一个div节点xpath(‘/body/div[last()]’) 选取body下最后一个div节点xpath(‘/body/div[last()-1]’) 选取body下倒数第二个div节点xpath(‘/body/div[positon()&lt;3]’) 选取body下前两个div节点xpath(‘/body/div[@class]’) 选取body下带有class属性的div节点xpath(‘/body/div[@class=”main”]’) 选取body下class属性为main的div节点xpath(‘/body/div[@price&gt;35.00]’) 选取body下price元素值大于35的div节点&quot;&quot;&quot; 1234567891011121314ret=selector.xpath(&quot;//p[@class=&apos;story&apos;]//a[2]&quot;)ret=selector.xpath(&quot;//p[@class=&apos;story&apos;]//a[last()]&quot;)&quot;&quot;&quot;通配符 Xpath通过通配符来选取未知的XML元素表达式 结果xpath（’/div/*’） 选取div下的所有子节点xpath(‘/div[@*]’) 选取所有带属性的div节点&quot;&quot;&quot;ret=selector.xpath(&quot;//p[@class=&apos;story&apos;]/*&quot;)ret=selector.xpath(&quot;//p[@class=&apos;story&apos;]/a[@class]&quot;) 8.2.4 取多个路径使用“|”运算符可以选取多个路径 1234567&quot;&quot;&quot;表达式 结果xpath(‘//div|//table’) 选取所有的div和table节点&quot;&quot;&quot;ret=selector.xpath(&quot;//p[@class=&apos;story&apos;]/a[@class]|//div[@class=&apos;d3&apos;]&quot;)print(ret) 8.2.5 Xpath轴轴可以定义相对于当前节点的节点集 123456789101112131415\"\"\"轴名称 表达式 描述ancestor xpath(‘./ancestor::*’) 选取当前节点的所有先辈节点（父、祖父）ancestor-or-self xpath(‘./ancestor-or-self::*’) 选取当前节点的所有先辈节点以及节点本身attribute xpath(‘./attribute::*’) 选取当前节点的所有属性child xpath(‘./child::*’) 返回当前节点的所有子节点descendant xpath(‘./descendant::*’) 返回当前节点的所有后代节点（子节点、孙节点）following xpath(‘./following::*’) 选取文档中当前节点结束标签后的所有节点following-sibing xpath(‘./following-sibing::*’) 选取当前节点之后的兄弟节点parent xpath(‘./parent::*’) 选取当前节点的父节点preceding xpath(‘./preceding::*’) 选取文档中当前节点开始标签前的所有节点preceding-sibling xpath(‘./preceding-sibling::*’) 选取当前节点之前的兄弟节点self xpath(‘./self::*’) 选取当前节点\"\"\" 8.2.6 功能函数使用功能函数能够更好的进行模糊搜索 12345678\"\"\"函数 用法 解释starts-with xpath(‘//div[starts-with(@id,”ma”)]‘) 选取id值以ma开头的div节点contains xpath(‘//div[contains(@id,”ma”)]‘) 选取id值包含ma的div节点and xpath(‘//div[contains(@id,”ma”) and contains(@id,”in”)]‘) 选取id值包含ma和in的div节点text() xpath(‘//div[contains(text(),”ma”)]‘) 选取节点文本包含ma的div节点\"\"\" 8.3 Element对象12345678910111213141516171819202122232425262728293031323334353637from lxml.etree import _Elementfor obj in ret: print(obj) print(type(obj)) # from lxml.etree import _Element\"\"\"Element对象class xml.etree.ElementTree.Element(tag, attrib=&#123;&#125;, **extra) tag：string，元素代表的数据种类。 text：string，元素的内容。 tail：string，元素的尾形。 attrib：dictionary，元素的属性字典。 ＃针对属性的操作 clear()：清空元素的后代、属性、text和tail也设置为None。 get(key, default=None)：获取key对应的属性值，如该属性不存在则返回default值。 items()：根据属性字典返回一个列表，列表元素为(key, value）。 keys()：返回包含所有元素属性键的列表。 set(key, value)：设置新的属性键与值。 ＃针对后代的操作 append(subelement)：添加直系子元素。 extend(subelements)：增加一串元素对象作为子元素。＃python2.7新特性 find(match)：寻找第一个匹配子元素，匹配对象可以为tag或path。 findall(match)：寻找所有匹配子元素，匹配对象可以为tag或path。 findtext(match)：寻找第一个匹配子元素，返回其text值。匹配对象可以为tag或path。 insert(index, element)：在指定位置插入子元素。 iter(tag=None)：生成遍历当前元素所有后代或者给定tag的后代的迭代器。＃python2.7新特性 iterfind(match)：根据tag或path查找所有的后代。 itertext()：遍历所有后代并返回text值。 remove(subelement)：删除子元素。\"\"\"","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"07 PyQuery  模块的使用","date":"2019-10-27T11:40:00.000Z","path":"2019/10/27/07 PyQuery  模块的使用/","text":"简介: PyQuery 模块的使用 7.1 简介强大又灵活的网页解析库。PyQuery使用lxml进行快速的xml和html操作，功能和BeautfulSoup。语法和jquery 语法相似。 官方文档 1https://pyquery.readthedocs.io/en/latest/ 7.1.1 安装1pip install pyquery 本文所有的测试html如下 1234567891011html = &apos;&apos;&apos;&lt;div&gt; &lt;ul&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos; 7.2 初始化BeautifulSoup 是对字符串进行初始化的，PyQuery 有三种方式进行初始化 7.2.1 字符串初始化1234567# 设置别名from pyquery import PyQuery as pq# 生成一个pyquery 对象，传入html 字符串doc = pq(html)# 传入选择器（CSS选择器)，选择li标签print(doc(&apos;li&apos;)) 结果 12345&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 7.2.2 URL初始化传入URL进行初始化 1234from pyquery import PyQuery as pqdoc = pq(url=&apos;http://www.baidu.com&apos;)# 获取head 标签数据print(doc(&apos;head&apos;)) 结果： 1&lt;head&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&quot;/&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; 7.2.3 文件初始化12345from pyquery import PyQuery as pq# html 文件存放在py代码文本同级目录，如果不同级，使用路径doc = pq(filename=&apos;demo.html&apos;)# 获取li标签的数据print(doc(&apos;li&apos;)) 7.3 获取标签元素的方式7.3.1 基本CSS选择器有三种常用的解析方式 ID 前面加”#“ class 前面加：”.” 标签什么也不加 1234567from pyquery import PyQuery as pqdoc = pq(html)# 查找id=container 下的class=list tag=li的对象，没有绝对的子元素的关系，只要是嵌套关系即可# 中间使用空格隔开print(doc(&apos;#container .list li&apos;)) 结果 12345&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 7.3.2 查找元素子元素使用pyquery对象的find方法查找当前元素下的子元素(可以使非父子元素)使用pyquery对象的children方法查找当前元素下的子元素(存在父子元素关系) 注意传入的是字符串，字符串之间隔开 123456789101112131415from pyquery import PyQuery as pqdoc = pq(html)# 查找class=list的元素内容items = doc(&apos;.list&apos;)# items 也是Pyquery对象print(type(items))print(items)# items 也是Pyquery对象，使用find 方法，查找li 标签lis = items.find(&apos;li&apos;)print(type(lis))print(lis) 结果: 123456789101112131415&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 1234# 使用children() 方法查找class= list下的所有数据lis = items.children() print(type(lis))print(lis) 123# 使用children() 方法查找class= list下的class=active的数据lis = items.children(&apos;.active&apos;)print(lis) 父元素使用pyquery对象的的parent方法，查找对象的父元素以内的内容 parents 方法是查找该元素的所有父节点（包括父节点的父节点）的内容，比parent多个s注意：parents 显示每一个父节点以下的内容，以a标签为例，会显示这个div内容，显示整个ul内容，显示li内容，会一共显示3段数据。 123456789from pyquery import PyQuery as pqdoc = pq(html)# 查找a 标签的上级标签，并包含a标签本身内容# a 标签父元素是li 标签items = doc(&apos;a&apos;)li = items.parent()print(type(li))print(li) 12345&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 123# 显示a 标签块外的的所有父节点内容，显示三段父节点内的数据（a标签外有三层嵌套）parent = items.parents(&apos;a&apos;)print(parent) 123# 如果使用祖先节点标识，则只会显示一段数据parent = items.parents(&apos;祖先类 祖先id &apos;)print(parent) 兄弟元素使用siblings方法，查找兄弟元素，包含自己 123456from pyquery import PyQuery as pqdoc = pq(html)# 多个类之间使用. ,同时包含item-0和active 类的标签,获取一个内容li = doc(&apos;.list .item-0.active&apos;)# 查找兄弟元素print(li.siblings()) 结果: 1234&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 筛选兄弟元素的方式 123456from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.list .item-0.active&apos;)# 查找兄弟元素中含有active 的元素print(li.siblings(&apos;.active&apos;)) 结果 1&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 7.3.3 遍历单个元素使用CSS 选择器，通过标签，id和类进行精准定位 12345from pyquery import PyQuery as pqdoc = pq(html)# 指定好标签的类或id，进行精准定位li = doc(&apos;.item-0.active&apos;)print(li) 多个元素如果是多个元素，使用遍历的方式 1234567from pyquery import PyQuery as pqdoc = pq(html)lis = doc(&apos;li&apos;).items() # 生成一个生成器，内容的是Pyquery 对象print(type(lis))for li in lis: print(li) # li是Pyquery 对象 结果 12345678910&lt;class &apos;generator&apos;&gt;&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 7.4 获取信息7.4.1 获取标签属性的值attr(‘规定要获取其值的属性’) 12345678from pyquery import PyQuery as pqdoc = pq(html)# 定位到一个a 标签a = doc(&apos;.item-0.active a&apos;)print(a)# href 属性的值，2种方法print(a.attr(&apos;href&apos;))print(a.attr.href) 结果 123&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;link3.htmllink3.html 7.4.2 获取文本text() 12345from pyquery import PyQuery as pqdoc = pq(html)a = doc(&apos;.item-0.active a&apos;)print(a)print(a.text()) 结果 12&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;third item 7.4.3 获取HTMLhtml() 获取html内容，包含标签，text 只获取文本 12345from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.item-0.active&apos;)print(li)print(li.html()) 结果 123&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; 7.5 DOM 操作对HTML节点的操作，下面的只是一部分 addClass、removeClassaddClass：增加类removeClass：删除类 12345678from pyquery import PyQuery as pqdoc = pq(html)li = doc(&apos;.item-0.active&apos;)print(&quot;初始:&quot; + str(li))li.removeClass(&apos;active&apos;)print(&quot;删除类后:&quot; + str(li))li.addClass(&apos;active&apos;)print(&quot;增加新类:&quot;+ str(li)) 结果 1234567891011初始:&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 删除类后:&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 增加新类:&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;``` ### attr、cssattr: 修改/增加标签属性 css: 修改/增加CSS 属性 from pyquery import PyQuery as pqdoc = pq(html)li = doc(‘.item-0.active’)print(li)li.attr(‘name’, ‘link’)print(li)li.css(‘font-size’, ‘14px’)print(li) 1结果 third item third item third item 123### remove删除标签 html = ‘’’ Hello, World This is a paragraph. ''' from pyquery import PyQuery as pq doc = pq(html) wrap = doc('.wrap') print(wrap.text()) # 删除p 标签 wrap.find('p').remove() print(wrap.text()) 1结果: Hello, World This is a paragraph.Hello, World 12### 其他DOM方法 http://pyquery.readthedocs.io/en/latest/api.html 12## 7.6 伪类选择器 from pyquery import PyQuery as pqdoc = pq(html)li = doc(‘li:first-child’)print(li)li = doc(‘li:last-child’)print(li)li = doc(‘li:nth-child(2)’)print(li)li = doc(‘li:gt(2)’)print(li)li = doc(‘li:nth-child(2n)’)print(li)li = doc(‘li:contains(second)’)print(li) 1结果 first item fifth item second item fourth item fifth item second item fourth item second item ``` 更多CSS选择器可以查看 http://www.w3school.com.cn/css/index.asp","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"06 BeautifulSoup 模块的使用","date":"2019-10-27T11:38:00.000Z","path":"2019/10/27/06 BeautifulSoup 模块的使用/","text":"简介: BeautifulSoup 模块的使用 6.1 BeautifulSoup 简介简单来说，BeautifulSoup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下： 12Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。 Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.你可能在寻找 Beautiful Soup3 的文档,Beautiful Soup 3 目前已经停止开发,官网推荐在现在的项目中使用Beautiful Soup 4。 6.1.1 安装1pip3 install beautifulsoup4 6.1.2 解析器Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用 Python默认的解析器，lxml 解析器更加强大，速度更快，推荐安装。 1pip3 install lxml 另一个可供选择的解析器是纯Python实现的 html5lib , html5lib的解析方式与浏览器相同,可以选择下列方法来安装html5lib: 1pip install html5lib 解析器对比： 解析器 使用方法 优势 劣势 Python 标准库 BeautifulSoup(markup,”html.parser”) Python的内置标准库、执行速度适中、文档容错能力强 Python 2.7.3 or 3.2.2 ) 前的版本中文容错能力差 lxml HTML 解析器 BeautifulSoup(markup,”lxml”) 速度快、文档容错能力强 需要安装C语言库 lxml XML 解析器 BeautifulSoup(markup,”xml”) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup,”html5lib”) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式文档 速度慢、不依赖外部扩展 beautifulsoup官方文档 6.1.3 简单使用下面的一段HTML代码将作为例子被多次用到.这是 爱丽丝梦游仙境的 的一段内容(以后内容中简称为 爱丽丝 的文档): 12345678910111213html_doc = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\" 使用BeautifulSoup解析这段代码,能够得到一个 BeautifulSoup 的对象BeautifulSoup的第一个参数是html代码，第二个参数 html.parser 必须填，html.parser简单的HTML和XHTML语法分析器，将前面的html代码交给html.parser进行解析，如果不写，则会报错；第二个参数可以不是html.parser，当必须是文档的解析器，后面的lxml； 12from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, &apos;html.parser&apos;) 从文档中找到所有标签的链接: 12for link in soup.find_all(&apos;a&apos;): print(link.get(&apos;href&apos;)) 从文档中获取所有文字内容(不带html标签): 1print(soup.get_text()) 6.2 标签对象 通俗点讲就是 HTML 中的一个个标签，Tag 对象与XML或HTML原生文档中的tag相同: 1234soup = BeautifulSoup(&apos;&lt;b class=&quot;boldest&quot;&gt;Extremely bold&lt;/b&gt;&apos;,&quot;html.parser&quot;)tag = soup.b # 获取b标签，成为一个标签对象print(type(tag))# &lt;class &apos;bs4.element.Tag&apos;&gt; 6.2.1 Tag的名字soup对象再以爱丽丝梦游仙境的html_doc为例，操作文档树最简单的方法就是告诉它你想获取的tag的name.如果想获取 标签,只要用 soup.head : 12345soup.head# &lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;soup.title# &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; 这是个获取tag的小窍门,可以在文档树的tag中多次调用这个方法.下面的代码可以获取标签中的第一个标签: 12soup.body.b# &lt;b&gt;The Dormouse&apos;s story&lt;/b&gt; 通过点取属性的方式只能获得当前名字的第一个tag: 12soup.a# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt; 如果想要得到所有的标签,或是通过名字得到比一个tag更多的内容的时候,就需要用到 Searching the tree 中描述的方法,比如: find_all() 1234soup.find_all(&apos;a&apos;)# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;] 我们可以利用 soup加标签名轻松地获取这些标签的内容，注意，它查找的是在所有内容中的第一个符合要求的标签。 6.2.2 Tag的name和attributes属性Tag有很多方法和属性,现在介绍一下tag中最重要的属性: name和attributes 每个tag都有自己的名字,通过”.name” 来获取: 12345678tag.name# u&apos;b&apos;tag[&apos;class&apos;]# u&apos;boldest&apos;tag.attrs# &#123;u&apos;class&apos;: u&apos;boldest&apos;&#125; tag的属性可以被添加,删除或修改. 再说一次, tag的属性操作方法与字典一样 1234567891011121314tag[&apos;class&apos;] = &apos;verybold&apos;tag[&apos;id&apos;] = 1tag# &lt;blockquote class=&quot;verybold&quot; id=&quot;1&quot;&gt;Extremely bold&lt;/blockquote&gt;del tag[&apos;class&apos;]del tag[&apos;id&apos;]tag# &lt;blockquote&gt;Extremely bold&lt;/blockquote&gt;tag[&apos;class&apos;]# KeyError: &apos;class&apos;print(tag.get(&apos;class&apos;))# None 标签对象的文本获取 12345678910111213141516171819202122232425# 4、获取标签的内容print(soup.p.string) # p下的文本只有一个时，取到，否则为Noneprint(soup.p.strings) #拿到一个生成器对象, 取到p下所有的文本内容print(soup.p.text) #取到p下所有的文本内容for line in soup.stripped_strings: #去掉空白 print(line)&apos;&apos;&apos;如果tag包含了多个子节点,tag就无法确定 .string 方法应该调用哪个子节点的内容, .string 的输出结果是 None，如果只有一个子节点那么就输出该子节点的文本，比如下面的这种结构， soup.p.string 返回为None, 但soup.p.strings就可以找到所有文本 &lt;p id=&apos;list-1&apos;&gt; 哈哈哈哈 &lt;a class=&apos;sss&apos;&gt; &lt;span&gt; &lt;h1&gt;aaaa&lt;/h1&gt; &lt;/span&gt; &lt;/a&gt; &lt;b&gt;bbbbb&lt;/b&gt;&lt;/p&gt;&apos;&apos;&apos; 6.3 遍历文档树12345678910111213141516171819202122232425262728# 1、嵌套选择print(soup.head.title.string)print(soup.body.a.string)#2、子节点、子孙节点print(soup.p.contents) #p下所有子节点print(soup.p.children) #得到一个迭代器,包含p下所有子节点for i,child in enumerate(soup.p.children): print(i,child)print(soup.p.descendants) #获取子孙节点,p下所有的标签都会选择出来for i,child in enumerate(soup.p.descendants): print(i,child)#3、父节点、祖先节点print(soup.a.parent) #获取a标签的父节点print(soup.a.parents) #找到a标签所有的祖先节点，父亲的父亲，父亲的父亲的父亲...#4、兄弟节点print(&apos;=====&gt;&apos;)print(soup.a.next_sibling) #下一个兄弟print(soup.a.previous_sibling) #上一个兄弟print(list(soup.a.next_siblings)) #下面的兄弟们=&gt;生成器对象print(soup.a.previous_siblings) #上面的兄弟们=&gt;生成器对象 6.4 搜索文档树BeautifulSoup定义了很多搜索方法,这里着重介绍2个: find() 和 find_all() .其它方法的参数和用法类似 1、五种过滤器五种过滤器: 字符串、正则表达式、列表、True、方法 123456789101112131415161718192021222324252627282930313233343536373839404142#搜索文档树：BeautifulSoup定义了很多搜索方法,这里着重介绍2个: find() 和 find_all() .其它方法的参数和用法类似html_doc = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p id=\"my p\" class=\"title\"&gt;&lt;b id=\"bbb\" class=\"boldest\"&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup=BeautifulSoup(html_doc,'lxml')#五种过滤器: 字符串、正则表达式、列表、True、方法#1.1、字符串：即标签名print(soup.find_all('b'))#1.2、正则表达式import reprint(soup.find_all(re.compile('^b'))) #找出b开头的标签，结果有body和b标签#1.3、列表：如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有&lt;a&gt;标签和&lt;b&gt;标签:print(soup.find_all(['a','b']))#1.4、True：可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点print(soup.find_all(True))for tag in soup.find_all(True): print(tag.name)#1.5、方法:如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数 ,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 Falsedef has_class_but_no_id(tag): return tag.has_attr('class') and not tag.has_attr('id')print(soup.find_all(has_class_but_no_id)) 2、find_all()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#2、find_all( name , attrs , recursive , text , **kwargs )#2.1、name: 搜索name参数的值可以使任一类型的 过滤器 ,字符窜,正则表达式,列表,方法或是 True .print(soup.find_all(name=re.compile('^t')))#2.2、keyword: key=value的形式，value可以是过滤器：字符串 , 正则表达式 , 列表, True .print(soup.find_all(id=re.compile('my')))print(soup.find_all(href=re.compile('lacie'),id=re.compile('\\d'))) #注意类要用class_print(soup.find_all(id=True)) #查找有id属性的标签# 有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性:data_soup = BeautifulSoup('&lt;div data-foo=\"value\"&gt;foo!&lt;/div&gt;','lxml')# data_soup.find_all(data-foo=\"value\") #报错：SyntaxError: keyword can't be an expression# 但是可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag:print(data_soup.find_all(attrs=&#123;\"data-foo\": \"value\"&#125;))# [&lt;div data-foo=\"value\"&gt;foo!&lt;/div&gt;]#2.3、按照类名查找，注意关键字是class_，class_=value,value可以是五种选择器之一print(soup.find_all('a',class_='sister')) #查找类为sister的a标签print(soup.find_all('a',class_='sister ssss')) #查找类为sister和sss的a标签，顺序错误也匹配不成功print(soup.find_all(class_=re.compile('^sis'))) #查找类为sister的所有标签#2.4、attrsprint(soup.find_all('p',attrs=&#123;'class':'story'&#125;))#2.5、text: 值可以是：字符，列表，True，正则print(soup.find_all(text='Elsie'))print(soup.find_all('a',text='Elsie'))#2.6、limit参数:如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果print(soup.find_all('a',limit=2))#2.7、recursive:调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False .print(soup.html.find_all('a'))print(soup.html.find_all('a',recursive=False))'''像调用 find_all() 一样调用tagfind_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. BeautifulSoup 对象和 tag 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:soup.find_all(\"a\")soup(\"a\")这两行代码也是等价的:soup.title.find_all(text=True)soup.title(text=True)''' 3、find()12345678910111213141516171819#3、find( name , attrs , recursive , text , **kwargs )find_all() 方法将返回文档中符合条件的所有tag,尽管有时候我们只想得到一个结果.比如文档中只有一个&lt;body&gt;标签,那么使用 find_all() 方法来查找&lt;body&gt;标签就不太合适, 使用 find_all 方法并设置 limit=1 参数不如直接使用 find() 方法.下面两行代码是等价的:soup.find_all('title', limit=1)# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]soup.find('title')# &lt;title&gt;The Dormouse's story&lt;/title&gt;唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.find_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None .print(soup.find(\"nosuchtag\"))# Nonesoup.head.title 是 tag的名字 方法的简写.这个简写的原理就是多次调用当前tag的 find() 方法:soup.head.title# &lt;title&gt;The Dormouse's story&lt;/title&gt;soup.find(\"head\").find(\"title\")# &lt;title&gt;The Dormouse's story&lt;/title&gt; 4、其他方法1#见官网:https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#find-parents-find-parent 5、css选择器我们在写 CSS 时，标签名不加任何修饰，类名前加点，id名前加 #，在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list （1）通过标签名查找 12print(soup.select(&quot;title&quot;)) #[&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;]print(soup.select(&quot;b&quot;)) #[&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;] （2）通过类名查找 12345678print(soup.select(\".sister\")) '''[&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;, &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;, &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]''' （3）通过 id 名查找 12print(soup.select(&quot;#link1&quot;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;] （4）组合查找 组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开 1234567891011121314print(soup.select(\"p #link2\"))#[&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;]直接子标签查找print(soup.select(\"p &gt; #link2\"))# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;]（5）属性查找查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。print(soup.select(\"a[href='http://example.com/tillie']\"))#[&lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容： 12345678for title in soup.select('a'): print (title.get_text())'''ElsieLacieTillie''' 五 修改文档树修改文档树","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"05 基础模块之Request模块和Response响应","date":"2019-10-27T11:31:00.000Z","path":"2019/10/27/05-基础模块之Request模块和Response响应/","text":"简介:介绍request 模块和Response对象 5、request 模块请求方式：主要有GET和POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等 请求URL：URL全称统一资源定位符，每个网页、图片、视频使用URL来唯一确定。 请求头：包含请求时头部信息，如User-Agent、Host、Cookies等信息。 请求体: 请求时额外携带的数据，如表单提交时的表单数据。 Requests 是用Python语言编写的，基于urllib库，比urllib库更加好用的Http库 安装 1pip install requests 5.1 基本语法requests模块支持的请求： 1234567import requestsrequests.get(\"http://httpbin.org/get\")requests.post(\"http://httpbin.org/post\")requests.put(\"http://httpbin.org/put\")requests.delete(\"http://httpbin.org/delete\")requests.head(\"http://httpbin.org/get\")requests.options(\"http://httpbin.org/get\") 5.2 get请求5.2.1、基本请求12345import requestsresponse=requests.get('https://www.jd.com/',) with open(\"jd.html\",\"wb\") as f: f.write(response.content) 返回的是response对象，response.content 返回的是二进制对象，所有使用wb模式写; 如果是response.content 返回的是文本字符串，使用”w” 模式写。 ==注意网站和本地字符串编码，有的网站是gbk，有的是utf-8== 显示网页内容，使用text方法 1234import requestsresponse=requests.get('https://www.jd.com/')print(response.text) # 显示京东页面 5.2.2、含参数的请求request 请求时使用 params参数 123import requestsresponse=requests.get('https://s.taobao.com/search?q=手机') response=requests.get('https://s.taobao.com/search',params=&#123;\"q\":\"美女\"&#125;) 123456789import requestsdata = &#123; 'name': 'alex', 'age': 23&#125;response=requests.get('https://httpbin.org/get', params=data)print(response.text) 结果 1234567891011121314&#123; &quot;args&quot;: &#123; &quot;age&quot;: &quot;23&quot;, &quot;name&quot;: &quot;alex&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.22.0&quot; &#125;, &quot;origin&quot;: &quot;111.196.243.241, 111.196.243.241&quot;, &quot;url&quot;: &quot;https://httpbin.org/get?name=alex&amp;age=23&quot; # url 带参数&#125; 5.2.3、含请求头的请求request 请求时使用 headers参数User-Agent是请求时，设备的信息；一些网站，做些反爬，简单的做法，判断是否有User-Agent ,没有则不会允许访问； 示例1: 123456import requestsresponse=requests.get('https://dig.chouti.com/', headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36', &#125; ) 如果不加请求头，抽屉网，返回你的请求带不合法参数，已被拦截！请误恶意提交 的页面 示例2: 1234567891011121314151617181920212223242526import requestsres=requests.post('https://www.lagou.com/jobs/positionAjax.json', headers=&#123; 'Referer':\"https://www.lagou.com/jobs/list_python\", 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36', &#125;, data=&#123; 'first':True, 'pn':2, 'kd':'java高级开发' &#125;, params=&#123; 'gj': '3年及以下', 'px': 'default', 'yx': '25k-50k', 'city': '北京', 'needAddtionalResult': False, 'isSchoolJob': 0 &#125; )comapines_list=r6.json()print(comapines_list) 5.2.4、 含cookies请求12345678import uuidimport requestsurl = 'http://httpbin.org/cookies'cookies = dict(sbid=str(uuid.uuid4()))res = requests.get(url, cookies=cookies)print(res.json()) 5.2.5、 request.session()123456789import requests# res=requests.get(\"https://www.zhihu.com/explore\")# print(res.cookies.get_dict())session=requests.session()res1=session.get(\"https://www.zhihu.com/explore\")print(session.cookies.get_dict())res2=session.get(\"https://www.zhihu.com/question/30565354/answer/463324517\",cookies=&#123;\"abs\":\"123\"&#125;) 会话维持模拟登陆 123456import requests# http://httpbin.org/coookies/set 是设置cookie的方式requests.get('http://httpbin.org/coookies/set/number/12345') # 访问一次页面# http://httpbin.org/cookies 是获取cookies的方式response = requests.get('http://httpbin.org/cookies') # 获取访问该网站的cookies值print(response.text) 结果 123&#123; &quot;cookies&quot;: &#123;&#125; # cookie 没有被保存&#125; 使用session 维持会话,如果不使用session ，上面的2次请求，相当从2个浏览器器访问地址，使用session ,保持了连接，保持了cookie, 2次请求相当在一个浏览器上发起的。 12345678import requestss=requests.session()# 第一次连接s.get('http://httpbin.org/cookies/set/number/12345')# 连接没有中断 ，再次获取cookie 值，response = s.get('http://httpbin.org/cookies')print(response.text) 结果 12345&#123; &quot;cookies&quot;: &#123; &quot;number&quot;: &quot;12345&quot; &#125;&#125; 5.3 post请求5.3.1 data参数requests.post()用法与requests.get()完全一致，特殊的是requests.post()多了一个data参数，用来存放请求体数据 1response=requests.post(\"http://httpbin.org/post\",params=&#123;\"a\":\"10\"&#125;, data=&#123;\"name\":\"yuan\"&#125;) 5.3.2 发送json数据data 默认是form 提交；json 是json 数据； 12345678import requestsres1=requests.post(url='http://httpbin.org/post', data=&#123;'name':'yuan'&#125;) #没有指定请求头,#默认的请求头:application/x-www-form-urlencoedprint(res1.json())res2=requests.post(url='http://httpbin.org/post',json=&#123;'age':\"22\",&#125;) #默认的请求头:application/json)print(res2.json())` 下面是提交from和json数据，响应有所不同 1234res1=requests.post(&quot;http://httpbin.org/post&quot;,headers=&#123;&#125;,cookies=&#123;&#125;,params=&#123;&quot;a&quot;:&quot;1&quot;&#125;,data=&#123;&quot;a&quot;:1234&#125;)res2=requests.post(&quot;http://httpbin.org/post&quot;,headers=&#123;&#125;,cookies=&#123;&#125;,params=&#123;&quot;a&quot;:&quot;1&quot;&#125;,json=&#123;&quot;a&quot;:1234&#125;)print(res1.text)print(res2.text) res1.txt : 提交的数据在from表单内，data 内无 12345678910111213141516171819202122&#123; &quot;args&quot;: &#123; &quot;a&quot;: &quot;1&quot; &#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123; &quot;a&quot;: &quot;1234&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;6&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;111.196.241.53&quot;, &quot;url&quot;: &quot;http://httpbin.org/post?a=1&quot;&#125; res2.txt 提交的数据是json 数据类型，数据存放在data 内，from 内没有数据，json 字段内也有数据； 12345678910111213141516171819202122&#123; &quot;args&quot;: &#123; &quot;a&quot;: &quot;1&quot; &#125;, &quot;data&quot;: &quot;&#123;\\&quot;a\\&quot;: 1234&#125;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot; &#125;, &quot;json&quot;: &#123; &quot;a&quot;: 1234 &#125;, &quot;origin&quot;: &quot;111.196.241.53&quot;, &quot;url&quot;: &quot;http://httpbin.org/post?a=1&quot;&#125; 5.3.3 文件上传open: 打卡文件并返回流 12345import requestsfiles = &#123;'file':open('a.png','rb')&#125;response = requests.post('https://httpbin.org/post',files=files)print(response.text) 5.4 Response对象响应状态: 200成功、301跳转、404 无页面、502 服务器错误 响应头: 如内容类型、内容长度、服务器信息、设置Cookie等 响应体: 最主要部分，包含请求资源的内容，如HTML、图片二进制数据等。 5.4.1 常见属性123456789101112131415import requestsrespone=requests.get('https://sh.lianjia.com/ershoufang/')# respone属性print(respone.text) # 字符串数据print(respone.content) # 二进制数据print(respone.status_code) # 状态吗print(respone.headers) # 请求头print(respone.cookies) # cookies 信息print(respone.cookies.get_dict()) # print(respone.cookies.items())print(respone.url) # 显示url print(respone.history) # 如果重定向，历史网址存在里面print(respone.encoding) # 编码 response.iter_content() # 内容可迭代，方便下载 5.4.2 编码问题12345import requestsresponse=requests.get('http://www.autohome.com/news')#response.encoding='gbk' #汽车之家网站返回的页面内容为gb2312编码的，而requests的默认编码为ISO-8859-1，如果不设置成gbk则中文乱码with open(\"res.html\",\"w\") as f: f.write(response.text) 1234567import requestsresponse = requests.get('url')response.encoding = response.apparent_encoding这样可以解决99.99%的乱码爬取网页中response.encoding:如果header中不存在charset,则默认编码为ISO-8859-1而response.apparent_encoding他是一个备用编码方式,他会根据内容自动匹配给你个合适的编码方式那为什么只能99.99%而不是100%解决,还有可能就是网站开发者故意放2种编码进去隐藏一些重要信息 5.4.3 下载二进制文件（图片，视频，音频）123456import requestsresponse=requests.get('http://bangimg1.dahe.cn/forum/201612/10/200447p36yk96im76vatyk.jpg')with open(\"res.png\",\"wb\") as f: # f.write(response.content) # 比如下载视频时,如果视频100G,用response.content然后一下子写到文件中是不合理的 for line in response.iter_content(): # 将内容可迭代，不是一下下载所有的内容 f.write(line) 网上随便找张图片，复制链接地址，下载到本地 12345import requestsresponse = requests.get('https://c-ssl.duitang.com/uploads/item/201704/21/20170421083329_3cxt8.thumb.700_0.png')with open(\"a.png\",\"wb\") as f: # 以二进制的方式写入 f.write(response.content) # 图片小，可以一下下载所有内容 5.4.4 解析json数据 1234567import requestsimport jsonresponse=requests.get('http://httpbin.org/get')res1=json.loads(response.text) #太麻烦res2=response.json() #直接获取json数据print(res1==res2) 12345678import jsonimport requestsresponse = requests.get('https://httpbin.org/get')print(type(response.text)) # 获取的是字符串类型print(response.json()) # 进行json序列化print(json.loads(response.text)) # 本地json 对字符串进行序列化print(type(response.json())) # requests 的json后是一个字典，和使用本地json模块是相同的效果 结果 1234&lt;class 'str'&gt;&#123;'args': &#123;&#125;, 'headers': &#123;'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.22.0'&#125;, 'origin': '111.196.243.241, 111.196.243.241', 'url': 'https://httpbin.org/get'&#125;&#123;'args': &#123;&#125;, 'headers': &#123;'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.22.0'&#125;, 'origin': '111.196.243.241, 111.196.243.241', 'url': 'https://httpbin.org/get'&#125;&lt;class 'dict'&gt; 5.4.5 Redirection and History默认情况下，除了 HEAD, Requests 会自动处理所有重定向。可以使用响应对象的 history 方法来追踪重定向。Response.history 是一个 Response 对象的==列表==，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。 1234567891011&gt;&gt;&gt; r = requests.get('http://github.com')&gt;&gt;&gt; r.url # response 对象的url 'https://github.com/'&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; r.history[&lt;Response [301]&gt;]&gt;&gt;&gt; r.history[0][&lt;Response [301]&gt;]&gt;&gt;&gt; r.history[0].url # 查看历史response对象的url'http://github.com/' 另外，还可以通过 allow_redirects 参数禁用重定向处理： 12345&gt;&gt;&gt; r = requests.get('http://github.com', allow_redirects=False)&gt;&gt;&gt; r.status_code301&gt;&gt;&gt; r.history[] 5.4.6 状态码判断requests.codes.ok 都对应于HTTP状态码200。 response.status_code 查看状态码 123import requestsresponse = requests.get('http://www.baidu.com')exit() if not response.status_code == requests.codes.ok else print('Request Successfully') status_code.py 源码内包含的状态码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# Informational.100: (&apos;continue&apos;,),101: (&apos;switching_protocols&apos;,),102: (&apos;processing&apos;,),103: (&apos;checkpoint&apos;,),122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;),200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\\\o/&apos;, &apos;✓&apos;),201: (&apos;created&apos;,),202: (&apos;accepted&apos;,),203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),204: (&apos;no_content&apos;,),205: (&apos;reset_content&apos;, &apos;reset&apos;),206: (&apos;partial_content&apos;, &apos;partial&apos;),207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),208: (&apos;already_reported&apos;,),226: (&apos;im_used&apos;,),# Redirection.300: (&apos;multiple_choices&apos;,),301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\\\o-&apos;),302: (&apos;found&apos;,),303: (&apos;see_other&apos;, &apos;other&apos;),304: (&apos;not_modified&apos;,),305: (&apos;use_proxy&apos;,),306: (&apos;switch_proxy&apos;,),307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),308: (&apos;permanent_redirect&apos;, &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0# Client Error.400: (&apos;bad_request&apos;, &apos;bad&apos;),401: (&apos;unauthorized&apos;,),402: (&apos;payment_required&apos;, &apos;payment&apos;),403: (&apos;forbidden&apos;,),404: (&apos;not_found&apos;, &apos;-o-&apos;),405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),406: (&apos;not_acceptable&apos;,),407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),408: (&apos;request_timeout&apos;, &apos;timeout&apos;),409: (&apos;conflict&apos;,),410: (&apos;gone&apos;,),411: (&apos;length_required&apos;,),412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),413: (&apos;request_entity_too_large&apos;,),414: (&apos;request_uri_too_large&apos;,),415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),417: (&apos;expectation_failed&apos;,),418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),421: (&apos;misdirected_request&apos;,),422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),423: (&apos;locked&apos;,),424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),428: (&apos;precondition_required&apos;, &apos;precondition&apos;),429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),444: (&apos;no_response&apos;, &apos;none&apos;),449: (&apos;retry_with&apos;, &apos;retry&apos;),450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),499: (&apos;client_closed_request&apos;,),# Server Error.500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\\\&apos;, &apos;✗&apos;),501: (&apos;not_implemented&apos;,),502: (&apos;bad_gateway&apos;,),503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),504: (&apos;gateway_timeout&apos;,),505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),506: (&apos;variant_also_negotiates&apos;,),507: (&apos;insufficient_storage&apos;,),509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),510: (&apos;not_extended&apos;,),511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;), 5.4.7 获取Cookie12345import requestsresponse = requests.get(\"https://www.baidu.com\")print(response.cookies)for key, value in response.cookies.items(): print(key + \"=\" + value) 结果 123456789101112131415161718192021&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;BDORZ=27315``` ### 5.5 其他 #### 5.5.1 证书验证如果使用request 请求时，如果是https协议，会进行一个证书验证的过程，可能会报SSLErroL的错误#### 5.5.2 设置代理```pythonimport requests# 指定代理IPproxies = &#123; &apos;http&apos;: &apos;http://localhost:8888&apos;, &apos;https&apos;: &apos;http://localhost:8888&apos;&#125;url = &apos;http://www.baidu.com&apos;# proxies ：设置代理的参数，下面的请求使用了代理requests.post(url, proxies=proxies, verify=False) #verify是否验证服务器的SSL证书 如果代理有用户名和密码，通过如下的方式进行设置 123456789import requestsproxies = &#123; 'http': 'http://user:password@127.0.0.1:8888', &#125;url = 'http://www.baidu.com'response = requests.get(url, proxies=proxies)print(response.status_code) 如果是socks代理，需要先安装一个模块后，进行测试 1安装 pip install requests[socks] 1234567891011import requests# 如果代理有用户名和密码，通过如下的方式进行设置proxies = &#123; 'http': 'socks5://localhost:8888', 'https': 'socks5://localhost:8888'&#125;url = 'http://www.baidu.com'response = requests.get(url, proxies=proxies)print(response.status_code) 5.5.3 设置超时1234import requestsresponse = requests.get('http://www.baidu.com',timeout=0.1) # 设置超时时间0.1sprint(response.status_code) 5.5.4 认证设置1234import requestsr = requests.get('http://120.27.34.24:9001', auth=('user', '123'))print(r.status_code) 1234import requestsr = requests.get(&apos;http://120.27.34.24:9001&apos;, auth=(&apos;user&apos;, &apos;123&apos;))print(r.status_code) 5.5.5 异常处理1234567891011import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout')except ConnectionError: print('Connection error')except RequestException: print('Error') 应用案例1、模拟GitHub登录，获取登录信息1、github 的cook 是保存在页面当中，有个标签”name=authenticity_token”；2、github登录后，网址还是github.com；在登录时，后端会返回一个session 信息，之后自动登录到自己的页面中；这个过程中，会有个自动跳转，无法捕获session ;chrome有preserve log 这个选项,会报存跳转之前的数据；从中获取cook信息；3、请求访问 模拟github登录分为三步： 1 get 请求访问 http://github.com/login2 post 请求访问 https://github.com/session3 get 请求访问 http://github.com 1234567891011121314151617181920212223242526272829303132import requestsimport re#请求1:r1=requests.get('https://github.com/login')r1_cookie=r1.cookies.get_dict() #拿到初始cookie(未被授权)authenticity_token=re.findall(r'name=\"authenticity_token\".*?value=\"(.*?)\"',r1.text)[0] #从页面中拿到CSRF TOKENprint(\"authenticity_token\",authenticity_token)#第二次请求：带着初始cookie和TOKEN发送POST请求给登录页面，带上账号密码data=&#123; 'commit':'Sign in', 'utf8':'✓', 'authenticity_token':authenticity_token, 'login':'xxxx@163.com', # Github 的用户名和密码 'password':'xxxxx'&#125;# 请求2:r2=requests.post('https://github.com/session', data=data, cookies=r1_cookie, # allow_redirects=False )print(r2.status_code) #200print(r2.url) #看到的是跳转后的页面:https://github.com/print(r2.history) #看到的是跳转前的response:[&lt;Response [302]&gt;]print(r2.history[0].text) #看到的是跳转前的response.textwith open(\"result.html\",\"wb\") as f: f.write(r2.content) 2、爬取豆瓣电影信息一段豆瓣T250 的html 1234567891011121314151617181920212223242526272829303132333435363738&lt;li&gt; &lt;div class=\"item\"&gt; &lt;div class=\"pic\"&gt; &lt;em class=\"\"&gt;1&lt;/em&gt; &lt;a href=\"https://movie.douban.com/subject/1292052/\"&gt; &lt;img width=\"100\" alt=\"肖申克的救赎\" src=\"https://img3.doubanio.com/view/photo/s_ratio_poster/public/p480747492.webp\" class=\"\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=\"info\"&gt; &lt;div class=\"hd\"&gt; &lt;a href=\"https://movie.douban.com/subject/1292052/\" class=\"\"&gt; &lt;span class=\"title\"&gt;肖申克的救赎&lt;/span&gt; &lt;span class=\"title\"&gt;&amp;nbsp;/&amp;nbsp;The Shawshank Redemption&lt;/span&gt; &lt;span class=\"other\"&gt;&amp;nbsp;/&amp;nbsp;月黑高飞(港) / 刺激1995(台)&lt;/span&gt; &lt;/a&gt; &lt;span class=\"playable\"&gt;[可播放]&lt;/span&gt; &lt;/div&gt; &lt;div class=\"bd\"&gt; &lt;p class=\"\"&gt; 导演: 弗兰克·德拉邦特 Frank Darabont&amp;nbsp;&amp;nbsp;&amp;nbsp;主演: 蒂姆·罗宾斯 Tim Robbins /...&lt;br&gt; 1994&amp;nbsp;/&amp;nbsp;美国&amp;nbsp;/&amp;nbsp;犯罪 剧情 &lt;/p&gt; &lt;div class=\"star\"&gt; &lt;span class=\"rating5-t\"&gt;&lt;/span&gt; &lt;span class=\"rating_num\" property=\"v:average\"&gt;9.6&lt;/span&gt; &lt;span property=\"v:best\" content=\"10.0\"&gt;&lt;/span&gt; &lt;span&gt;1151286人评价&lt;/span&gt; &lt;/div&gt; &lt;p class=\"quote\"&gt; &lt;span class=\"inq\"&gt;希望让人自由。&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/li&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import requestsimport reimport jsonimport timefrom concurrent.futures import ThreadPoolExecutorpool=ThreadPoolExecutor(50)def getPage(url): response=requests.get(url) return response.textdef parsePage(res): # 通过有名分组的方式获取数据，re.S 模式下的\".\" 能够识别换行符 com=re.compile('&lt;div class=\"item\"&gt;.*?&lt;div class=\"pic\"&gt;.*?&lt;em .*?&gt;(?P&lt;id&gt;\\d+).*?&lt;span class=\"title\"&gt;(?P&lt;title&gt;.*?)&lt;/span&gt;' '.*?&lt;span class=\"rating_num\" .*?&gt;(?P&lt;rating_num&gt;.*?)&lt;/span&gt;.*?&lt;span&gt;(?P&lt;comment_num&gt;.*?)评价&lt;/span&gt;',re.S) iter_result=com.finditer(res) return iter_resultdef gen_movie_info(iter_result): for i in iter_result: yield &#123; \"id\":i.group(\"id\"), \"title\":i.group(\"title\"), \"rating_num\":i.group(\"rating_num\"), \"comment_num\":i.group(\"comment_num\"), &#125;def stored(gen): with open(\"move_info.txt\",\"a\",encoding=\"utf8\") as f: for line in gen: data=json.dumps(line,ensure_ascii=False) f.write(data+\"\\n\")def spider_movie_info(url): res=getPage(url) iter_result=parsePage(res) gen=gen_movie_info(iter_result) stored(gen)def main(num): url='https://movie.douban.com/top250?start=%s&amp;filter='%num pool.submit(spider_movie_info,url) #spider_movie_info(url)if __name__ == '__main__': before=time.time() count=0 for i in range(10): main(count) count+=25 after=time.time() print(\"总共耗费时间：\",after-before) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import requestsimport redef get_html(url): response=requests.get(url) return response.textdef parser_html(res_html): #print(res_html) ret=re.findall('&lt;div class=\"item\"&gt;.*?&lt;a href=\"(.*?)\"&gt;.*?&lt;span class=\"title\"&gt;(.*?)&lt;/span&gt;.*?&lt;span class=\"rating_num\".*?&gt;(.*?)&lt;/span&gt;.*?&lt;span&gt;(.*?)人评价&lt;/span&gt;',res_html,re.S) return retdef store(data): with open(\"douban.txt\",\"a\",encoding=\"utf8\") as f: for item in data: s=\" \".join(list(item)) line=s+\"\\n\" print(line) f.write(line)if __name__ == '__main__': index=0 for i in range(10): url=\"https://movie.douban.com/top250?start=%s&amp;filter=\"%index # 1 爬取资源 res_html=get_html(url) # 2 解析资源 data=parser_html(res_html) # 3 持久化 store(data) index+=25 3、爬取拉钩网经过查找发现，拉钩网的职位信息是是通过ajax 获取的（主页面没有数据，有个ajax 数据）； 在浏览器上点击XHR(ajax对象)，过滤出该ajax 的数据，找出有职位的数据ajax；从该ajax 的数据上看：Headers 》request headers 下Referer:https://www.lagou.com/jobs/list_pyton?labelWords=&amp;fromSearch=true&amp;suginput=得知中间跳转了一个网址 https://www.lagou.com/jobs/list_pytonForm Data: # post 请求提交的数据，如果不加，返回的是网站默认的职位数据，不是pythonfirst:truepn:1kd:pyton 12345678910111213141516171819import requestssession=requests.session() # 使用session 获取cookie session.get(\"https://www.lagou.com/\") # 第一次访问主站获取携带cookiesession.get(\"https://www.lagou.com/jobs/list_python\") # 获取进行跳转的的cokieres=session.post(\"https://www.lagou.com/jobs/positionAjax.json?px=default&amp;city=%E5%8C%97%E4%BA%AC&amp;needAddtionalResult=false\", headers=&#123; \"Referer\": \"https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=\", \"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" &#125;, data=&#123; \"first\": \"true\", \"pn\":1, \"kd\":\"python\", #查询的条件，如果不加，返回的json 数据不是python，而是全网的默认的 &#125; )print(res.text) 登录豆瓣访问主义1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-import reimport requestsimport jsons = requests.Session()url_login = 'https://accounts.douban.com/j/mobile/login/basic'formdata = &#123; \"ck\": \"\", \"name\": \"18610984857\", \"password\": \"123456\", \"remember\" : \"false\", \"ticket\": \"\"&#125;headers = &#123; 'user-agent' : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"&#125;r = s.post(url_login,data=formdata,headers = headers)content = r.textuid = json.loads(content).get(\"payload\").get(\"account_info\").get(\"uid\")url_contacts = \"https://www.douban.com/people/%s/\" % uidhtml = s.get(url_contacts,headers = headers)print(html.text)","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"04 基础模块之 Urllib","date":"2019-10-27T11:28:00.000Z","path":"2019/10/27/04-基础模块之-Urllib/","text":"简介: Urllib 的使用 4.1 介绍Urllib库是python内置HTTP请求库，不需要额外安装即可使用。 相比而言，Request库更加好用(基于Urllib库来实现的)，但是作为一个基本库，了解用法和原理。 4.1.1 包含4个模块包含4个模块 1234urllib.request 请求模块urllib.error 异常处理模块 捕捉异常并进行处理，让程序不会终止urllib.parse url 解析模块 比如拆分、解析、合并等等的方法。urllib.rebotparser robots.txt解析模块 然后判断哪些网站可以爬，哪些网站不可以爬的，其实用的比较少。 12345robots.txt是一个纯文本文件，在这个文件中网站管理者可以声明该网站中不想被搜索引擎访问的部分，或者指定搜索引擎只收录指定的内容。当一个搜索引擎（又称搜索机器人或蜘蛛程序）访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，那么搜索机器人就沿着链接抓取。 4.1.2 与Python2的变化python2 123import urllib2 #urlopen 在python3中移动到request模块中response = urllib2.urlopen('http://www.baidu.com') python3 12import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com') 4.2 Request 请求处理模块4.2.1 urlopen打开URL URL，它可以是字符串或Request对象 1urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None) url: 网站的urldata：额外是数据，例如post数据。必须是指定要发送到的其他数据的对象服务器，如果不需要此类数据，则为“无”。timeout ：可选的* timeout 参数指定以秒为单位的超时，阻止连接尝试之类的操作（如果未指定，则将使用全局默认超时设置）。这仅适用于HTTP，HTTPS和FTP连接。cafile和capath : 可选的 *cafile 和 capath 参数指定一组受信任的CA HTTPS请求的证书。 Get请求1234import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com') # get请求一个网页print(response.read().decode('utf-8')) Post请求如果要以POST发送一个请求，只需要把参数data以bytes形式传入。不加data,是get方式传送。 123456789import urllib.parseimport urllib.requestdata = &#123; # 传入的数据是一个字典数据 'word': 'hello'&#125;query_string = bytes(urllib.parse.urlencode(data),encoding='utf8') # 注意 encoding是传入bytes的参数非urlencode参数response = urllib.request.urlopen('http://httpbin.org/post',data=query_string)print(response.read()) 返回结果 1b'&#123;\\n \"args\": &#123;&#125;, \\n \"data\": \"\", \\n \"files\": &#123;&#125;, \\n \"form\": &#123;\\n \"word\": \"hello\"\\n &#125;, \\n \"headers\": &#123;\\n \"Accept-Encoding\": \"identity\", \\n \"Content-Length\": \"10\", \\n \"Content-Type\": \"application/x-www-form-urlencoded\", \\n \"Host\": \"httpbin.org\", \\n \"User-Agent\": \"Python-urllib/3.7\"\\n &#125;, \\n \"json\": null, \\n \"origin\": \"111.196.243.150, 111.196.243.150\", \\n \"url\": \"https://httpbin.org/post\"\\n&#125;\\n' 设置超时定以秒为单位的超时阻止连接尝试之类的操作（如果未指定，则将使用全局默认超时设置） 123456789101112import socketimport urllib.requestimport urllib.errorresponse1 = urllib.request.urlopen('http://httpbin.org/get', timeout=1) # Get 请求，1s内请求，请求成功返回请求数据print(response1.read())try: response = urllib.request.urlopen('http://httpbin.org/get',timeout=0.1) # 0.1s， 请求会超时，会抛出异常except urllib.error.URLError as e : # 捕捉异常， if isinstance(e.reason,socket.timeout): # 这里reason中返回的是socket.timeout 类型对象。 print(\"TIME OUT\") 结果 12b&apos;&#123;\\n &quot;args&quot;: &#123;&#125;, \\n &quot;headers&quot;: &#123;\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.7&quot;\\n &#125;, \\n &quot;origin&quot;: &quot;111.196.243.150, 111.196.243.150&quot;, \\n &quot;url&quot;: &quot;https://httpbin.org/get&quot;\\n&#125;\\n&apos;TIME OUT 补充: 12345678910URLError是urllib.error模块的一个类，当使用request产生异常时都可以通过捕获这个类来处理。它具有一个属性reason，返回错误的原因。另外，它还有一个子类是HTTPError，专门处理 HTTP 请求错误。它有如下 3 个属性： 1）code： 返回 HTTP 状态码，比如 404 表示网页不存在， 500 表示服务器内部错误等。 2）reason：同父类一样，用于返回错误的原因。 3）headers： 返回请求头。URLError 中有很多类型的异常，而 socket.timeout 只是其中一种，所以用 isinstance(e.reason, socket.timeout) 来判断，对超时这种异常单独处理isinstance(object, classinfo) # 判读对象是否属于一个类型，是则返回True, 例如 isinstance (1 ,int) 返回true 4.2.2 Response 响应信息响应类型、状态码、响应头1234567import urllib.requestresponse = urllib.request.urlopen('http://www.python.org')print(type(response)) # Response 的类型是http.client.HTTPResponseprint(response.status) # 获取状态码print(response.getheaders()) # 查看响应头，集合内包含数组类型print(response.getheader('Server')) # 具体的响应头内容，使用的方法是getheader，所有用的0是getheaders 结果 1234&lt;class &apos;http.client.HTTPResponse&apos;&gt;200[(&apos;Server&apos;, &apos;nginx&apos;), (&apos;Content-Type&apos;, &apos;text/html; charset=utf-8&apos;), (&apos;X-Frame-Options&apos;, &apos;DENY&apos;), (&apos;Via&apos;, &apos;1.1 vegur&apos;), (&apos;Via&apos;, &apos;1.1 varnish&apos;), (&apos;Content-Length&apos;, &apos;48699&apos;), (&apos;Accept-Ranges&apos;, &apos;bytes&apos;), (&apos;Date&apos;, &apos;Thu, 17 Oct 2019 16:26:43 GMT&apos;), (&apos;Via&apos;, &apos;1.1 varnish&apos;), (&apos;Age&apos;, &apos;1038&apos;), (&apos;Connection&apos;, &apos;close&apos;), (&apos;X-Served-By&apos;, &apos;cache-iad2129-IAD, cache-hnd18743-HND&apos;), (&apos;X-Cache&apos;, &apos;HIT, HIT&apos;), (&apos;X-Cache-Hits&apos;, &apos;1, 1594&apos;), (&apos;X-Timer&apos;, &apos;S1571329603.476597,VS0,VE0&apos;), (&apos;Vary&apos;, &apos;Cookie&apos;), (&apos;Strict-Transport-Security&apos;, &apos;max-age=63072000; includeSubDomains&apos;)]nginx 显示网页的所有内容 1234import urllib.requestresponse = urllib.request.urlopen('http://www.python.org')print(response.read().decode('utf-8')) 4.2.3 Request我们知道利用urlopen( )方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需要加入Headers等信息，就可以利用更强大的Request类来构建。 例子1：获取页面 123456import urllib.requestrequest = urllib.request.Request('https://python.org') # 是&lt;class 'urllib.request.Request'&gt; ， response = urllib.request.urlopen(request) # 将Request 对象 传给urlopen, 请求网页，与上面的请求效果相同print(response.read().decode('utf-8')) 例子2:设置请求头数据进行访问 12345678910111213from urllib import request,parseurl_v = 'http://httpbin.org/post'headers_v = &#123; 'User-Agent':'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host':'httpbin.org'&#125;dict_value = &#123; 'name':'Germey'&#125;data_v = bytes(parse.urlencode(dict_value),encoding='utf8') # 生成post 请求数据req = request.Request(url=url_v,data=data_v,headers=headers_v,method='POST') # Request 使用自定义的请求头进行访问response = request.urlopen(req)print(response.read().decode('utf-8')) 结果 123456789101112131415161718&#123; &quot;args&quot;: &#123;&#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123; &quot;name&quot;: &quot;Germey&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;Python-urllib/3.7,Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;111.196.243.150, 111.196.243.150&quot;, &quot;url&quot;: &quot;https://httpbin.org/post&quot;&#125; 另外headers的一些属性，下面的需要特别注意一下： 123456User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用application/json ： 在 JSON RPC 调用时使用application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务 4.2.4 Handler代理 什么是代理：代理就是第三方代替本体处理相关事务。例如：生活中的代理：代购，中介，微商…… 爬虫中为什么需要使用代理？ 12一些网站会有相应的反爬虫措施，例如很多网站会检测某一段时间某个IP的访问次数，如果访问频率太快以至于看起来不像正常访客，它可能就会会禁止这个IP的访问。所以我们需要设置一些代理IP，每隔一段时间换一个代理IP，就算IP被禁止，依然可以换个IP继续爬取。 代理的分类： 12 正向代理：代理客户端获取数据。正向代理是为了保护客户端防止被追究责任。 反向代理：代理服务器提供数据。反向代理是为了保护服务器或负责负载均衡。 123456789101112131415161718192021import urllib.requestimport urllib.parse#1.创建处理器对象，在其内部封装代理ip和端口handler=urllib.request.ProxyHandler(proxies=&#123;'http':'95.172.58.224:52608'&#125;)#2.创建opener对象，然后使用该对象发起一个请求opener=urllib.request.build_opener(handler)url='http://www.baidu.com/s?ie=UTF-8&amp;wd=ip'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',&#125;request = urllib.request.Request(url, headers=headers)#使用opener对象发起请求，该请求对应的ip即为我们设置的代理ipresponse = opener.open(request)with open('./daili.html','wb') as fp: fp.write(response.read()) 4.2.6 Cookie cookie概念 1当用户通过浏览器首次访问一个域名时，访问的web服务器会给客户端发送数据，以保持web服务器与客户端之间的状态保持，这些数据就是cookie。 cookie作用 123我们在浏览器中，经常涉及到数据的交换，比如你登录邮箱，登录一个页面。我们经常会在此时设置30天内记住我，或者自动登录选项。那么它们是怎么记录信息的呢，答案就是今天的主角cookie了，Cookie是由HTTP服务器设置的，保存在浏览器中，但HTTP协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。 例子1：读取网页，获取cookie 数据 123456789101112import http.cookiejar,urllib.request# 闯将cookiejar对象cookle_v = http.cookiejar.CookieJar()# 创建处理器对象(携带cookiejar对象的)handler_v = urllib.request.HTTPCookieProcessor(cookle_v)# 创建一个opener 对象，传入handleopener =urllib.request.build_opener(handler_v)# 使用opener.open打开百度网页，cookie会自动赋值,cookle_v 中自动存入百度cookie数据response = opener.open('http://www.baidu.com')for item in cookle_v: print(item.name+\"=\"+item.value) 结果 1234567BAIDUID=796A3014C5B8B3D6522BA0479D83A132:FG=1BIDUPSID=796A3014C5B8B3D6522BA0479D83A132H_PS_PSSID=1427_21097_18560_29568_29220_29911PSTM=1571347700delPer=0BDSVRTM=0BD_HOME=0 例子2：将cookie的数据保存到本地 12345678910import http.cookiejar,urllib.requestfilename= 'Cookie.txt'# 生成cookiejar的子类对象cookie_v = http.cookiejar.MozillaCookieJar(filename)handler_v = urllib.request.HTTPCookieProcessor(cookie_v)opener =urllib.request.build_opener(handler_v)response = opener.open('http://www.baidu.com')#使用save方法cookie_v.save(ignore_discard=True,ignore_expires=True) 本地创建Cookie.txt ，内容如下 1234567891011# Netscape HTTP Cookie File# http://curl.haxx.se/rfc/cookie_spec.html# This is a generated file! Do not edit..baidu.com TRUE / FALSE 3718832329 BAIDUID A3096B19FD83945F9C7D59240C098881:FG=1.baidu.com TRUE / FALSE 3718832329 BIDUPSID A3096B19FD83945F9C7D59240C098881.baidu.com TRUE / FALSE H_PS_PSSID 1443_21113_29568_29220_26350_29910.baidu.com TRUE / FALSE 3718832329 PSTM 1571348681.baidu.com TRUE / FALSE delPer 0www.baidu.com FALSE / FALSE BDSVRTM 0www.baidu.com FALSE / FALSE BD_HOME 0 保存方式2： 12345678import http.cookiejar,urllib.requestfilename= &apos;Cookie.txt&apos;cookie_v = http.cookiejar.LWPCookieJar(filename) # 使用类不同handler_v = urllib.request.HTTPCookieProcessor(cookie_v)opener =urllib.request.build_opener(handler_v)response = opener.open(&apos;http://www.baidu.com&apos;)cookie_v.save(ignore_discard=True,ignore_expires=True) 结果，格式与上面的不同 12345678#LWP-Cookies-2.0Set-Cookie3: BAIDUID=&quot;2B2C53DE6F9F35482D6A3DCB7FD52B35:FG=1&quot;; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2020-10-16 21:53:43Z&quot;; comment=bd; version=0Set-Cookie3: BIDUPSID=2B2C53DE6F9F35486E1E82B0F21C8A56; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2087-11-05 01:07:50Z&quot;; version=0Set-Cookie3: H_PS_PSSID=1423_21082_29567_29699_29220; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; discard; version=0Set-Cookie3: PSTM=1571349222; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2087-11-05 01:07:50Z&quot;; version=0Set-Cookie3: delPer=0; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; discard; version=0Set-Cookie3: BDSVRTM=0; path=&quot;/&quot;; domain=&quot;www.baidu.com&quot;; path_spec; discard; version=0Set-Cookie3: BD_HOME=0; path=&quot;/&quot;; domain=&quot;www.baidu.com&quot;; path_spec; discard; version=0 4.3 error模块 异常处理官方文档 1https://docs.python.org/3/library/urllib.error.html urllib.error模块为urllib.request引发的异常而定义异常类。基本异常类是URLError。 urllib.error.URLError : 处理程序遇到问题时会引发此异常（或派生的异常） 1reason 出现此错误的原因。它可以是消息字符串或其他异常实例。 urllib.error.HTTPError : 尽管是一个异常（URLError的子类）， HTTPError但也可以用作非异常文件状的返回值（与urlopen()返回的东西相同）。在处理异常的HTTP错误（例如身份验证请求）时，此功能很有用。 urllib.error.ContentTooShortError（msg，content ）:当urlretrieve() 函数检测到下载数据量小于预期量（由Content-Length标头提供）时，将引发此异常。该content属性存储下载的（假定为截断的）数据。 例1: 在不存在的网址下，打印出错误信息，error.URLError 12345from urllib import request,errortry: response = request.urlopen('http://cuiqingcal.com/index.html') # 地址不存在except error.URLError as e: # 捕获错误信息 error.URLError print(e.reason) # reason 是错误原因 结果 1[Errno 11001] getaddrinfo failed # 获取地址失败 例2：打印请求失败响应头信息 注意:HTTPError 是URLErros 的子类，所以放在其前面，当HTTPError捕捉不到时，才使用父类进行捕捉 12345678910from urllib import request, errortry: response = request.urlopen('https://www.bilibili.com/inddex.html')except error.HTTPError as e: print(e.reason, e.code, e.headers, sep='\\n')except error.URLError as e: print(e.reason)else: print('Request Successfully') 结果 12345678Not Found404 # 显示网页不存在Date: Thu, 17 Oct 2019 23:26:46 GMTContent-Type: text/htmlTransfer-Encoding: chunkedConnection: closeServer: TengineContent-Encoding: gzip 例3: 超时异常的捕捉 123456789101112import socketimport urllib.requestimport urllib.errorresponse1 = urllib.request.urlopen('http://httpbin.org/get', timeout=1) # Get 请求，1s内请求，请求成功返回请求数据print(response1.read())try: response = urllib.request.urlopen('http://httpbin.org/get',timeout=0.1) # 0.1s， 请求会超时，会抛出异常except urllib.error.URLError as e : # 捕捉异常， if isinstance(e.reason,socket.timeout): # 这里reason中返回的是socket.timeout 类型对象。 print(\"TIME OUT\") 4.4 Parse URL解析模块4.4.1 urlpase1urllib.parse.urlparse(url, scheme='', allow_fragments=True) 12URL分割成6部分 &lt;scheme&gt;://&lt;netloc&gt;/&lt;path&gt;;&lt;params&gt;?&lt;query&gt;#&lt;fragment&gt;返回一个 6个元素的元组: (scheme, netloc, path, params, query, fragment). 例1： 默认将URL分为6部分 1234from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=4#comment')print(type(result),result,sep=\"\\n\") 结果 12&lt;class &apos;urllib.parse.ParseResult&apos;&gt; # 显示类型ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=4&apos;, fragment=&apos;comment&apos;) # 路径，get请求数据，将URL划分为6部分 例2: 无http协议时，指定协议 12result = urlparse(&apos;www.baidu.com/index.html;user?id=4#comment&apos;,scheme=&apos;https&apos;) # 无http 协议，指定为https协议，如果有协议，指定的http 协议不生效print(result) 结果: 1ParseResult(scheme=&apos;https&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=4&apos;, fragment=&apos;comment&apos;) 1result = urlparse(&apos;http://www.baidu.com/index.html;user?id=4#comment&apos;,scheme=&apos;https&apos;) # 如果有协议，指定的https协议不生效 例3: allow_fragments 123456# comment 是一个锚点，allow_fragments=False后，将comment 拼接到前面去result = urlparse(&apos;www.baidu.com/index.html;user?id=4#comment&apos;,allow_fragments=False)print(result)result2 = urlparse(&apos;www.baidu.com/index.html#comment&apos;,allow_fragments=False)print(result2) 结果 1234# 有查询参数comment拼接到查询上去ParseResult(scheme=&apos;&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=4#comment&apos;, fragment=&apos;&apos;) # 无查询参数comment拼接到html路径上去ParseResult(scheme=&apos;&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html#comment&apos;, params=&apos;&apos;, query=&apos;&apos;, fragment=&apos;&apos;) 4.4.2 urlparse与urlpase的相反函数，将6个参数拼接为一个URL 123from urllib.parse import urlunparsedata = ['http','www.baidu.com','index.html','user','a=6','comment']print(urlunparse(data)) 结果 1http://www.baidu.com/index.html;user?a=6#comment 4.4.3 urljoin拼接2个地址，一般是url +path 12345from urllib.parse import urljoinurl='http://ip/path='api/user/login'urljoin(url,path)拼接后的路径为'http//ip/api/user/login' 4.4.4 urllencode将字典或由两个元素组成的元组序列编码为URL查询字符串 12345678910from urllib.parse import urlencodeparams = &#123; 'name': 'germey', 'age': 22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) 结果 1http://www.baidu.com?name=germey&amp;age=22","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"05 hexo 好用的插件","date":"2019-10-27T11:13:00.000Z","path":"2019/10/27/05-hexo-其他/","text":"简介： hexo 的一些插件使用后台发布插件hexo-admin、本地图片、在hexo中使用markdown图两种方法 1 安装后台发布插件hexo-admin进行安装插件 1234npm install --save hexo-admin# 如果报错缺少组件，则缺少什么安装什么，npm install 加缺少的组件。# 例如 npm install minimatch@&quot;3.0.2&quot; # 组件安装后执行 npm update -d 1.1 配置文件设置用户名和密码设置密码在hexo的_config.yml配置hexo-admin 12345admin: username: test # 用户名 password_hash: $2a$10$5nJewMv0le/TQZRjx6inKud6EEHj50Pu0vdx5rPjc8rsoEqjhpgy. # 密码的哈希映射值，可以为空 secret: hey hexo #用以cookies安全； deployCommand: &apos;./admin_script/hexo-generate.sh&apos; 使用的是bcrypt哈希算法，使用下面的方式计算哈希值 123456[root@blog blog]# node &gt; const bcrypt = require(&apos;bcrypt-nodejs&apos;)undefined&gt; bcrypt.hashSync(&apos;test1&apos;) # 密码是test1&apos;$2a$10$5nJewMv0le/TQZRjx6inKud6EEHj50Pu0vdx5rPjc8rsoEqjhpgy.&apos;&gt; exit 这里的command对应于界面中的deploy按钮，在下面写上脚本，可以一键生成html页面，并提交到托管的地址，这就是hexo admin的核心思想！ 1.2 创建发布脚本1234mkdir admin_script # 创建文件夹cd admin_scripttouch hexo-generate.sh # 创建脚本 vim hexo-generate.sh # 写脚本 脚本内容， 12#!/usr/bin/env shhexo g 对脚本赋予权限 1chmod +x hexo-generate.sh 1.3 启动服务，进行测试12hexo server -dopen http://localhost:4000/admin/ 进入后台之后点击Deploy，里面的Deploy按钮是用来执行发布脚本的 2 本地图片2.1 安装1npm install --save hexo-featured-image 2.2 创建目录在${hexoHome}/source目录下新建images目录 我的目录 1E:\\github_blog\\blog\\source\\images 2.3 引用图片12在需要引用图片的地方使用markdown的格式进行引用，需要注意的是引用图片的路径必须为/images/yourImageName。例如引用localSearch.png文档应该写![local search](/images/localSearch.png)。 1![aa](/images/01.png) 3 在hexo中使用markdown图方法一12官网https://github.com/webappdevelp/hexo-filter-mermaid-diagrams 3.1 安装1npm install --save hexo-filter-mermaid-diagrams 3.2 配置在hexo的_config.yml文件（根目录的并非主题的）中，添加以下内容： 123456# mermaid chartmermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: &quot;7.1.2&quot; # default v7.1.2 options: # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js#startOnload: true // default true 3.3 js 文件修改位置 1themes/next/layout/_partials/after_footer.pug 我的文件是after_footer.jade 1234567if theme.mermaid.enable == true script(type=&apos;text/javascript&apos;, id=&apos;maid-script&apos; mermaidoptioins=theme.mermaid.options src=&apos;https://unpkg.com/mermaid@&apos;+ theme.mermaid.version + &apos;/dist/mermaid.min.js&apos; + &apos;?v=&apos; + theme.version) script. if (window.mermaid) &#123; var options = JSON.parse(document.getElementById(&apos;maid-script&apos;).getAttribute(&apos;mermaidoptioins&apos;)); mermaid.initialize(options); &#125; 格式有after_footer.pug , after-footer.ejs ,footer.swig等。不同格式写法不同，详情见官网 3.4 语法示例语言标识符：mermaid 1234567...mermaid (.号防止形成代码块,替换成`)graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D;... 效果 graph TD; A-->B; A-->C; B-->D; C-->D; 4 在hexo中使用markdown图方法二4.1 安装1npm install --save hexo-filter-flowchart 4.2 配置在hexo的_config.yml文件（根目录的并非主题的）中，添加以下内容： 1234flowchart: # raphael: # optional, the source url of raphael.js # flowchart: # optional, the source url of flowchart.js options: # options used for `drawSVG` 4.3 语法语言标识符：flow 1234567891011121314151617...flow (.号防止形成代码块,替换成 ` )st=&gt;start: Start|past:&gt;http://www.google.com[blank]e=&gt;end: End:&gt;http://www.google.comop1=&gt;operation: My Operation|pastop2=&gt;operation: Stuff|currentsub1=&gt;subroutine: My Subroutine|invalidcond=&gt;condition: Yesor No?|approved:&gt;http://www.google.comc2=&gt;condition: Good idea|rejectedio=&gt;inputoutput: catch something...|requestst-&gt;op1(right)-&gt;condcond(yes, right)-&gt;c2cond(no)-&gt;sub1(left)-&gt;op1c2(yes)-&gt;io-&gt;ec2(no)-&gt;op2-&gt;e... 效果 st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"03 爬虫的基本原理","date":"2019-10-27T09:31:00.000Z","path":"2019/10/27/03-爬虫的基本原理/","text":"简介:介绍爬虫的原理 3、 爬虫简介3.1 爬虫概述&emsp;&emsp;近年来，随着网络应用的逐渐扩展和深入，如何高效的获取网上数据成为了无数公司和个人的追求，在大数据时代，谁掌握了更多的数据，谁就可以获得更高的利益，而网络爬虫是其中最为常用的一种从网上爬取数据的手段。 &emsp;&emsp;网络爬虫，即Web Spider，是一个很形象的名字。如果把互联网比喻成一个蜘蛛网，那么Spider就是在网上爬来爬去的蜘蛛。网络蜘蛛是通过网页的链接地址来寻找网页的。从网站某一个页面（通常是首页）开始，读取网页的内容，找到在网页中的其它链接地址，然后通过这些链接地址寻找下一个网页，这样一直循环下去，直到把这个网站所有的网页都抓取完为止。如果把整个互联网当成一个网站，那么网络蜘蛛就可以用这个原理把互联网上所有的网页都抓取下来。 3.2 爬虫的价值&emsp;&emsp;互联网中最有价值的便是数据，比如天猫商城的商品信息，链家网的租房信息，雪球网的证券投资信息等等，这些数据都代表了各个行业的真金白银，可以说，谁掌握了行业内的第一手数据，谁就成了整个行业的主宰，如果把整个互联网的数据比喻为一座宝藏，那我们的爬虫课程就是来教大家如何来高效地挖掘这些宝藏，掌握了爬虫技能， 你就成了所有互联网信息公司幕后的老板，换言之，它们都在免费为你提供有价值的数据。 3.3 爬虫的基本流程graph LR; A[发送请求]-->B[获取响应]; B-->C[解析数据]; C-->D[数据持久化]; 发送请求: 12通过发送HTTP库向目标站点发送请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应。 获取响应内容: 12如果服务器能正常响应，会得到一个Response，Response的内容便是要获取的页面内容，类型有HTML和Json字符串，二进制数据(图片视频)等类型。 解析内容: 123得到的内容可能是HTML,可以使用正则表达式、网页解析库进行解析。可能是json,可以直接转为Json对象解析。可能是二进制数据，可以保存或者进一步的处理。 数据持久化: 1保存多种多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。 123chrome浏览器右上角菜单 》 更多工具》开发者工具点击Network标题, 刷新页面，左下角会出现服务端给客户端返回的数据，点击其中任意文件，右边会出现请求头Headers（request 内容）和Response 内容 123chrome浏览器右上角菜单 》 更多工具》开发者工具点击Network标题, 刷新页面，左下角会出现服务端给客户端返回的数据，点击其中任意文件，右边会出现请求头Headers（request 内容）和Response 内容 graph LR; A[客户端]--Request-->B[服务器]; B--Response-->A; 3.4 能抓取那些数据 网页文本 ：如HTML 文档，Json格式文本 图片：二进制文件，保存为图片格式 视频：二进制文件，保存为视频格式 其他：只要能够请求到的，都能获取 3.5 解析方式 直接处理：简单的网页，内容较少 Json 解析： 使用Ajax加载的数据，解析字符串 正则表达式：常用 BeautifulSoup: 解析库解析 XPath: 解析库解析 PyQuery: 解析库解析 3.6 保存数据 文本: 纯文本、Json、Xml 关系型数据库: MySQL、Oracle、SQL Server等具有结构化表结构形式存储 非关系型数据库: Redis、MongoDB 等key-Value 形式存储 二进制文件: 如图片、视频、音频等直接保存为特定格式 3.7 http库测试工具 - httpbin请求和响应服务类似这种能够很方便调试接口的还有很多，但是无疑还是httpbin最好用。requestbputsreqttpresponderrunscope 12345678import requests s = requests.Session() print s.get('http://httpbin.org/ip').text print s.get('http://httpbin.org/get').json() print s.post('http://httpbin.org/post', &#123;'key':'value'&#125;,headers=&#123;'user-agent':'LAOGAO'&#125;).text print s.get('http://httpbin.org/status/404').status_code print s.get('http://httpbin.org/html').text print s.get('http://httpbin.org/deny').text 一些常见的接口 Endpoint Description / This page. /ip Returns Origin IP. /user-agent Returns user-agent. /headers Returns header dict. /get Returns GET data. /post Returns POST data. /patch Returns PATCH data. /put Returns PUT data. /delete Returns DELETE data …….. ……..","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"02 爬虫相关库的安装","date":"2019-10-27T09:29:00.000Z","path":"2019/10/27/02-爬虫相关库的安装/","text":"简介:爬虫相关库的安装 2.1 re和urllib库re: 正则库urllib : URL 处理库 默认re 库和urllib 的库是安装的 12345678910C:\\Users\\wutai&gt;pythonPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32....&gt;&gt;&gt; import urllib # 库没有错，说明是安装上的&gt;&gt;&gt; import urllib.request&gt;&gt;&gt; urllib.request.urlopen(&apos;http://www.baidu.com&apos;) # &lt;http.client.HTTPResponse object at 0x000001C437E5E5F8&gt;&gt;&gt;&gt; import re # 库没有错，说明是安装上的&gt;&gt;&gt; 2.2 requestsrequest模块是一个用于访问网络的模块 12345678910 C:\\Users\\wutai&gt;pip -Vpip 19.1.1 from D:\\Program Files\\Anaconda3\\lib\\site-packages\\pip (python 3.7)C:\\Users\\wutai&gt;pip list #查看安装的库Package Version---------------------------------- ---------alabaster 0.7.12anaconda-client 1.7.2anaconda-navigator 1.9.7... 默认request模块没有安装 123C:\\Users\\wutai&gt;pip install requests # 安装request模块Collecting request... 安装后查看并进行测试 12345678910111213C:\\Users\\wutai&gt;pip list #查看安装的库Package Version---------------------------------- ---------...request 2019.4.13requests 2.22.0 #查看已经安装上了....C:\\Users\\wutai&gt;python&gt;&gt;&gt; import requests # 导入模块并测试&gt;&gt;&gt; requests.get(&apos;http://www.baidu.com&apos;)&lt;Response [200]&gt; 2.3 seleniumselenium : 是一个用于Web应用程序测试的工具 安装 1C:\\Users\\wutai&gt;pip install selenium 测试 123456789101112131415161718192021222324C:\\Users\\wutai&gt;pythonPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32&gt;&gt;&gt; import selenium&gt;&gt;&gt; from selenium import webdriver&gt;&gt;&gt; driver = webdriver.Chrome()Traceback (most recent call last): File &quot;D:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py&quot;, line 76, in start stdin=PIPE) File &quot;D:\\Program Files\\Anaconda3\\lib\\subprocess.py&quot;, line 775, in __init__ restore_signals, start_new_session) File &quot;D:\\Program Files\\Anaconda3\\lib\\subprocess.py&quot;, line 1178, in _execute_child startupinfo)FileNotFoundError: [WinError 2] 系统找不到指定的文件。During handling of the above exception, another exception occurred:Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py&quot;, line 73, in __init__ self.service.start() File &quot;D:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py&quot;, line 83, in start os.path.basename(self.path), self.start_error_message)selenium.common.exceptions.WebDriverException: Message: &apos;chromedriver&apos; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home # chromedriver 不在环境变量中 2.4 chromedriver使用WebDriver在Chrome浏览器上进行测试时，需要从http://chromedriver.storage.googleapis.com/index.html网址中下载与本机chrome浏览器对应的驱动程序，驱动程序名为chromedriver； 下载是需要翻墙的，使用国内镜像 12345678910111213http://npm.taobao.org/mirrors/chromedriver/下载的版本需要和chrome 浏览器的版本对应，否则会报错浏览器版本 77.0.3865.120下载chromdriver版本:77.0.3865.10将chromedriver.exe 复制到python的命令的位置，我复制到D:\\Program Files\\Anaconda3，在环境变量中，C:\\Users\\wutai&gt;chromedriverStarting ChromeDriver 77.0.3865.10 (bc3579f611bbc73331171afe020ec7a45e6ccc55-refs/branch-heads/3865@&#123;#93&#125;) on port 9515Only local connections are allowed.Please protect ports used by ChromeDriver and related test frameworks to prevent access by malicious code. 1234&gt;&gt;&gt; import selenium&gt;&gt;&gt; from selenium import webdriver&gt;&gt;&gt; driver = webdriver.Chrome() # 浏览器打开chrom浏览器的一个页面DevTools listening on ws://127.0.0.1:59729/devtools/browser/a34b37e9-2755-4d4e-837c-ec2ae5c81afa 2.5 phantomjsPhantomJS是一个基于Webkit的”无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器更高效。 如果我们把Selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理JavaScript、Cookie、headers，以及任何我们真实用户需要做的事情。 该程序的开发成员已经不在更新该软件，软件版本”2.1.1”” 1下载地址： https://phantomjs.org/download.html 123解压软件包到D:\\phantomjs-2.1.1-windows在环境变量中添加该目录D:\\phantomjs-2.1.1-windows\\bin ，该文件夹内有 测试 12345C:\\Users\\wutai&gt;phantomjs # 进入交互模式phantomjs&gt; console.log(&apos;Hello World&apos;)Hello Worldundefinedphantomjs&gt; 2.6 lxmllxml是python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高 XPath，全称XML Path Language，即XML路径语言，它是一门在XML文档中查找信息的语言，它最初是用来搜寻XML文档的，但是它同样适用于HTML文档的搜索 安装 12 C:\\Users\\wutai&gt;pip install lxmlRequirement already satisfied: lxml in d:\\program files\\anaconda3\\lib\\site-packages (4.3.4) 2.7 beautifulsoup12345Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。 安装 123C:\\Users\\wutai&gt;pip install beautifulsoup4Requirement already satisfied: beautifulsoup4 in d:\\program files\\anaconda3\\lib\\site-packages (4.7.1)Requirement already satisfied: soupsieve&gt;=1.2 in d:\\program files\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.8) 2.8 pyquerypyquery库是jQuery的Python实现，能够以jQuery的语法来操作解析 HTML 文档，易用性和解析速度都很好，和它差不多的还有BeautifulSoup，都是用来解析的。 官方文档 1https://pythonhosted.org/pyquery/ 安装 1C:\\Users\\wutai&gt; pip install pyquery 2.9 pymysql操作mysql数据库的一个库，在python3中使用 安装 1C:\\Users\\wutai&gt;pip install pymysql 2.10 pymongo操作mongodb数据库的一个库 安装 1C:\\Users\\wutai&gt;pip install pymongo 2.11 redis操作redis数据库的一个库 安装 1C:\\Users\\wutai&gt;pip install pymongo 2.12 flaskFlask是一个使用 Python 编写的轻量级 Web 应用框架 安装 1C:\\Users\\wutai&gt;pip install flask 2.13 django Web 应用框架 安装 1C:\\Users\\wutai&gt;pip install django 2.14 jupyterJupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。 Jupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等 [1] 。 安装 1C:\\Users\\wutai&gt;pip install jupyter 测试 1234C:\\Users\\wutai&gt;jupyter notebook # 会弹出一个浏览器页面点击右侧 NEW &gt; Notebook &gt; python3 会出现一个命令行print(&quot;aaa&quot;) # 使用户ctrl + 回车 会python3会执行该命令","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"01 爬虫前期软件安装","date":"2019-10-27T09:27:00.000Z","path":"2019/10/27/01爬虫/","text":"安装环境:windows简介: 爬虫前安装软件，Anacoda3,MongoDB ，MySql 1.1 安装Anacoda3如果没有手动安装Python3， 推荐使用Anacoda3 1.1.1 安装12官网地址，下载exe文件https://www.anaconda.com/distribution/#download-section 安装完成后，在最后一步，默认是没有添加环境变量的（添加环境变量的勾没有添加），在windows 上添加环境变量 12345D:\\Program Files\\Anaconda3D:\\Program Files\\Anaconda3\\ScriptsD:\\Program Files\\Anaconda3\\Library\\mingw-w64\\binD:\\Program Files\\Anaconda3\\Library\\usr\\binD:\\Program Files\\Anaconda3\\Library\\bin 1.1.2 测试在cmd上执行命令 123456789101112131415161718192021222324252627C:\\Users\\wutai&gt;conda info active environment : None user config file : C:\\Users\\wutai\\.condarc populated config files : C:\\Users\\wutai\\.condarc conda version : 4.7.10 conda-build version : 3.18.8 python version : 3.7.3.final.0 # python 版本 virtual packages : __cuda=9.2 base environment : D:\\Program Files\\Anaconda3 (writable) channel URLs : https://repo.anaconda.com/pkgs/main/win-64 https://repo.anaconda.com/pkgs/main/noarch https://repo.anaconda.com/pkgs/r/win-64 https://repo.anaconda.com/pkgs/r/noarch https://repo.anaconda.com/pkgs/msys2/win-64 https://repo.anaconda.com/pkgs/msys2/noarch package cache : D:\\Program Files\\Anaconda3\\pkgs C:\\Users\\wutai\\.conda\\pkgs C:\\Users\\wutai\\AppData\\Local\\conda\\conda\\pkgs envs directories : D:\\Program Files\\Anaconda3\\envs C:\\Users\\wutai\\.conda\\envs C:\\Users\\wutai\\AppData\\Local\\conda\\conda\\envs platform : win-64 user-agent : conda/4.7.10 requests/2.22.0 CPython/3.7.3 Windows/10 Windows/10.0.18362 administrator : False netrc file : None offline mode : False 安装完成后，Python 和pip 程序都默认安装完毕 123456789101112131415161718C:\\Users\\wutai&gt;pythonPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32Warning:This Python interpreter is in a conda environment, but the environment hasnot been activated. Libraries may fail to load. To activate this environmentplease see https://conda.io/activationType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; exit()C:\\Users\\wutai&gt;pip listPackage Version---------------------------------- ---------alabaster 0.7.12anaconda-client 1.7.2anaconda-navigator 1.9.7anaconda-project 0.8.3 1.2 安装MongoDB1.2.1 默认安装123https://www.mongodb.com/download-center/community选择版本，下载exe 文件，版本是社区版本我的版本是：v4.2.0 软件安装位置 1D:\\MongoDB 配置环境变量 123456789101112将 &quot;D:\\MongoDB\\bin&quot; 添加到环境变量Path中,该文件夹是MongoDB的命令文件夹进入CMD命令行C:\\Users\\wutai&gt;mongod -version # 进行测试db version v4.2.0git version: a4b751dcf51dd249c5865812b390cfd1c0129c30allocator: tcmallocmodules: nonebuild environment: distmod: 2012plus distarch: x86_64 target_arch: x86_64 安装mogodb服务后，会在本地有一个客户端，默认连接localhost的27017端口，可以可视化日志。 1.2.2 启动服务端 在 “D:\\MongoDB\\data “ 下创建 db 文件夹，存放数据（也可选择其他文件路径） 启动数据库服务端 1234567891011121314151617C:\\Users\\wutai&gt;mongod --dbpath &quot;D:\\MongoDB\\data\\db&quot; # 指定数据库位置2019-10-16T05:44:50.796+0800 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols &apos;none&apos;2019-10-16T05:44:50.803+0800 I CONTROL [initandlisten] MongoDB starting : pid=14084 port=27017 dbpath=D:\\MongoDB\\data\\db 64-bit host=WUTAI-PC2019-10-16T05:44:50.804+0800 I CONTROL [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R22019-10-16T05:44:50.805+0800 I CONTROL [initandlisten] db version v4.2.02019-10-16T05:44:50.806+0800 I CONTROL [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c302019-10-16T05:44:50.806+0800 I CONTROL [initandlisten] allocator: tcmalloc2019-10-16T05:44:50.807+0800 I CONTROL [initandlisten] modules: none2019-10-16T05:44:50.807+0800 I CONTROL [initandlisten] build environment:2019-10-16T05:44:50.809+0800 I CONTROL [initandlisten] distmod: 2012plus2019-10-16T05:44:50.810+0800 I CONTROL [initandlisten] distarch: x86_642019-10-16T05:44:50.810+0800 I CONTROL [initandlisten] target_arch: x86_642019-10-16T05:44:50.811+0800 I CONTROL [initandlisten] options: &#123; storage: &#123; dbPath: &quot;D:\\MongoDB\\data\\db&quot; &#125; &#125; # 数据库目录......2019-10-16T05:44:52.403+0800 I NETWORK [initandlisten] Listening on 127.0.0.1 # 监听IP2019-10-16T05:44:52.403+0800 I SHARDING [LogicalSessionCacheReap] Marking collection config.transactions as collection version: &lt;unsharded&gt;2019-10-16T05:44:52.406+0800 I NETWORK [initandlisten] waiting for connections on port 27017 # 启用的端口 MongoDB的默认端口号：27017 （redis：6379 MySql：3306） 1.2.3 验证(服务端不关闭)12345678910111213方式一： 在浏览器器上访问： http://localhost:27017/网页显示 ：It looks like you are trying to access MongoDB over HTTP on the native driver port.方式二: CMD 命名行执行mongoC:\\Users\\wutai&gt;mongoMongoDB shell version v4.2.0connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session &#123; &quot;id&quot; : UUID(&quot;7c204c1e-9f5c-435d-b125-837bc4a857e7&quot;) &#125;MongoDB server version: 4.2.0Welcome to the MongoDB shell.....&gt; 1.2.4 将MongoDB设置成Windows 服务创建日志文件 1D:\\MongoDB\\log 目录下创建mongodb.log文件 在管理员命令行执行（普通用户执行不生效） 1C:\\Users\\wutai&gt;mongod --bind_ip 0.0.0.0 --logpath &quot;D:\\MongoDB\\log\\mongodb.log&quot; --logappend --dbpath &quot;D:\\MongoDB\\data\\db&quot; --port 27017 --directoryperdb --serviceName &quot;MongoDB&quot; --serviceDisplayName &quot;MongoDB&quot; --install 绑定IP,设置日志位置(日志是追加logappend)，数据库文件位置，端口，配置服务名称”Mongodb”,配置服务后显示名称”Mongodb” 设置成windows 服务遇到2个问题 1234567891011121314151、启动服务时报1053 错误解决办法：删除服务：启动管理员cmd , sc delete &quot;MongoDB&quot; 创建服务：启动管理员cmd , D: cd D:\\MongoDB\\bin mongod --bind_ip 0.0.0.0 ....2、再次启动时报100错误解决办法：cd D:\\MongoDB\\data\\db # 数据库文件 1.找到你数据库文件夹中的这两个文件 mongod.lock storage.bson 2.删掉他们再次启动数据库则不会报错 在windows 服务界面看到MongoDB 服务，设置为自动 , 这样随windows 启动，服务也会启动 12服务名称： MongoDB显示名称： MongoDB 1.2.5 可视化MongoDB 客户端Robo3TRobo 3T是一个支持windows、Mac、Linux三个平台的mongo图形化客户端。 1下载页面：https://robomongo.org/download studio 3T是收费的，Robo3T是简化版免费的。 1.3 安装redisWindow 下安装 12下载地址：https://github.com/MSOpenTech/redis/releases下载为msi ,安装注意要勾选添加到环境变量。安装后，会自动创建服务（自启动） 可视化工具 Redis Desktop Manager因为Redis Desktop Manager作者在 0.9.4 版本之后选择对所有的安装包收费，不再提供安装包下载，但是源码依旧公开。0.9.3版本对于大量数据查询优化不够好，推荐使用0.9.4。 1.4 安装mysql1234地址 https://dev.mysql.com/downloads/installer/1、选择的windows版本是 mysql-installer-community-8.0.18.0.msi2、进入下载页面，点击“No thanks,just start my download ” 直接进行下载 1234567安装注意事项:1、选择安装类型“Choosing a setup Type ” 中选择“server only” 只安装服务端， 默认的是安装MySQL服务器和MySQL应用程序开发所需的工具。 如果您打算为现有服务器开发应用程序，这将很有用。2、&quot;high availability&quot; 高可用下选择第一个”standalone MySQL Server /Classic MysSQL Replication“ 经典的mysql 服务器选项；3、&quot;Type and Networking&quot; 配置服务器类型，默认即可，默认端口是 3306， 4、“Accounts and Method” 选择默认，认证方式；5、设置密码 admin123 验证 121、windows 服务上有一个MySQL80 服务2、安装有一个mysql 客户端，&quot;MySQL8.0 Command line Client&quot;,进入命令行，输入密码后，执行“show datbases;” 添加客户端环境变量 123456mysql的安装位置在C盘，将下面目录添加到PATH环境变量中C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin环境变量设置后，可以在命令行允行mysql 客户端C:\\Users\\wutai&gt;mysql -u root -pEnter password: ******** mysql 可视化工具 SQLyog12345配置新连接报错：错误号码 2058，分析是 mysql 密码加密方法变了。解决方法：windows 下cmd 登录 mysql -u root -p 登录你的 mysql 数据库，然后执行这条SQL：ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_password BY &apos;admin123&apos;;（注意分号）","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"05 windows10 安装hexo","date":"2019-10-15T20:47:00.000Z","path":"2019/10/16/05-windows10-安装hexo/","text":"简介: 在windows10上部署hexo,发布到github上随笔 一、准备工作1.1 安装git软件地址: https://git-scm.com/download/win 1.2 安装nodejs软件地址: https://nodejs.org/en/download/ 我的安装目录D:\\Program Files\\NodeJs 该目录下创建两个文件夹，这是用来放npm全局模块的安装目录（可以放在其他位置） node-cache node-global 配置环境变量 1、新建一个系统变量（电脑》属性》系统保护（打开系统属性）》高级》环境变量） 12变量名：NODE_HOME变量值（你的安装目录）：D:\\Program Files\\NodeJs 2、编辑Path变量 新增两个条目 12%NODE_HOME%%NODE_HOME%\\node-global 测试,进入cmd 12345C:\\Users\\wutai&gt;node -vv10.16.3C:\\Users\\wutai&gt;npm -v6.9.0 1.3 安装hexo123456C:\\Users\\wutai&gt;npm install -g hexo-clinpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules\\hexo-cli\\node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)+ hexo-cli@2.0.0added 187 packages from 434 contributors in 39.078s 二、Hexo的使用进入目录E:\\github_blog ,右击打开Git Bash ，执行 12345678wutai@WUTAI-PC MINGW64 /e/github_blog$ hexo init blog # 生成目录并进行初始化$ cd blog$ git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/BlueLake# 渲染插件$ npm install hexo-renderer-jade@0.3.0 --save$ npm install hexo-renderer-stylus --save 启用hexo 主题在Hexo根配置文件（blog/_config.yml）中把主题设置修改为BlueLake。 1theme: BlueLake 验证12# 调试模式$ hexo s --debug 在服务启动的过程， 当命令行输出中提示出：INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。如果是公网，使用公网IP 进行访问，注意开4000端口的防火墙。 大部分和Linux上主题配置相同 Hexo Admin插件管理安装 1npm install --save hexo-admin 根_config.yml 123456# 密码是test1admin: username: test password_hash: $2a$10$5nJewMv0le/TQZRjx6inKud6EEHj50Pu0vdx5rPjc8rsoEqjhpgy. secret: hey hexo deployCommand: &apos;hexo-pubish.bat&apos; 在同级目录新建hexo-pubish.bat，内容 1hexo g -d github 免密码设置根_config.yml 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/wutaiv1/wutaiv1.github.io.git branch: master 123$ git config user.email # 查看自己设置的邮箱2677575015@qq.com$ ssh-keygen -t rsa -C &quot;2677575015@qq.com&quot; # 执行后提示输入参数，默认，直接回车就行 123C:\\Users\\wutai\\.ssh # 目录下生成2个文件id_rsa和id_rsa.pub 文件 之后在github添加SSH Key,在任意界面右上角，点击你的头像，选择Settings-&gt; SSH and GPG keys –&gt; SSH keys-&gt;New SSH key .然后将生成的 id_rsa.pub 中的内容全选复制到 key 输入框中，然后点击 Add key 完成添加公钥。 Title 1window hexo key 1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDClKy74sNNqYWUd1mkV7UDrIBv2QP5QNP1hBfU1ZrP4/8TxfXnsqURWpAO9VCi0MmP2+w4tWWzEi9mvmXudrpb3PNPHa0r4GcmSPHi3CU7iu6/9ux+Fw4EMrtYZBuUUQqM5eM+GoJqo2zCQ4Gzr3tSXhjy/IsXbF6Hzqp1gE8DFpRuQYfXspPOE6urfO/74BebQM6JFzoJXWn8bFC8ub8MAGIzKsUjatFuGtI6Iyg22oW7vafQNY+196LQgREryDGcGgmlksAxSHv6BTs7sUF+FkoFvcsd9B10YbjcViO6jL0f9i5hL9N62Yh/enthsshWqPekJdWQTS7Tw8ZcCRij6FggISooIMO29XOUca3K6KZIBJ5lgilUE/o1RjCnktJZUjSlkeN0TBzbEeFdr5BCihLd/lTZ/CAc139otsPg3ljji3kjA2K3SQEDpkCTkDTisxRAcFiUYhE/0dosUUPeho0o6UQb4QZAfG4DvIs3KDsXbWSPj9QkrTqMXWG8YTc= 2677575015@qq.com 1234567进行测设$ ssh -T git@github.comThe authenticity of host &apos;github.com (13.250.177.223)&apos; can&apos;t be established.RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.Are you sure you want to continue connecting (yes/no/[fingerprint])? yes # 输入yesWarning: Permanently added &apos;github.com,13.250.177.223&apos; (RSA) to the list of known hosts.Hi wutaiv1! You&apos;ve successfully authenticated, but GitHub does not provide shell access. # 代表成功 执行”hexo g d” 不再提示输入github 的登录名和密码","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"04 hexo-admin 后台发布插件","date":"2019-10-15T20:43:00.000Z","path":"2019/10/16/04-hexo-其他/","text":"简介: 后台发布插件hexo-admin ，让发布博客更加简单 1 安装后台发布插件hexo-admin进行安装插件 1234npm install --save hexo-admin# 如果报错缺少组件，则缺少什么安装什么，npm install 加缺少的组件。# 例如 npm install minimatch@&quot;3.0.2&quot; # 组件安装后执行 npm update -d 配置文件设置用户名和密码设置密码在hexo的_config.yml配置hexo-admin 12345admin: username: test # 用户名 password_hash: $2a$10$5nJewMv0le/TQZRjx6inKud6EEHj50Pu0vdx5rPjc8rsoEqjhpgy. # 密码的哈希映射值，可以为空 secret: hey hexo #用以cookies安全； deployCommand: &apos;./admin_script/hexo-generate.sh&apos; 使用的是bcrypt哈希算法，使用下面的方式计算哈希值 123456[root@blog blog]# node &gt; const bcrypt = require(&apos;bcrypt-nodejs&apos;)undefined&gt; bcrypt.hashSync(&apos;test1&apos;) # 密码是test1&apos;$2a$10$5nJewMv0le/TQZRjx6inKud6EEHj50Pu0vdx5rPjc8rsoEqjhpgy.&apos;&gt; exit 这里的command对应于界面中的deploy按钮，在下面写上脚本，可以一键生成html页面，并提交到托管的地址，这就是hexo admin的核心思想！ 创建发布脚本1234mkdir admin_script # 创建文件夹cd admin_scripttouch hexo-generate.sh # 创建脚本 vim hexo-generate.sh # 写脚本 脚本内容， 12#!/usr/bin/env shhexo g 对脚本赋予权限 1chmod +x hexo-generate.sh 启动服务，进行测试12hexo server -dopen http://localhost:4000/admin/ 进入后台之后点击Deploy，里面的Deploy按钮是用来执行发布脚本的","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"03 hexo 的常用命令","date":"2019-10-15T20:40:00.000Z","path":"2019/10/16/03-hexo-的常用命令/","text":"简介: hexo 的常用命令 进入hexo 的根目录,才能执行下面的命令 hexo new 创建文章hexo new 命令用于新建文章，一般可以简写为 ==hexo n== 12hexo new &lt;title&gt; #title 必填参数，用以指定文章标题，如果参数值中含有空格，则需要使用双引号包围 1hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; hexo new page “页面名” 添加页面(菜单选项)123456// 添加分类页面hexo new page &quot;categories&quot;// 添加标签页面hexo new page “tags”// 添加关于页面hexo new page &quot;about 执行上面命令，会在source文件夹中生成相关的文件夹，文件夹内index.md文件 hexo new photo “文章名” 创建一个模板文章在新建文章时，Hexo 会根据 scaffolds 文件夹内相对应的文件来建立文件，例如： 1$ hexo new photo &quot;My Gallery&quot; 在执行这行指令时，Hexo 会尝试在 scaffolds 文件夹中寻找 photo.md，并根据其内容建立文章，以下是您可以在模版中使用的变量： 下面是默认创建使用的模板， 123456[hexo@blog scaffolds]$ cat post.md ---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:--- 1234567891011121314151617[hexo@blog blog]$ hexo new photo test3 # 谁用模板创建的效果INFO Created: ~/blog/source/_posts/test3.md[hexo@blog blog]$ cat ~/blog/source/_posts/test3.md---layout: phototitle: test3date: 2019-08-02 21:43:46tags:---[hexo@blog blog]$ cat ~/blog/source/_posts/testtest1.md test3.md [hexo@blog blog]$ cat ~/blog/source/_posts/test1.md ---title: test1date: 2019-08-02 21:15:01tags:--- hexo generate 生成静态文件将markdown 的文件转为静态的html 文件 一般可以简写为 ==hexo g== 1hexo generate hexo deploy 部署文件(上传远程服务器)一般可以简写为 ==hexo d== 说明 ：部署前需要修改 _config.yml 配置文件，下面以 git 为例进行说明 1234type: gitrepo: &lt;repository url&gt;branch: mastermessage: 自定义提交消息，默认为Site updated: &#123;&#123; now(&apos;YYYY-MM-DD HH:mm:ss&apos;) &#125;&#125; 生成文件后，提交给远程的服务器，例如git服务器 两个命令的作用是相同的,生成静态文件并部署 12345678hexo generate --deployhexo deploy --generatehexo deploy -ghexo server -ghexo g -dhexo d -g hexo clean 清理缓存文件1hexo clean 草稿和正式文章12hexo new draft &quot;new draft&quot; 新建草稿hexo publish [layout] &lt;title&gt; 变成正式文章 hexo server 启动hexo服务123456hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。 （默认端口4000，’ctrl + c’关闭server）hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo s --debug # 进行调试，默认访问本地的4000 端口 挂载到后台,并启用80端口，使用root 用户执行 1hexo server -p 80 &amp; 通用的模板1hexo server -p 80 &amp; #如果服务已经启动，则不许执行该步 1234567891011$ hexo new &quot;test1&quot; # 新建文章INFO Created: ~/blog/source/_posts/test1.md$ hexo new photo &quot;My Gallery&quot; # 创建模板文章$ hexo new page &quot;test1&quot; # 新建页面INFO Created: ~/blog/source/test1/index.md$ hexo generate #生成静态页面至public目录$ hexo deploy #将.deploy目录部署到GitHub hexo 首页文章只显示一部分这个只要在文章中加上 1&lt;!--more--&gt; 标记 ，该标记以后部分就不在显示了，只有展开全部才显示，这是hexo定义的。","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"01 安装hexo 的前期软件","date":"2019-10-15T20:38:00.000Z","path":"2019/10/16/01-安装hexo-的前期软件/","text":"环境: centos 7.3简介: 安装hexo的前期软件,安装Nodejs,npm,git 安装 nodejs 和npm1、到官网找对应版本的链接 12# 官方中文版本页面http://nodejs.cn/download/ 进入官网后，不要下载源代码，例如（node-v10.16.0.tar.gz）而是进入阿里云镜像，下载编译好的软件 2、在/opt目录下下载软件 12345cd /optwget http://cdn.npm.taobao.org/dist/node/v10.16.0/node-v10.16.0-linux-x64.tar.xz# 阿里云镜像wget https://nodejs.org/dist/v10.16.0/node-v10.16.0-linux-x64.tar.xz 3、解压软件 1tar -xf node-v10.8.0-linux-x64.tar.xz 4、建立软链接，变为全局npm 是安装node 后自带的软件，程序目录在源码安装目录下的bin目录下 12ln -s /opt/node-v10.16.0-linux-x64/bin/npm /usr/local/bin/ln -s /opt/node-v10.16.0-linux-x64/bin/node /usr/local/bin 5、查看node 和npm 版本 1234[root@my-site ~]# node -v v10.16.0[root@my-site ~]# npm -v6.9.0 ==安装nodejs 和npm使用稳定版本，使用下面的操作进行升级，否则可能会报错== 升级 node 和rpm安装 cnmpcnmp 是淘宝镜像，安装速度快 1npm install -g cnpm --registry=https://registry.npm.taobao.org 安装后，程序目录在源码安装目录下的bin目录下，建立软链接 1ln -s /opt/node-v10.8.0-linux-x64/bin/cnpm /usr/local/bin/ 查看版本 12345678[root@my-site ~]# cnpm -vcnpm@6.0.0 (/opt/node-v10.8.0-linux-x64/lib/node_modules/cnpm/lib/parse_argv.js)npm@6.4.0 (/opt/node-v10.8.0-linux-x64/lib/node_modules/cnpm/node_modules/npm/lib/npm.js)node@10.8.0 (/opt/node-v10.8.0-linux-x64/bin/node)npminstall@3.11.0 (/opt/node-v10.8.0-linux-x64/lib/node_modules/cnpm/node_modules/npminstall/lib/index.js)prefix=/opt/node-v10.8.0-linux-x64linux x64 3.10.0-514.26.2.el7.x86_64registry=https://registry.npm.taobao.org 安装n模块(用于升级nodejs)123npm install -g n命令目录：/opt/node-v10.16.0-linux-x64/bin 12345n stable # 升级node.js到最新稳定版n latest # 升级node.js到最新版n v7.10.0 # 升级node.js到制定版本 升级到稳定版本 12345678910[root@blog ~]# /opt/node-v10.16.0-linux-x64/bin/n stable installing : node-v10.16.1 mkdir : /usr/local/n/versions/node/10.16.1 fetch : https://nodejs.org/dist/v10.16.1/node-v10.16.1-linux-x64.tar.gz installed : v10.16.1[root@blog ~]# node -vv10.16.1[root@blog ~]# 升级npm1npm install npm@latest -g 1234567891011121314151617[root@blog ~]# npm version&#123; npm: &apos;6.9.0&apos;, ares: &apos;1.15.0&apos;, brotli: &apos;1.0.7&apos;, cldr: &apos;35.1&apos;, http_parser: &apos;2.8.0&apos;, icu: &apos;64.2&apos;, modules: &apos;64&apos;, napi: &apos;4&apos;, nghttp2: &apos;1.34.0&apos;, node: &apos;10.16.1&apos;, openssl: &apos;1.1.1c&apos;, tz: &apos;2019a&apos;, unicode: &apos;12.1&apos;, uv: &apos;1.28.0&apos;, v8: &apos;6.8.275.32-node.54&apos;, zlib: &apos;1.2.11&apos; &#125; 将目录存放到家目录12vim ~/.bash_profilePATH=$PATH:$HOME/bin:/opt/node-v10.8.0-linux-x64/bin 安装git123[root@blog ~]# yum install -y git[root@blog ~]# git --versiongit version 1.8.3.1","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"02 安装hexo和主题","date":"2019-10-15T20:29:00.000Z","path":"2019/10/16/02-安装hexo和主题/","text":"简介:安装hexo软件和hexo主题BlueLake 1 安装hexo1.1 安装hexo程序安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js (Should be at least nodejs 6.9) Git 安装hexo 程序 12# 官网安装命令$ npm install hexo-cli -g 安装成功后，在/opt/node-v10.16.0-linux-x64/lib/node_modules/hexo-cli目录下生成程序文件 使用hexo用户管理博客，所以将hexo命令目录添加到该用户的环境变量中,hexo用户才能使用hexo命令 12# vim .bash_profilePATH=$PATH:$HOME/.local/bin:$HOME/bin:/opt/node-v10.16.0-linux-x64/lib/node_modules/hexo-cli/bin hexo 和hexo-cli1234English:Hexo-cli is a cli service for create a project.when project was created, all the things doing with hexo module.you can see the hexo module and version in /package.json file in a new hexo project. hexo cli，负责创建 hexo 工程，创建了工程之后，就hexo-cli就全身而退了。hexo-cli 把接力棒交给了 hexo。hexo 模块，负责真正的项目管理工作（生成静态页面、预览等）。 1.2 建站hexo init 初始化必须在一个空目录下进行，否则报错 123456789# 创建建站目录su - hexomkdir blog······hexo init blog # 或者进行目录进行初始化cd bloghexo init 1.3 初始化目录下简介12345678_config.yml .gitignore node_modulespackage.jsonpackage-lock.jsonscaffoldssourcethemes 2 安装 hexo主题本主题使用的是Hexo 的BlueLake 主题这是一个简洁轻量化的响应式Hexo博客主题。 该主题的github位置 1https://github.com/chaooo/hexo-theme-BlueLake 2.1 安装主题软件123[hexo@blog ~]$ su - hexo[hexo@blog ~]$ cd blog[hexo@blog ~]$ git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/BlueLake 2.2 安装主题渲染器BlueLake是基于jade和stylus写的，所以需要安装hexo-renderer-jade和hexo-renderer-stylus来渲染。 12$ npm install hexo-renderer-jade@0.3.0 --save$ npm install hexo-renderer-stylus --save 123456# 查看已安装渲染器，安装过程中会有警告，不用管，安装上就行[hexo@blog blog]$ npm list |grep hexo-re├─┬ hexo-renderer-ejs@0.3.1├─┬ hexo-renderer-jade@0.3.0├─┬ hexo-renderer-marked@1.0.1├─┬ hexo-renderer-stylus@0.3.3 2.3 启用hexo 主题在Hexo根配置文件（blog/_config.yml）中把主题设置修改为BlueLake。 1theme: BlueLake 如果你想生成压缩后的css，在（hexo/_config.yml）中添加： 12stylus: compress: true 2.4 验证12# 调试模式hexo s --debug 在服务启动的过程， 当命令行输出中提示出：INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。如果是公网，使用公网IP 进行访问，注意开4000端口的防火墙。 2.5 进行主题更新12cd themes/BlueLakegit pull 3 主题配置3.1 配置网站头部显示文字打开根_config.yml，找到： 1234title: subtitle: description: author: title和subtitle分别是网站主标题和副标题，会显示在网站头部；description在网站界面不会显示，内容会加入网站源码的meta标签中，主要用于SEO；author就填写网站所有者的名字，会在网站底部的Copyright处有所显示。 3.2 设置语言该主题目前有七种语言：简体中文（zh-CN），繁体中文（zh-TW），英语（en），法语（fr-FR），德语（de-DE），韩语 （ko）,西班牙语（es-ES）； 例如选用简体中文，在根_config.yml配置如下： 根_config.yml_config.yml 1language: zh-CN 3.3 设置菜单打开主题_config.yml，找到： 1cd /home/hexo/blog/themes/BlueLake 主题_config.ymlthemes/BlueLake/_config.yml 12345678910111213menu: - page: home directory: . icon: fa-home - page: archive directory: archives/ icon: fa-archive # - page: about # directory: about/ # icon: fa-user - page: rss directory: atom.xml icon: fa-rss 主题默认是展示四个菜单，即主页home，归档archive，关于about，订阅RSS；about需要手动添加，RSS需要安装插件，若您并不需要，可以直接注释掉。每个页面底部的footer中联系博主的三个图标分别是邮箱，微博主页链接地址，GitHUb个人页链接地址，直接使用主题_config.yml中about页面的配置，若不需要about页面，只需要如下配置就好： 主题_config.ymlthemes/BlueLake/_config.yml 12345# About page about: email: ## 个人邮箱 weibo_url: ## 微博主页链接地址 github_url: ## github主页链接地址 3.3.1 添加about页此主题默认page页面是关于我页面的布局，在根目录下打开命令行窗口，生成一个关于我页面： 1234[hexo@blog blog]$ hexo new page aboutINFO Created: ~/blog/source/about/index.md[hexo@blog blog]$ pwd/home/hexo/blog 打开主题_config.yml，补全关于我页面的详细信息： 主题_config.ymlthemes/BlueLake/_config.yml 12345678# About page about: photo_url: ## 头像的链接地址 email: ## 个人邮箱 weibo_url: ## 微博主页链接地址 weibo_name: ## 微博用户名 github_url: ## github主页链接地址 github_name: ## github用户名 当然您也可以自定义重新布局about页面，只需要修改layout/page.jade模板就好。 3.3.2 安装 RSS(订阅) 和 sitemap(网站地图) 插件在根目录下打开命令行窗口： 123$ npm install hexo-generator-feed --save$ npm install hexo-generator-sitemap --save$ npm install hexo-generator-baidu-sitemap --save 添加主题_config.yml配置： 12345678910111213141516主题_config.ymlthemes/BlueLake/_config.ymlPlugins: hexo-generator-feed hexo-generator-sitemap hexo-generator-baidu-sitemapfeed: type: atom path: atom.xml limit: 20sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 2.4 添加本地搜索默认本地搜索是用原生JS写的，但还需要HEXO插件创建的JSON数据文件配合。安装插件hexo-generator-json-content来创建JSON数据文件： 123[hexo@blog blog]$ npm install hexo-generator-json-content@2.2.0 --save[hexo@blog blog]$ npm list | grep hexo-generator-json-content├─┬ hexo-generator-json-content@2.2.0 然后在根_config.yml添加配置(根配置文件中默认没有下面的内容)： 根_config.yml_config.yml 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 最后在主题_config.yml添加配置： 主题_config.ymlthemes/BlueLake/_config.yml 1local_search: true # 默认是开启的 3.5 修改站点图标站点图标存放在主题的Source目录下，已经默认为您准备了两张图片。您也可以自己设计站点LOGO。您需要准备一张ico格式并命名为** favicon.ico ，请将其放入hexo目录的source文件夹，建议大小：32px X 32px。您需要为苹果设备添加网站徽标，请命名为 apple-touch-icon.png **的图像放入hexo目录的“source”文件夹中，建议大小为：114px X 114px。(有很多网站都可以在线生成ico格式的图片。) 3.6 添加站点关键字请在hexo目录的根_config.yml中添加keywords字段，如： 根_config.yml_config.yml 1234567# Sitetitle: Hexosubtitle: 副标题description: 网站简要描述,如：Charles·Zheng&apos;s blog.keywords: 网站关键字, key, key1, key2, key3author: Charleslanguage: zh-CN 3.7 首页添加文章置顶在根目录下打开命令行窗口安装： 12$ npm uninstall hexo-generator-index --save$ npm install hexo-generator-index-pin-top --save 然后在需要置顶的文章的Front-matter中加上top: true即可。 123456---title: BlueLake博客主题的详细配置tags: [hexo,BlueLake]categories: hexo博客折腾top: true--- 3.8 更换主题背景和添加文章版权信息更换主题背景为深色 主题_config.ymlthemes/BlueLake/_config.yml 12# Theme tonedark: true #true/false 添加文章版权信息 主题_config.ymlthemes/BlueLake/_config.yml 12345# Theme tone#Copyrightcopyright: enable: true #true/false describe: 转载请注明出处(必须保留原文作者署名原文链接) #自定义描述替换默认描述 3.9 其他配置主题_config.yml的其他配置 12show_category_count——是否显示分类下的文章数。widgets_on_small_screens——是否在小屏显示侧边栏，若true,则侧边栏挂件将显示在底部。 主题_config.ymlthemes/BlueLake/_config.yml 12show_category_count: true widgets_on_small_screens: true 4.集成第三方服务4.1 添加评论目前主题集成六种第三方评论，分别是多说评论、Disqus评论、来必力评论、友言评论、网易云跟帖评论、畅言评论，多说马上就要停止服务了，友言好像也没怎么维护,目前我已把自己的博客评论从多说转移到畅言了。 123456789注册并获得代码。若使用多说评论，注册多说后获得short_name。若使用Disqus评论，注册Disqus后获得short_name。若使用来必力评论，注册来必力,获得data-uid。若使用友言评论，注册友言,获得uid。若使用网易云跟帖评论，注册网易云跟帖,获得productKey。若使用畅言评论，注册畅言，获得appid，appkey。配置主题_config.yml：主题_config.ymlthemes/BlueLake/_config.yml 12345678910#Cmmentscomment: duoshuo: ## duoshuo_shortname disqus: ## disqus_shortname livere: ## 来必力(data-uid) uyan: ## 友言(uid) cloudTie: ## 网易云跟帖(productKey) changyan: ## 畅言需在下方配置两个参数，此处不填。 appid: ## 畅言(appid) appkey: ##畅言(appkey) 4.2 百度统计登录百度统计，定位到站点的代码获取页面。复制//hm.baidu.com/hm.js?后面那串统计脚本id(假设为：8006843039519956000)配置主题_config.yml:主题_config.ymlthemes/BlueLake/_config.yml 1baidu_analytics: 8006843039519956000 注意： baidu_analytics不是你的百度id或者百度统计id如若使用谷歌统计，配置方法与百度统计类似。 4.3 卜算子阅读次数统计主题_config.ymlthemes/BlueLake/_config.yml 1busuanzi: true 若设置为true将计算文章的阅读量(Hits)，并显示在文章标题下的小手图标旁。 4.4 微博秀微博秀挂件的代码放在layout/_widget/weibo.jade下，需要您去微博开放平台获取您自己的微博秀代码来替换。 登录微博开放平台，选择微博秀。为了与主题风格统一，作如下配置基础设置：高400px；勾选宽度自适应；颜色选择白色；样式设置：主字色#333；链接色#40759b；鼠标悬停色#f7f8f8；模块设置：去掉标题、边框、粉丝的勾选框，只留微博。复制代码里src=””里引号包裹的内容，替换到layout/_widget/weibo.jadeweibo.jadelayout/_widget/weibo.jade 1234.widget .widget-title i(class=&apos;fa fa-weibo&apos;)= &apos; &apos; + __(&apos;新浪微博&apos;) iframe(width=&quot;100%&quot;,height=&quot;400&quot;,class=&quot;share_self&quot;,frameborder=&quot;0&quot;,scrolling=&quot;no&quot;,src=&quot;http://widget.weibo.com/weiboshow/index.php?language=&amp;width=0&amp;height=400&amp;fansRow=2&amp;ptype=1&amp;speed=0&amp;skin=5&amp;isTitle=0&amp;noborder=0&amp;isWeibo=1&amp;isFans=0&amp;uid=1700139362&amp;verifier=85be6061&amp;colors=d6f3f7,ffffff,333,40759b,f7f8f8&amp;dpc=1&quot;) 这只是为了和主题的风格统一，当然您也可以自由随意发挥。==注意：最主要是是要把src里uid=和verifier=后面的字段替换为您自己代码里的就好。== 备注 123本文章除了第一部分，大部分摘抄 郑超(Charles·Zheng)的内容原文链接: http://chaooo.github.io/2016/12/29/BlueLake博客主题的详细配置.html","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"Windows 10 安装 Anacoda3(64位)","date":"2019-10-15T20:08:00.000Z","path":"2019/10/16/Windows-10-安装-Anacoda3-64位/","text":"简介：用来管理不同版本的Python环境，自带很多常用软件包以及科学计算包。里面所包含的Jupyter Notebook是数据挖掘领域中最热门的工具。(例如Kaggle网站) 安装12官网地址，下载exe文件https://www.anaconda.com/distribution/#download-section 安装完成后，在最后一步，默认是没有添加环境变量的（添加环境变量的勾没有添加），在windows 上添加环境变量 12345D:\\Program Files\\Anaconda3D:\\Program Files\\Anaconda3\\ScriptsD:\\Program Files\\Anaconda3\\Library\\mingw-w64\\binD:\\Program Files\\Anaconda3\\Library\\usr\\binD:\\Program Files\\Anaconda3\\Library\\bin 测试在cmd上执行命令 123456789101112131415161718192021222324252627C:\\Users\\wutai&gt;conda info active environment : None user config file : C:\\Users\\wutai\\.condarc populated config files : C:\\Users\\wutai\\.condarc conda version : 4.7.10 conda-build version : 3.18.8 python version : 3.7.3.final.0 # python 版本 virtual packages : __cuda=9.2 base environment : D:\\Program Files\\Anaconda3 (writable) channel URLs : https://repo.anaconda.com/pkgs/main/win-64 https://repo.anaconda.com/pkgs/main/noarch https://repo.anaconda.com/pkgs/r/win-64 https://repo.anaconda.com/pkgs/r/noarch https://repo.anaconda.com/pkgs/msys2/win-64 https://repo.anaconda.com/pkgs/msys2/noarch package cache : D:\\Program Files\\Anaconda3\\pkgs C:\\Users\\wutai\\.conda\\pkgs C:\\Users\\wutai\\AppData\\Local\\conda\\conda\\pkgs envs directories : D:\\Program Files\\Anaconda3\\envs C:\\Users\\wutai\\.conda\\envs C:\\Users\\wutai\\AppData\\Local\\conda\\conda\\envs platform : win-64 user-agent : conda/4.7.10 requests/2.22.0 CPython/3.7.3 Windows/10 Windows/10.0.18362 administrator : False netrc file : None offline mode : False 安装完成后，Python 和pip 程序都默认安装完毕 123456789101112131415161718C:\\Users\\wutai&gt;pythonPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32Warning:This Python interpreter is in a conda environment, but the environment hasnot been activated. Libraries may fail to load. To activate this environmentplease see https://conda.io/activationType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; exit()C:\\Users\\wutai&gt;pip listPackage Version---------------------------------- ---------alabaster 0.7.12anaconda-client 1.7.2anaconda-navigator 1.9.7anaconda-project 0.8.3 Anaconda3 的命令12D:\\Program Files\\Anaconda3\\Scripts 存放可执行的文件，包括 conda 在cmd 上执行命令 123conda # 显示可执行的命令参数conda list # 显示所有的安装的包conda install requests # 安装requests 程序","tags":[{"name":"Anaconda","slug":"Anaconda","permalink":"http://yoursite.com/tags/Anaconda/"}]},{"title":"Hello World","date":"2019-08-29T08:59:21.306Z","path":"2019/08/29/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]