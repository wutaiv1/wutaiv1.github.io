<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title> 12 scrapy 框架基础知识 | 吴泰的博客</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden"> 12 scrapy 框架基础知识</h1><a id="logo" href="/.">吴泰的博客</a><p class="description">学海无涯,不进则退</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title"> 12 scrapy 框架基础知识</h1><div class="post-meta"><a href="/2019/10/27/12-scrapy-框架基础知识/#comments" class="comment-count"></a><p><span class="date">Oct 27, 2019</span><span><a href="/categories/爬虫/" class="category">爬虫</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>简介：scrapy 框架基础知识</p>
<a id="more"></a>


<h2 id="12-1-scrapy框架简介"><a href="#12-1-scrapy框架简介" class="headerlink" title="12.1 scrapy框架简介"></a>12.1 scrapy框架简介</h2><h3 id="12-1-1-介绍"><a href="#12-1-1-介绍" class="headerlink" title="12.1.1 介绍"></a>12.1.1 介绍</h3><p>Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。</p>
<p>整体架构大致如下：</p>
<p><img src="https://gitee.com/wutaipy/img/raw/master/2018/09/%E7%88%AC%E8%99%AB/scrapy%20%E6%A1%86%E6%9E%B6.png" alt="image"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Components：</span><br><span class="line"></span><br><span class="line">1、引擎(EGINE)</span><br><span class="line">引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。</span><br><span class="line"></span><br><span class="line">2、调度器(SCHEDULER)</span><br><span class="line">用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</span><br><span class="line"></span><br><span class="line">3、下载器(DOWLOADER)</span><br><span class="line">用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的</span><br><span class="line"></span><br><span class="line">4、爬虫(SPIDERS)</span><br><span class="line">SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求</span><br><span class="line"></span><br><span class="line">5、项目管道(ITEM PIPLINES)</span><br><span class="line">在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作</span><br><span class="line">下载器中间件(Downloader Middlewares)位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，</span><br><span class="line">你可用该中间件做以下几件事：</span><br><span class="line">  　　(1) process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website);</span><br><span class="line">  　　(2) change received response before passing it to a spider;</span><br><span class="line">  　　(3) send a new Request instead of passing received response to a spider;</span><br><span class="line">  　　(4) pass response to a spider without fetching a web page;</span><br><span class="line">  　　(5) silently drop some requests.</span><br><span class="line"></span><br><span class="line">6、爬虫中间件(Spider Middlewares)</span><br><span class="line">位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests）</span><br></pre></td></tr></table></figure>

<p>执行顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. Spider的yield将requests发送给Engine</span><br><span class="line">1. Engine对requests不做任何的处理就发送给Scheduler</span><br><span class="line">1. Scheduler（url调度器），生成requests交给Engine</span><br><span class="line">1. Engine拿到requests，通过middleware进行层层过滤发送给Downloader</span><br><span class="line">1. downloader在网上获取到response数据之后，又经过middleware进行层层过滤发送给Engine</span><br><span class="line">1. Engine获取到response之后，返回给Spider，Spider的parse（）方法对获取到的response数据进行处理解析出items或者requests</span><br><span class="line">1. 将解析出来的items或者requests发送给Engine</span><br><span class="line">1. Engine获取到items或者requests，将items发送给ITEMPIPELINES，将requests发送给Scheduler</span><br></pre></td></tr></table></figure>

<p><a href="https://docs.scrapy.org/en/latest/topics/architecture.htm" target="_blank" rel="noopener">官网链接</a></p>
<h3 id="12-1-2-安装"><a href="#12-1-2-安装" class="headerlink" title="12.1.2 安装"></a>12.1.2 安装</h3><p>Twisted是用Python实现的基于事件驱动的网络引擎框架，Twisted支持许多常见的传输及应用层协议，包括TCP、UDP、SSL/TLS、HTTP、IMAP、SSH、IRC以及FTP。安装scrapy 之前必须安装这个框架。</p>
<p>安装Twisted 注意:</p>
<ul>
<li>cp 后面跟的是python的版本，cp36 代表python3.6</li>
<li>注意系统版本，36位还是64位</li>
</ul>
<p>Pywin32是一个Python库，为python提供访问Windows API的扩展，提供了齐全的windows常量、接口、线程以及COM机制等等。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#Windows平台</span><br><span class="line">    1、pip3 install wheel #安装后，便支持通过wheel文件安装软件，</span><br><span class="line">        #wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs</span><br><span class="line">    3、pip3 install lxml</span><br><span class="line">    4、pip3 install pyopenssl</span><br><span class="line">    5、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/</span><br><span class="line">    6、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line">    7、执行pip3 install 下载目录\Twisted-17.9.0-cp36-cp36m-win_amd64.whl</span><br><span class="line">    8、pip3 install scrapy</span><br><span class="line">  </span><br><span class="line">#Linux平台</span><br><span class="line">    1、pip3 install scrapy</span><br></pre></td></tr></table></figure>

<h3 id="12-1-3-命令行工具"><a href="#12-1-3-命令行工具" class="headerlink" title="12.1.3 命令行工具"></a>12.1.3 命令行工具</h3><ul>
<li>查看帮助<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy -h   # 查看所有可用命令</span><br><span class="line">scrapy &lt;command&gt; -h</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#  有两种命令：其中Project-only必须切到项目文件夹下才能执行，而Global的命令则不需要</span><br><span class="line">    Global commands:</span><br><span class="line">        startproject #创建项目</span><br><span class="line">        genspider    #创建爬虫程序</span><br><span class="line">        settings     #如果是在项目目录下，则得到的是该项目的配置</span><br><span class="line">        runspider    #运行一个独立的python文件，不必创建项目</span><br><span class="line">        shell        #scrapy shell url地址  在交互式调试，如选择器规则正确与否</span><br><span class="line">        fetch        #独立于程单纯地爬取一个页面，可以拿到请求头</span><br><span class="line">        view         #下载完毕后直接弹出浏览器，以此可以分辨出哪些数据是ajax请求</span><br><span class="line">        version      #scrapy version 查看scrapy的版本，scrapy version -v查看scrapy依赖库的版本</span><br><span class="line">        </span><br><span class="line">    Project-only commands:</span><br><span class="line">        crawl        #运行爬虫，必须创建项目才行，确保配置文件中ROBOTSTXT_OBEY = False</span><br><span class="line">        check        #检测项目中有无语法错误</span><br><span class="line">        list         #列出项目中所包含的爬虫名</span><br><span class="line">        edit         #编辑器，一般不用</span><br><span class="line">        parse        #scrapy parse url地址 --callback 回调函数  #以此可以验证我们的回调函数是否正确</span><br><span class="line">        bench        #scrapy bentch压力测试</span><br><span class="line"></span><br><span class="line">#  官网链接</span><br><span class="line">    https://docs.scrapy.org/en/latest/topics/commands.html</span><br></pre></td></tr></table></figure>

<ul>
<li>startproject : 创建项目</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法 ： scrapy startproject &lt;project_name&gt; [project_dir]</span><br><span class="line">需要项目：否</span><br><span class="line"></span><br><span class="line">说明： roject_name在project_dir 目录下创建一个名为的新Scrapy项目。如果project_dir未指定，project_dir则与相同project_name</span><br></pre></td></tr></table></figure>

<p>示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br></pre></td></tr></table></figure>

<ul>
<li>genspider ：创建蜘蛛<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">语法: scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br><span class="line">需要项目：是</span><br><span class="line"></span><br><span class="line">spiders如果从项目内部调用，则在当前文件夹或当前项目的文件夹中创建一个新的蜘蛛。</span><br><span class="line">&lt;name&gt;参数设置为蜘蛛的name</span><br><span class="line">&lt;domain&gt;用于生成allowed_domains和start_urls蜘蛛的属性</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy genspider -l    # 创建时生成的模板</span><br><span class="line">Available templates:</span><br><span class="line">  basic</span><br><span class="line">  crawl</span><br><span class="line">  csvfeed</span><br><span class="line">  xmlfeed</span><br><span class="line"></span><br><span class="line">$ scrapy genspider example example.com   # 创建蜘蛛</span><br><span class="line">Created spider &apos;example&apos; using template &apos;basic&apos;</span><br><span class="line"></span><br><span class="line">$ scrapy genspider -t crawl scrapyorg scrapy.org   # 使用特定的模板</span><br><span class="line">Created spider &apos;scrapyorg&apos; using template &apos;crawl&apos;</span><br></pre></td></tr></table></figure>

<ul>
<li>crawl : 起动蜘蛛爬网<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">语法: scrapy crawl &lt;spider&gt;</span><br><span class="line">需要项目：是</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl myspider</span><br><span class="line"></span><br><span class="line">$ scrapy crawl example   # 上面创建爬虫名为example</span><br></pre></td></tr></table></figure>

<ul>
<li>check : 检查蜘蛛的完整性<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy check [-l] &lt;spider&gt;</span><br><span class="line">需要项目：是</span><br><span class="line"></span><br><span class="line">检查爬虫是否有错误，检查一些语法、import和warning等错误</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy check</span><br></pre></td></tr></table></figure>

<ul>
<li>list : 显示蜘蛛名称<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy list</span><br><span class="line">需要项目：是</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy list</span><br><span class="line">example</span><br><span class="line">quotes</span><br></pre></td></tr></table></figure>

<ul>
<li>edit : 编辑指定蜘蛛<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy edit &lt;spider&gt;</span><br><span class="line">需要项目：是</span><br><span class="line"></span><br><span class="line">使用EDITOR环境变量或（如果未设置）定义的编辑器编辑给定的蜘蛛EDITOR。</span><br><span class="line">windows上未设置相关变量，所有不能执行</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy edit spider1</span><br></pre></td></tr></table></figure>

<ul>
<li>fetch : 下载网页<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy fetch &lt;url&gt;</span><br><span class="line">需要项目：否</span><br><span class="line"></span><br><span class="line">使用Scrapy下载器下载给定的URL，并将内容写入标准输出。</span><br><span class="line"></span><br><span class="line">支持的选项：</span><br><span class="line">  --nolog: 不打印日志</span><br><span class="line">  --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛</span><br><span class="line">  --headers：打印响应的HTTP标头，而不是响应的正文</span><br><span class="line">  --no-redirect：不遵循HTTP 3xx重定向（默认为遵循它们）</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy fetch --nolog --headers &quot;http://www.baidu.com&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li>view : 请求一个url,Document下载下来，在本地保存文件，并在浏览器中打开<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy view &lt;url&gt;</span><br><span class="line">需要项目：否</span><br><span class="line"></span><br><span class="line">支持的选项：</span><br><span class="line">   --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛</span><br><span class="line">    --no-redirect：不遵循HTTP 3xx重定向（默认为遵循它们）</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy view &quot;https://www.bilibili.com/&quot;</span><br><span class="line"># 访问B站成功，并下载B站的主页面到本地，并在浏览器中打开</span><br><span class="line"># file:///C:/Users/wutai/AppData/Local/Temp/tmpgcmddg5j.html</span><br><span class="line"># 网页显示不完全，是由于一部分数据是使用ajax 进行加载的，是看不出完全的效果的</span><br><span class="line"></span><br><span class="line">scrapy view &quot;https://www.baidu.com/&quot;</span><br><span class="line"># 打开百度失败，从robots.txt 上禁止 &lt;GET https://www.baidu.com&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>shell : 命令行交互模式<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy shell [url]</span><br><span class="line">需要项目：否</span><br><span class="line"></span><br><span class="line">为给定的URL（如果给定）启动Scrapy shell，如果没有给定的URL，则为空。</span><br><span class="line"></span><br><span class="line">支持的选项：</span><br><span class="line">   --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛</span><br><span class="line">   -c code：评估shell中的代码，打印结果并退出</span><br><span class="line">   --no-redirect：不遵循HTTP 3xx重定向（默认为遵循重定向）；这只会影响您可能在命令行中作为参数传递的URL；</span><br><span class="line">                  一旦进入shell，fetch(url)默认情况下仍将遵循HTTP重定向。</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy shell http://www.example.com/some/page.html</span><br></pre></td></tr></table></figure>

<p>shell 使用见官网</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell</span><br></pre></td></tr></table></figure>

<ul>
<li>parse : 解析url<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy parse &lt;url&gt; [options]</span><br><span class="line">需要项目：是</span><br><span class="line"></span><br><span class="line">使用随--callback选项传递的方法（parse如果未提供），获取给定的URL并与处理它的蜘蛛解析。</span><br><span class="line"></span><br><span class="line">支持的选项：</span><br><span class="line">     --spider=SPIDER：绕过蜘蛛自动检测功能并强制使用特定蜘蛛</span><br><span class="line">     --a NAME=VALUE：设置蜘蛛参数（可以重复）</span><br><span class="line">     --callback或-c：蜘蛛方法用作解析响应的回调</span><br><span class="line">     --meta或-m：将传递给回调请求的其他请求元。这必须是有效的json字符串。例如：–meta =&apos;&#123;“ foo”：“ bar”&#125;&apos;</span><br><span class="line">     --cbkwargs：将传递给回调的其他关键字参数。这必须是有效的json字符串。示例：–cbkwargs =&apos;&#123;“ foo”：“ bar”&#125;&apos;</span><br><span class="line">     --pipelines：通过管道处理项目</span><br><span class="line">     --rules或-r：使用CrawlSpider 规则发现用于解析响应的回调（即蜘蛛方法）</span><br><span class="line">     --noitems：不显示刮擦的物品</span><br><span class="line">     --nolinks：不显示提取的链接</span><br><span class="line">     --nocolour：避免使用pygments给输出着色</span><br><span class="line">     --depth或-d：应递归遵循请求的深度级别（默认值：1）</span><br><span class="line">     --verbose或-v：显示每个深度级别的信息</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy parse http://www.example.com/ -c parse_item</span><br></pre></td></tr></table></figure>

<ul>
<li>settings : 获取Scrapy设置的值。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy settings [options]</span><br><span class="line">需要项目：否</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy settings --get BOT_NAME</span><br><span class="line">scrapybot</span><br><span class="line">$ scrapy settings --get DOWNLOAD_DELAY</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<ul>
<li>runspider : 无需创建项目即可运行包含在Python文件中的蜘蛛程序<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句法： scrapy runspider &lt;spider_file.py&gt;</span><br><span class="line">需要项目：否</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy runspider myspider.py</span><br><span class="line">[ ... spider starts crawling ... ]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>version : 查看scrapy 版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">句法： scrapy version [-v]</span><br><span class="line">需要项目：否</span><br><span class="line">打印Scrapy版本。如果与-v它一起使用，还可以打印Python，Twisted和Platform信息，这对于错误报告很有用。</span><br></pre></td></tr></table></figure>
</li>
<li><p>bench ：运行快速基准测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">语法： scrapy bench</span><br><span class="line">需要项目：否</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="12-1-4-创建、启动项目"><a href="#12-1-4-创建、启动项目" class="headerlink" title="12.1.4 创建、启动项目"></a>12.1.4 创建、启动项目</h3><ul>
<li><p>创建项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject quotetutorial  # 初始化创建quotetutorial 项目，会在该目录下自动创建目录quotetutorial；</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入项目后，创建一个新的蜘蛛</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd quotetutorial </span><br><span class="line">scrapy genspider quotes quotes.toscrape.com</span><br><span class="line"># 会自动创建一个爬虫项目文件quotes.py，quotes 是别名，别名不能与项目名相同；</span><br><span class="line"># 文件目录 quotetutorial/quotetutorial/spiders/quotes.py，  </span><br><span class="line"># genspider 后面第一个个是别名，第二个是域名</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动蜘蛛爬行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes  </span><br><span class="line">#  启动quotes.py项目 ，抓取目标网站数据，默认没有进行网页处理，所以速度很快</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>生成的quotes.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;quotes&apos;</span><br><span class="line">    allowed_domains = [&apos;quotes.toscrape.com&apos;]</span><br><span class="line">    start_urls = [&apos;http://quotes.toscrape.com/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>

<h3 id="12-1-5-目录结构"><a href="#12-1-5-目录结构" class="headerlink" title="12.1.5 目录结构"></a>12.1.5 目录结构</h3><p>创建项目后，内部有一个同项目名称的文件夹和.cfg 文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">project_name/</span><br><span class="line">   scrapy.cfg</span><br><span class="line">   project_name/</span><br><span class="line">       __init__.py</span><br><span class="line">       items.py</span><br><span class="line">       pipelines.py</span><br><span class="line">       settings.py</span><br><span class="line">       spiders/</span><br><span class="line">           __init__.py</span><br><span class="line">           爬虫1.py</span><br><span class="line">           爬虫2.py</span><br><span class="line">           爬虫3.py</span><br></pre></td></tr></table></figure>

<p>文件说明：<br>scrapy.cfg  项目的主配置信息，用来部署scrapy时使用，爬虫相关的配置信息在settings.py文件中。<br>items.py    设置数据存储模板，用于结构化数据，如：Django的Model<br>pipelines    数据处理行为，如：一般结构化的数据持久化<br>settings.py 配置文件，如：递归的层数、并发数，延迟下载等。强调:配置文件的选项必须大写否则视为无效，正确写法USER_AGENT=’xxxx’<br>spiders      爬虫目录，如：创建文件，编写爬虫规则  </p>
<p>注意：<br>1、一般创建爬虫文件时，以网站域名命名</p>
<p>2、默认只能在终端执行命令，为了更便捷操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#在项目根目录下新建：entrypoint.py</span><br><span class="line">from scrapy.cmdline import execute</span><br><span class="line">execute([&apos;scrapy&apos;, &apos;crawl&apos;, &apos;xiaohua&apos;])</span><br><span class="line">框架基础：spider类，选择器，</span><br></pre></td></tr></table></figure>

<h2 id="12-2-Spider类"><a href="#12-2-Spider类" class="headerlink" title="12.2 Spider类"></a>12.2 Spider类</h2><p>Spiders是定义如何抓取某个站点（或一组站点）的类，包括如何执行爬行（即跟随链接）以及如何从其页面中提取结构化数据（即抓取项目）。换句话说，Spiders是您为特定站点（或者在某些情况下，一组站点）爬网和解析页面定义自定义行为的地方。　</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">1、 生成初始的Requests来爬取第一个URLS，并且标识一个回调函数</span><br><span class="line">     第一个请求定义在start_requests()方法内默认从start_urls列表中获得url地址来生成Request请求，</span><br><span class="line">     默认的回调函数是parse方法。回调函数在下载完成返回response时自动触发</span><br><span class="line"></span><br><span class="line">2、 在回调函数中，解析response并且返回值</span><br><span class="line">     返回值可以4种：</span><br><span class="line">          包含解析数据的字典</span><br><span class="line">          Item对象</span><br><span class="line">          新的Request对象（新的Requests也需要指定一个回调函数）</span><br><span class="line">          或者是可迭代对象（包含Items或Request）</span><br><span class="line"></span><br><span class="line">3、在回调函数中解析页面内容</span><br><span class="line">   通常使用Scrapy自带的Selectors，但很明显你也可以使用Beutifulsoup，lxml或其他你爱用啥用啥。</span><br><span class="line"></span><br><span class="line">4、最后，针对返回的Items对象将会被持久化到数据库</span><br><span class="line">   通过Item Pipeline组件存到数据库：https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline）</span><br><span class="line">   或者导出到不同的文件（通过Feed exports：https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports）</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class QuotesSpider(scrapy.Spider):  </span><br><span class="line">    name = &apos;quotes&apos;   </span><br><span class="line">    allowed_domains = [&apos;quotes.toscrape.com&apos;]  </span><br><span class="line">    start_urls = [&apos;http://quotes.toscrape.com/&apos;]</span><br><span class="line">    </span><br><span class="line">    custom_settings = &#123;   # 覆盖全局配置</span><br><span class="line">        DEFAULT_REQUEST_HEADERS : &#123;</span><br><span class="line">           &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</span><br><span class="line">           &apos;Accept-Language&apos;: &apos;en&apos;,</span><br><span class="line">   &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">    def parse(self, response):  </span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scrapy.spiders.Spider</span><br><span class="line">    name : 用于定义此蜘蛛的名称</span><br><span class="line">    allowed_domains : 包含此蜘蛛可以爬网的域的字符串的可选列表。</span><br><span class="line">                      如果OffsiteMiddleware启用，则不遵循对不属于此列表中指定的域名（或其子域）的URL的请求 。</span><br><span class="line">    start_urls : 起始url 列表，拿到每个链接，进行get请求</span><br><span class="line">    custom_settings ： 特定的字典配置，会覆盖项目的配置settings.py</span><br><span class="line">    crawler ：  封装了许多组件以供其单项访问（例如扩展，中间件，信号管理器等）</span><br><span class="line">    settings ： 控制核心插件、组件等配置</span><br><span class="line">    from_crawler ： 这是Scrapy用于创建蜘蛛的类方法。</span><br><span class="line">    start_requests : 此方法必须返回带有第一个Request的Iterable，以对该蜘蛛进行爬网。</span><br><span class="line">    ....</span><br></pre></td></tr></table></figure>

<p>如果您需要通过使用POST请求登录来开始，则可以执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class MySpider(scrapy.Spider):</span><br><span class="line">    name = &apos;myspider&apos;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        return [scrapy.FormRequest(&quot;http://www.example.com/login&quot;,</span><br><span class="line">                                   formdata=&#123;&apos;user&apos;: &apos;john&apos;, &apos;pass&apos;: &apos;secret&apos;&#125;,</span><br><span class="line">                                   callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line">    def logged_in(self, response):</span><br><span class="line">        # here you would extract links to follow and return Requests for</span><br><span class="line">        # each of them, with another callback</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>

<p>从单个回调返回多个请求和项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MySpider(scrapy.Spider):</span><br><span class="line">    name = &apos;example.com&apos;</span><br><span class="line">    allowed_domains = [&apos;example.com&apos;]</span><br><span class="line">    start_urls = [</span><br><span class="line">        &apos;http://www.example.com/1.html&apos;,</span><br><span class="line">        &apos;http://www.example.com/2.html&apos;,</span><br><span class="line">        &apos;http://www.example.com/3.html&apos;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for h3 in response.xpath(&apos;//h3&apos;).getall():</span><br><span class="line">            yield &#123;&quot;title&quot;: h3&#125;</span><br><span class="line"></span><br><span class="line">        for href in response.xpath(&apos;//a/@href&apos;).getall():</span><br><span class="line">            yield scrapy.Request(response.urljoin(href), self.parse)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MySpider(scrapy.Spider):</span><br><span class="line">    name = &apos;myspider&apos;</span><br><span class="line"></span><br><span class="line">    def __init__(self, category=None, *args, **kwargs):</span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [&apos;http://www.example.com/categories/%s&apos; % category]</span><br><span class="line">        # ...</span><br></pre></td></tr></table></figure>

<p>默认的<strong>init</strong>方法将使用任何蜘蛛参数，并将其作为属性复制到蜘蛛。上面的示例也可以编写如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MySpider(scrapy.Spider):</span><br><span class="line">    name = &apos;myspider&apos;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        yield scrapy.Request(&apos;http://www.example.com/categories/%s&apos; % self.category)</span><br></pre></td></tr></table></figure>

<h2 id="12-3-选择器"><a href="#12-3-选择器" class="headerlink" title="12.3 选择器"></a>12.3 选择器</h2><p>为了解释如何使用选择器，我们将使用Scrapy shell（提供交互式测试）和Scrapy文档服务器中的示例页面，</p>
<p>这是它的HTML代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line"> &lt;head&gt;</span><br><span class="line">  &lt;base href=&apos;http://example.com/&apos; /&gt;</span><br><span class="line">  &lt;title&gt;Example website&lt;/title&gt;</span><br><span class="line"> &lt;/head&gt;</span><br><span class="line"> &lt;body&gt;</span><br><span class="line">  &lt;div id=&apos;images&apos;&gt;</span><br><span class="line">   &lt;a href=&apos;image1.html&apos;&gt;Name: My image 1 &lt;br /&gt;&lt;img src=&apos;image1_thumb.jpg&apos; /&gt;&lt;/a&gt;</span><br><span class="line">   &lt;a href=&apos;image2.html&apos;&gt;Name: My image 2 &lt;br /&gt;&lt;img src=&apos;image2_thumb.jpg&apos; /&gt;&lt;/a&gt;</span><br><span class="line">   &lt;a href=&apos;image3.html&apos;&gt;Name: My image 3 &lt;br /&gt;&lt;img src=&apos;image3_thumb.jpg&apos; /&gt;&lt;/a&gt;</span><br><span class="line">   &lt;a href=&apos;image4.html&apos;&gt;Name: My image 4 &lt;br /&gt;&lt;img src=&apos;image4_thumb.jpg&apos; /&gt;&lt;/a&gt;</span><br><span class="line">   &lt;a href=&apos;image5.html&apos;&gt;Name: My image 5 &lt;br /&gt;&lt;img src=&apos;image5_thumb.jpg&apos; /&gt;&lt;/a&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line"> &lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<h4 id="进入shell"><a href="#进入shell" class="headerlink" title="进入shell"></a>进入shell</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先，让我们打开shell：</span><br><span class="line"></span><br><span class="line">1 scrapy shell https://doc.scrapy.org/en/latest/_static/selectors-sample1.html</span><br><span class="line"># 这个网址是官网提供进行选择器测试的网页，</span><br><span class="line"># 然后，在shell加载之后，您将获得响应作为response shell变量，并在response.selector属性中附加选择器，selectctor是一个类。</span><br></pre></td></tr></table></figure>

<h4 id="初步使用"><a href="#初步使用" class="headerlink" title="初步使用"></a>初步使用</h4><p>原始方式response.selector.xpath()和response.selector.css()<br>便捷方式 response.xpath()和response.css()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">让我们构建一个XPath来选择title标签内的文本：</span><br><span class="line"></span><br><span class="line">In [1]: response.selector.xpath(&apos;//title/text()&apos;)</span><br><span class="line">Out[1]: [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</span><br><span class="line"># 返回的是selector的列表，使用extract_first() 方法提取内容</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用XPath和CSS查询响应非常常见，响应包括两个便捷快捷方式：response.xpath()和response.css()：</span><br><span class="line"># 返回的都是SelectorList，这是新的选择列表。</span><br><span class="line"># 方式一： 使用response.xpath() 获取内容</span><br><span class="line">In [2]: response.selector.xpath(&apos;//title/text()&apos;).extract_first()   </span><br><span class="line">Out[2]: &apos;Example website&apos;</span><br><span class="line"># 使用extract_first 方法获取文本内容（extract:提取）</span><br><span class="line"></span><br><span class="line"># 方式二：使用response.css 获取内容</span><br><span class="line">In [3]: response.selector.css(&apos;title::text&apos;).extract_first()</span><br><span class="line">Out[3]: &apos;Example website&apos;</span><br><span class="line"></span><br><span class="line"># 为了方便使用，可以不加selector 方法，直接使用</span><br><span class="line">In [4]: response.xpath(&apos;//title/text()&apos;)</span><br><span class="line">Out[4]: [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</span><br><span class="line">In [5]: response.css(&apos;title::text&apos;)</span><br><span class="line">In [5]:[&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</span><br><span class="line">正如你所看到的，.xpath()并且.css()方法返回一个 SelectorList实例，这是新的选择列表。此API可用于快速选择嵌套数据：</span><br></pre></td></tr></table></figure>

<p>获取img 标签中src 属性内容，生成一个文本列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">In [9]: response.css(&apos;img&apos;)   # 通过img 标签，获取SelectorList</span><br><span class="line">Out[9]: </span><br><span class="line">[&lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;descendant-or-self::img&apos; data=&apos;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&apos;&gt;]</span><br><span class="line"></span><br><span class="line">In [10]: response.css(&apos;img&apos;).xpath(&apos;@src&apos;) # 获取标签内src属性内容，生成的是SelectorList</span><br><span class="line">Out[10]: </span><br><span class="line">[&lt;Selector xpath=&apos;@src&apos; data=&apos;image1_thumb.jpg&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;@src&apos; data=&apos;image2_thumb.jpg&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;@src&apos; data=&apos;image3_thumb.jpg&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;@src&apos; data=&apos;image4_thumb.jpg&apos;&gt;,</span><br><span class="line"> &lt;Selector xpath=&apos;@src&apos; data=&apos;image5_thumb.jpg&apos;&gt;]</span><br><span class="line"></span><br><span class="line">In [11]: response.css(&apos;img&apos;).xpath(&apos;@src&apos;).extract() # extract() 提取SelectorList中的文本</span><br><span class="line">Out[11]: </span><br><span class="line">[&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> &apos;image2_thumb.jpg&apos;,</span><br><span class="line"> &apos;image3_thumb.jpg&apos;,</span><br><span class="line"> &apos;image4_thumb.jpg&apos;,</span><br><span class="line"> &apos;image5_thumb.jpg&apos;]</span><br></pre></td></tr></table></figure>

<h4 id="xpath和css结合使用"><a href="#xpath和css结合使用" class="headerlink" title="xpath和css结合使用"></a>xpath和css结合使用</h4><p>css(‘img::attr(src)’) ：  ::attr 是获取标签属性内容 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 结合xpath和css 提取数据，xpath 定位到块的内容，css 定位到具体标签内容</span><br><span class="line">In [21]: response.xpath(&apos;//div[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(src)&apos;).extract()</span><br><span class="line">Out[21]: </span><br><span class="line">[&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> &apos;image2_thumb.jpg&apos;,</span><br><span class="line"> &apos;image3_thumb.jpg&apos;,</span><br><span class="line"> &apos;image4_thumb.jpg&apos;,</span><br><span class="line"> &apos;image5_thumb.jpg&apos;]</span><br></pre></td></tr></table></figure>

<h4 id="提取SelectorList-的data数据，转为文本"><a href="#提取SelectorList-的data数据，转为文本" class="headerlink" title="提取SelectorList 的data数据，转为文本"></a>提取SelectorList 的data数据，转为文本</h4><p>extract()： 是获取所有<br>extract_first() : 获取第一个元素<br>extract_first(default=’’) : 获取第一个元素,如果没有则显示default 内的内容</p>
<p>提取id=”images”的div下的a标签的文本内容 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [16]: response.xpath(&apos;//div[@id=&quot;images&quot;]/a/text()&apos;).extract()</span><br><span class="line">Out[16]: </span><br><span class="line">[&apos;Name: My image 1 &apos;,</span><br><span class="line"> &apos;Name: My image 2 &apos;,</span><br><span class="line"> &apos;Name: My image 3 &apos;,</span><br><span class="line"> &apos;Name: My image 4 &apos;,</span><br><span class="line"> &apos;Name: My image 5 &apos;]</span><br><span class="line"></span><br><span class="line">In [17]: response.xpath(&apos;//div[@id=&quot;images&quot;]/a/text()&apos;).extract_first()</span><br><span class="line">Out[17]: &apos;Name: My image 1 &apos;</span><br></pre></td></tr></table></figure>

<h4 id="提取属性"><a href="#提取属性" class="headerlink" title="提取属性"></a>提取属性</h4><p>使用css和xpath  分别提取a标签下的href属性内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [23]: response.xpath(&apos;//a/@href&apos;).extract()</span><br><span class="line">Out[23]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]</span><br><span class="line"></span><br><span class="line">In [24]: response.css(&apos;a::attr(href)&apos;).extract()</span><br><span class="line">Out[24]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]</span><br></pre></td></tr></table></figure>

<h4 id="获取标签文本"><a href="#获取标签文本" class="headerlink" title="获取标签文本"></a>获取标签文本</h4><p>获取a标签下的文本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [26]: response.css(&apos;a::text&apos;).extract()</span><br><span class="line">Out[26]: </span><br><span class="line">[&apos;Name: My image 1 &apos;,</span><br><span class="line"> &apos;Name: My image 2 &apos;,</span><br><span class="line"> &apos;Name: My image 3 &apos;,</span><br><span class="line"> &apos;Name: My image 4 &apos;,</span><br><span class="line"> &apos;Name: My image 5 &apos;]</span><br><span class="line"></span><br><span class="line">In [28]: response.xpath(&apos;//a/text()&apos;).extract()</span><br><span class="line">Out[28]: </span><br><span class="line">[&apos;Name: My image 1 &apos;,</span><br><span class="line"> &apos;Name: My image 2 &apos;,</span><br><span class="line"> &apos;Name: My image 3 &apos;,</span><br><span class="line"> &apos;Name: My image 4 &apos;,</span><br><span class="line"> &apos;Name: My image 5 &apos;]</span><br></pre></td></tr></table></figure>

<h4 id="标签属性包含匹配"><a href="#标签属性包含匹配" class="headerlink" title="标签属性包含匹配"></a>标签属性包含匹配</h4><p>a标签的href属性名称包含image的所有超链接</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># href*=image ： href属性内包含image数据</span><br><span class="line">In [33]: response.css(&apos;a[href*=image]::attr(href)&apos;).extract()</span><br><span class="line">Out[33]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]</span><br><span class="line"></span><br><span class="line"># contains(@属性名,值)   </span><br><span class="line">In [34]: response.xpath(&apos;//a[contains(@href,&quot;image&quot;)]/@href&apos;).extract()</span><br><span class="line">Out[34]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]</span><br></pre></td></tr></table></figure>

<p>属性href = image 的a 标签下的img标签的src属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 注意中间有个空格</span><br><span class="line">In [37]: response.css(&apos;a[href*=image] img::attr(src)&apos;).extract()</span><br><span class="line">Out[37]: </span><br><span class="line">[&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> &apos;image2_thumb.jpg&apos;,</span><br><span class="line"> &apos;image3_thumb.jpg&apos;,</span><br><span class="line"> &apos;image4_thumb.jpg&apos;,</span><br><span class="line"> &apos;image5_thumb.jpg&apos;]</span><br><span class="line"></span><br><span class="line">In [38]: response.xpath(&apos;//a[contains(@href,&quot;image&quot;)]/img/@src&apos;).extract()</span><br><span class="line">Out[38]: </span><br><span class="line">[&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> &apos;image2_thumb.jpg&apos;,</span><br><span class="line"> &apos;image3_thumb.jpg&apos;,</span><br><span class="line"> &apos;image4_thumb.jpg&apos;,</span><br><span class="line"> &apos;image5_thumb.jpg&apos;]</span><br></pre></td></tr></table></figure>

<h4 id="使用正则提取字符串"><a href="#使用正则提取字符串" class="headerlink" title="使用正则提取字符串"></a>使用正则提取字符串</h4><p>对SelectorList 使用正则，非extract()后使用正则  </p>
<p>对a标签下内容进行提取字符串</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [43]: response.xpath(&apos;//a/text()&apos;).re(&apos;Name\:(.*)&apos;)</span><br><span class="line">Out[43]: </span><br><span class="line">[&apos; My image 1 &apos;,</span><br><span class="line"> &apos; My image 2 &apos;,</span><br><span class="line"> &apos; My image 3 &apos;,</span><br><span class="line"> &apos; My image 4 &apos;,</span><br><span class="line"> &apos; My image 5 &apos;]</span><br><span class="line"></span><br><span class="line">In [44]: response.css(&apos;a::text&apos;).re(&apos;Name\:(.*)&apos;)</span><br><span class="line">Out[44]: </span><br><span class="line">[&apos; My image 1 &apos;,</span><br><span class="line"> &apos; My image 2 &apos;,</span><br><span class="line"> &apos; My image 3 &apos;,</span><br><span class="line"> &apos; My image 4 &apos;,</span><br><span class="line"> &apos; My image 5 &apos;]</span><br><span class="line"></span><br><span class="line">In [45]: response.css(&apos;a::text&apos;).re_first(&apos;Name\:(.*)&apos;)   # 提取第一个</span><br><span class="line">Out[45]: &apos; My image 1 &apos;</span><br></pre></td></tr></table></figure>

<h2 id="12-4-Item-项目"><a href="#12-4-Item-项目" class="headerlink" title="12.4 Item(项目)"></a>12.4 Item(项目)</h2><p>抓取的主要目标是从非结构化源（通常是网页）中提取结构化数据。Scrapy蜘蛛可以像Python一样返回提取的数据。虽然方便和熟悉，但P很容易在字段名称中输入拼写错误或返回不一致的数据，尤其是在具有许多蜘蛛的较大项目中。</p>
<p>为了定义通用输出数据格式，Scrapy提供了Item类。 Item对象是用于收集抓取数据的简单容器。它们提供类似字典的 API，并具有用于声明其可用字段的方便语法。</p>
<h3 id="12-4-1-声明项目"><a href="#12-4-1-声明项目" class="headerlink" title="12.4.1 声明项目"></a>12.4.1 声明项目</h3><p>使用简单的类定义语法和Field 对象声明项。这是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"> </span><br><span class="line">class Product(scrapy.Item):</span><br><span class="line">   name = scrapy.Field()</span><br><span class="line">   price = scrapy.Field()</span><br><span class="line">   stock = scrapy.Field()</span><br><span class="line">   last_updated = scrapy.Field(serializer=str)</span><br></pre></td></tr></table></figure>

<p>注意那些熟悉Django的人会注意到Scrapy Items被宣告类似于Django Models，除了Scrapy Items更简单，因为没有不同字段类型的概念。</p>
<h3 id="12-4-2-项目字段"><a href="#12-4-2-项目字段" class="headerlink" title="12.4.2 项目字段"></a>12.4.2 项目字段</h3><p>Field对象用于指定每个字段的元数据。例如，last_updated上面示例中说明的字段的序列化函数。</p>
<p>您可以为每个字段指定任何类型的元数据。Field对象接受的值没有限制。出于同样的原因，没有所有可用元数据键的参考列表。</p>
<p>Field对象中定义的每个键可以由不同的组件使用，只有那些组件知道它。您也可以根据Field自己的需要定义和使用项目中的任何其他键。</p>
<p>Field对象的主要目标是提供一种在一个地方定义所有字段元数据的方法。通常，行为取决于每个字段的那些组件使用某些字段键来配置该行为。</p>
<h3 id="12-4-3-使用项目"><a href="#12-4-3-使用项目" class="headerlink" title="12.4.3 使用项目"></a>12.4.3 使用项目</h3><p>以下是使用上面声明的Product项目对项目执行的常见任务的一些示例 。您会注意到API与dict API非常相似。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; product = Product(name=&apos;Desktop PC&apos;, price=1000)</span><br><span class="line">&gt;&gt;&gt; print product</span><br><span class="line">Product(name=&apos;Desktop PC&apos;, price=1000)</span><br><span class="line">获取字段值</span><br><span class="line">&gt;&gt;&gt; product[&apos;name&apos;]</span><br><span class="line">Desktop PC</span><br><span class="line">&gt;&gt;&gt; product.get(&apos;name&apos;)</span><br><span class="line">Desktop PC</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product[&apos;price&apos;]</span><br><span class="line">1000</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product[&apos;last_updated&apos;]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">...</span><br><span class="line">KeyError: &apos;last_updated&apos;</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product.get(&apos;last_updated&apos;, &apos;not set&apos;)</span><br><span class="line">not set</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product[&apos;lala&apos;] # getting unknown field</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">...</span><br><span class="line">KeyError: &apos;lala&apos;</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product.get(&apos;lala&apos;, &apos;unknown field&apos;)</span><br><span class="line">&apos;unknown field&apos;</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; &apos;name&apos; in product # is name field populated?</span><br><span class="line">True</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; &apos;last_updated&apos; in product # is last_updated populated?</span><br><span class="line">False</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; &apos;last_updated&apos; in product.fields # is last_updated a declared field?</span><br><span class="line">True</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; &apos;lala&apos; in product.fields # is lala a declared field?</span><br><span class="line">False</span><br><span class="line">设定字段值</span><br><span class="line">&gt;&gt;&gt; product[&apos;last_updated&apos;] = &apos;today&apos;</span><br><span class="line">&gt;&gt;&gt; product[&apos;last_updated&apos;]</span><br><span class="line">today</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product[&apos;lala&apos;] = &apos;test&apos; # setting unknown field</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">...</span><br><span class="line">KeyError: &apos;Product does not support field: lala&apos;</span><br><span class="line">访问所有填充值</span><br><span class="line">要访问所有填充值，只需使用典型的dict API：</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product.keys()</span><br><span class="line">[&apos;price&apos;, &apos;name&apos;]</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product.items()</span><br><span class="line">[(&apos;price&apos;, 1000), (&apos;name&apos;, &apos;Desktop PC&apos;)]</span><br><span class="line">其他常见任务</span><br><span class="line">复制项目：</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product2 = Product(product)</span><br><span class="line">&gt;&gt;&gt; print product2</span><br><span class="line">Product(name=&apos;Desktop PC&apos;, price=1000)</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; product3 = product2.copy()</span><br><span class="line">&gt;&gt;&gt; print product3</span><br><span class="line">Product(name=&apos;Desktop PC&apos;, price=1000)</span><br><span class="line">从项目创建dicts：</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; dict(product) # create a dict from all populated values</span><br><span class="line">&#123;&apos;price&apos;: 1000, &apos;name&apos;: &apos;Desktop PC&apos;&#125;</span><br><span class="line">从dicts创建项目：</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; Product(&#123;&apos;name&apos;: &apos;Laptop PC&apos;, &apos;price&apos;: 1500&#125;)</span><br><span class="line">Product(price=1500, name=&apos;Laptop PC&apos;)</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; Product(&#123;&apos;name&apos;: &apos;Laptop PC&apos;, &apos;lala&apos;: 1500&#125;) # warning: unknown field in dict</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">...</span><br><span class="line">KeyError: &apos;Product does not support field: lala&apos;</span><br></pre></td></tr></table></figure>

<h3 id="12-4-4-扩展项目"><a href="#12-4-4-扩展项目" class="headerlink" title="12.4.4 扩展项目"></a>12.4.4 扩展项目</h3><p>您可以通过声明原始Item的子类来扩展Items（以添加更多字段或更改某些字段的某些元数据）。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class DiscountedProduct(Product):</span><br><span class="line">      discount_percent = scrapy.Field(serializer=str)</span><br><span class="line">      discount_expiration_date = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h2 id="12-5-Item-PipeLine"><a href="#12-5-Item-PipeLine" class="headerlink" title="12.5 Item PipeLine"></a>12.5 Item PipeLine</h2><p> 项目下的piplines.py文件</p>
<p>在一个项目被蜘蛛抓取之后，它被发送到项目管道，该项目管道通过顺序执行的几个组件处理它。</p>
<p>每个项目管道组件（有时简称为“项目管道”）是一个实现简单方法的Python类。他们收到一个项目并对其执行操作，同时决定该项目是否应该继续通过管道或被丢弃并且不再处理。</p>
<p>项目管道的典型用途是：</p>
<ul>
<li><p>cleansing HTML data</p>
</li>
<li><p>validating scraped data (checking that the items contain certain fields)</p>
</li>
<li><p>checking for duplicates (and dropping them)</p>
</li>
<li><p>storing the scraped item in a database</p>
</li>
<li><p>清理HTML数据</p>
</li>
<li><p>验证抓取的数据（检查项目是否包含某些字段）</p>
</li>
<li><p>检查重复项（并将其删除）</p>
</li>
<li><p>将刮擦的物品存储在数据库中</p>
</li>
</ul>
<h3 id="12-5-1-编写自己的项目管道"><a href="#12-5-1-编写自己的项目管道" class="headerlink" title="12.5.1 编写自己的项目管道"></a>12.5.1 编写自己的项目管道</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">每个项管道组件都是一个必须实现以下方法的Python类：</span><br><span class="line"></span><br><span class="line">process_item（self，项目，蜘蛛）</span><br><span class="line">为每个项目管道组件调用此方法。process_item() </span><br><span class="line"></span><br><span class="line">必须要么：返回带数据的dict，返回一个Item （或任何后代类）对象，返回Twisted Deferred或引发 DropItem异常。丢弃的项目不再由其他管道组件处理。</span><br><span class="line"></span><br><span class="line">此外，他们还可以实现以下方法：</span><br><span class="line"></span><br><span class="line">open_spider（self，蜘蛛）</span><br><span class="line">打开蜘蛛时会调用此方法。</span><br><span class="line"></span><br><span class="line">close_spider（self，蜘蛛）</span><br><span class="line">当蜘蛛关闭时调用此方法。</span><br><span class="line"></span><br><span class="line">from_crawler（cls，crawler ）</span><br><span class="line">如果存在，则调用此类方法以从a创建管道实例Crawler。它必须返回管道的新实例。Crawler对象提供对所有Scrapy核心组件的访问，</span><br><span class="line">如设置和信号; 它是管道访问它们并将其功能挂钩到Scrapy的一种方式。</span><br></pre></td></tr></table></figure>

<h3 id="12-5-2-项目管道示例"><a href="#12-5-2-项目管道示例" class="headerlink" title="12.5.2 项目管道示例"></a>12.5.2 项目管道示例</h3><h4 id="1-价格验证和丢弃物品没有价格"><a href="#1-价格验证和丢弃物品没有价格" class="headerlink" title="(1) 价格验证和丢弃物品没有价格"></a>(1) 价格验证和丢弃物品没有价格</h4><p>让我们看看下面的假设管道，它调整 price那些不包含增值税（price_excludes_vat属性）的项目的属性，并删除那些不包含价格的项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"> </span><br><span class="line">class PricePipeline(object):</span><br><span class="line"> </span><br><span class="line">    vat_factor = 1.15</span><br><span class="line"> </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if item[&apos;price&apos;]:</span><br><span class="line">            if item[&apos;price_excludes_vat&apos;]:</span><br><span class="line">                item[&apos;price&apos;] = item[&apos;price&apos;] * self.vat_factor</span><br><span class="line">            return item</span><br><span class="line">        else:</span><br><span class="line">            raise DropItem(&quot;Missing price in %s&quot; % item)</span><br></pre></td></tr></table></figure>

<h4 id="2-将项目写入JSON文件"><a href="#2-将项目写入JSON文件" class="headerlink" title="(2) 将项目写入JSON文件"></a>(2) 将项目写入JSON文件</h4><p>以下管道将所有已删除的项目（来自所有蜘蛛）存储到一个items.jl文件中，每行包含一个以JSON格式序列化的项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"> </span><br><span class="line">class JsonWriterPipeline(object):</span><br><span class="line"> </span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.file = open(&apos;items.jl&apos;, &apos;w&apos;)</span><br><span class="line"> </span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.file.close()</span><br><span class="line"> </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        line = json.dumps(dict(item)) + &quot;\n&quot;</span><br><span class="line">        self.file.write(line)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>

<p>注意JsonWriterPipeline的目的只是介绍如何编写项目管道。如果您确实要将所有已删除的项目存储到JSON文件中，则应使用Feed导出。</p>
<h4 id="3-将项目写入数据库"><a href="#3-将项目写入数据库" class="headerlink" title="(3) 将项目写入数据库"></a>(3) 将项目写入数据库</h4><p>在这个例子中，我们将使用pymongo将项目写入MongoDB。MongoDB地址和数据库名称在Scrapy设置中指定; MongoDB集合以item类命名。</p>
<p>这个例子的要点是展示如何使用from_crawler() 方法以及如何正确地清理资源：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pymongo</span><br><span class="line"> </span><br><span class="line">class MongoPipeline(object):</span><br><span class="line"> </span><br><span class="line">    collection_name = &apos;scrapy_items&apos;</span><br><span class="line"> </span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"> </span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</span><br><span class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;)</span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"> </span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line"> </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>

<h4 id="4-重复过滤"><a href="#4-重复过滤" class="headerlink" title="(4) 重复过滤"></a>(4) 重复过滤</h4><p>一个过滤器，用于查找重复项目，并删除已处理的项目。假设我们的项目具有唯一ID，但我们的蜘蛛会返回具有相同ID的多个项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"> </span><br><span class="line">class DuplicatesPipeline(object):</span><br><span class="line"> </span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"> </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if item[&apos;id&apos;] in self.ids_seen:</span><br><span class="line">            raise DropItem(&quot;Duplicate item found: %s&quot; % item)</span><br><span class="line">        else:</span><br><span class="line">            self.ids_seen.add(item[&apos;id&apos;])</span><br><span class="line">            return item</span><br></pre></td></tr></table></figure>

<h3 id="12-5-3-激活项目管道组件"><a href="#12-5-3-激活项目管道组件" class="headerlink" title="12.5.3 激活项目管道组件"></a>12.5.3 激活项目管道组件</h3><p>要激活Item Pipeline组件，必须将其类添加到 ITEM_PIPELINES设置中，如下例所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &apos;myproject.pipelines.PricePipeline&apos;: 300,</span><br><span class="line">    &apos;myproject.pipelines.JsonWriterPipeline&apos;: 800,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>您在此设置中为类分配的整数值决定了它们运行的顺序：项目从较低值到较高值类进行。习惯上在0-1000范围内定义这些数字。</p>
<h2 id="12-6-下载中间件"><a href="#12-6-下载中间件" class="headerlink" title="12.6 下载中间件"></a>12.6 下载中间件</h2><ul>
<li>middleware.py 文件</li>
</ul>
<p>处理request 和response的组件，处理异常，以及其他相关功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class MyDownMiddleware(object):</span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        请求需要被下载时，经过所有下载器中间件的process_request调用</span><br><span class="line">        :param request: </span><br><span class="line">        :param spider: </span><br><span class="line">        :return:  </span><br><span class="line">            None,继续后续中间件去下载；</span><br><span class="line">            Response对象，停止process_request的执行，开始执行process_response</span><br><span class="line">            Request对象，停止中间件的执行，将Request重新调度器</span><br><span class="line">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def process_response(self, request, response, spider):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        spider处理完成，返回时调用</span><br><span class="line">        :param response:</span><br><span class="line">        :param result:</span><br><span class="line">        :param spider:</span><br><span class="line">        :return: </span><br><span class="line">            Response 对象：转交给其他中间件process_response</span><br><span class="line">            Request 对象：停止中间件，request会被重新调度下载</span><br><span class="line">            raise IgnoreRequest 异常：调用Request.errback</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        print(&apos;response1&apos;)</span><br><span class="line">        return response</span><br><span class="line"></span><br><span class="line">    def process_exception(self, request, exception, spider):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span><br><span class="line">        :param response:</span><br><span class="line">        :param exception:</span><br><span class="line">        :param spider:</span><br><span class="line">        :return: </span><br><span class="line">            None：继续交给后续中间件处理异常；</span><br><span class="line">            Response对象：停止后续process_exception方法</span><br><span class="line">            Request对象：停止中间件，request将会被重新调用下载</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return None</span><br></pre></td></tr></table></figure>

<h2 id="12-7-settings配置"><a href="#12-7-settings配置" class="headerlink" title="12.7 settings配置"></a>12.7 settings配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># Scrapy settings for step8_king project</span><br><span class="line">#</span><br><span class="line"># For simplicity, this file contains only settings considered important or</span><br><span class="line"># commonly used. You can find more settings consulting the documentation:</span><br><span class="line">#</span><br><span class="line">#     http://doc.scrapy.org/en/latest/topics/settings.html</span><br><span class="line">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span><br><span class="line">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span><br><span class="line"></span><br><span class="line"># 1. 爬虫名称</span><br><span class="line">BOT_NAME = &apos;step8_king&apos;</span><br><span class="line"></span><br><span class="line"># 2. 爬虫应用路径</span><br><span class="line">SPIDER_MODULES = [&apos;step8_king.spiders&apos;]</span><br><span class="line">NEWSPIDER_MODULE = &apos;step8_king.spiders&apos;</span><br><span class="line"></span><br><span class="line"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br><span class="line"># 3. 客户端 user-agent请求头</span><br><span class="line"># USER_AGENT = &apos;step8_king (+http://www.yourdomain.com)&apos;</span><br><span class="line"></span><br><span class="line"># Obey robots.txt rules</span><br><span class="line"># 4. 禁止爬虫配置</span><br><span class="line"># ROBOTSTXT_OBEY = False</span><br><span class="line"></span><br><span class="line"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="line"># 5. 并发请求数</span><br><span class="line"># CONCURRENT_REQUESTS = 4</span><br><span class="line"></span><br><span class="line"># Configure a delay for requests for the same website (default: 0)</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span><br><span class="line"># See also autothrottle settings and docs</span><br><span class="line"># 6. 延迟下载秒数</span><br><span class="line"># DOWNLOAD_DELAY = 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The download delay setting will honor only one of:</span><br><span class="line"># 7. 单域名访问并发数，并且延迟下次秒数也应用在每个域名</span><br><span class="line"># CONCURRENT_REQUESTS_PER_DOMAIN = 2</span><br><span class="line"># 单IP访问并发数，如果有值则忽略：CONCURRENT_REQUESTS_PER_DOMAIN，并且延迟下次秒数也应用在每个IP</span><br><span class="line"># CONCURRENT_REQUESTS_PER_IP = 3</span><br><span class="line"></span><br><span class="line"># Disable cookies (enabled by default)</span><br><span class="line"># 8. 是否支持cookie，cookiejar进行操作cookie</span><br><span class="line"># COOKIES_ENABLED = True</span><br><span class="line"># COOKIES_DEBUG = True</span><br><span class="line"></span><br><span class="line"># Disable Telnet Console (enabled by default)</span><br><span class="line"># 9. Telnet用于查看当前爬虫的信息，操作爬虫等...</span><br><span class="line">#    使用telnet ip port ，然后通过命令操作</span><br><span class="line"># TELNETCONSOLE_ENABLED = True</span><br><span class="line"># TELNETCONSOLE_HOST = &apos;127.0.0.1&apos;</span><br><span class="line"># TELNETCONSOLE_PORT = [6023,]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 10. 默认请求头</span><br><span class="line"># Override the default request headers:</span><br><span class="line"># DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">#     &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</span><br><span class="line">#     &apos;Accept-Language&apos;: &apos;en&apos;,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Configure item pipelines</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span><br><span class="line"># 11. 定义pipeline处理请求</span><br><span class="line"># ITEM_PIPELINES = &#123;</span><br><span class="line">#    &apos;step8_king.pipelines.JsonPipeline&apos;: 700,</span><br><span class="line">#    &apos;step8_king.pipelines.FilePipeline&apos;: 500,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 12. 自定义扩展，基于信号进行调用</span><br><span class="line"># Enable or disable extensions</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span><br><span class="line"># EXTENSIONS = &#123;</span><br><span class="line">#     # &apos;step8_king.extensions.MyExtension&apos;: 500,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 13. 爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度</span><br><span class="line"># DEPTH_LIMIT = 3</span><br><span class="line"></span><br><span class="line"># 14. 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span><br><span class="line"></span><br><span class="line"># 后进先出，深度优先</span><br><span class="line"># DEPTH_PRIORITY = 0</span><br><span class="line"># SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleLifoDiskQueue&apos;</span><br><span class="line"># SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.LifoMemoryQueue&apos;</span><br><span class="line"># 先进先出，广度优先</span><br><span class="line"></span><br><span class="line"># DEPTH_PRIORITY = 1</span><br><span class="line"># SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleFifoDiskQueue&apos;</span><br><span class="line"># SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.FifoMemoryQueue&apos;</span><br><span class="line"></span><br><span class="line"># 15. 调度器队列</span><br><span class="line"># SCHEDULER = &apos;scrapy.core.scheduler.Scheduler&apos;</span><br><span class="line"># from scrapy.core.scheduler import Scheduler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 16. 访问URL去重</span><br><span class="line"># DUPEFILTER_CLASS = &apos;step8_king.duplication.RepeatUrl&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="line"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">17. 自动限速算法</span><br><span class="line">    from scrapy.contrib.throttle import AutoThrottle</span><br><span class="line">    自动限速设置</span><br><span class="line">    1. 获取最小延迟 DOWNLOAD_DELAY</span><br><span class="line">    2. 获取最大延迟 AUTOTHROTTLE_MAX_DELAY</span><br><span class="line">    3. 设置初始下载延迟 AUTOTHROTTLE_START_DELAY</span><br><span class="line">    4. 当请求下载完成后，获取其&quot;连接&quot;时间 latency，即：请求连接到接受到响应头之间的时间</span><br><span class="line">    5. 用于计算的... AUTOTHROTTLE_TARGET_CONCURRENCY</span><br><span class="line">    target_delay = latency / self.target_concurrency</span><br><span class="line">    new_delay = (slot.delay + target_delay) / 2.0 # 表示上一次的延迟时间</span><br><span class="line">    new_delay = max(target_delay, new_delay)</span><br><span class="line">    new_delay = min(max(self.mindelay, new_delay), self.maxdelay)</span><br><span class="line">    slot.delay = new_delay</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 开始自动限速</span><br><span class="line"># AUTOTHROTTLE_ENABLED = True</span><br><span class="line"># The initial download delay</span><br><span class="line"># 初始下载延迟</span><br><span class="line"># AUTOTHROTTLE_START_DELAY = 5</span><br><span class="line"># The maximum download delay to be set in case of high latencies</span><br><span class="line"># 最大下载延迟</span><br><span class="line"># AUTOTHROTTLE_MAX_DELAY = 10</span><br><span class="line"># The average number of requests Scrapy should be sending in parallel to each remote server</span><br><span class="line"># 平均每秒并发数</span><br><span class="line"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="line"></span><br><span class="line"># Enable showing throttling stats for every response received:</span><br><span class="line"># 是否显示</span><br><span class="line"># AUTOTHROTTLE_DEBUG = True</span><br><span class="line"></span><br><span class="line"># Enable and configure HTTP caching (disabled by default)</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">18. 启用缓存</span><br><span class="line">    目的用于将已经发送的请求或相应缓存下来，以便以后使用</span><br><span class="line">    </span><br><span class="line">    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span><br><span class="line">    from scrapy.extensions.httpcache import DummyPolicy</span><br><span class="line">    from scrapy.extensions.httpcache import FilesystemCacheStorage</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 是否启用缓存策略</span><br><span class="line"># HTTPCACHE_ENABLED = True</span><br><span class="line"></span><br><span class="line"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span><br><span class="line"># HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.DummyPolicy&quot;</span><br><span class="line"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span><br><span class="line"># HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.RFC2616Policy&quot;</span><br><span class="line"></span><br><span class="line"># 缓存超时时间</span><br><span class="line"># HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="line"></span><br><span class="line"># 缓存保存路径</span><br><span class="line"># HTTPCACHE_DIR = &apos;httpcache&apos;</span><br><span class="line"></span><br><span class="line"># 缓存忽略的Http状态码</span><br><span class="line"># HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="line"></span><br><span class="line"># 缓存存储的插件</span><br><span class="line"># HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">19. 代理，需要在环境变量中设置</span><br><span class="line">    from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware</span><br><span class="line">    </span><br><span class="line">    方式一：使用默认</span><br><span class="line">        os.environ</span><br><span class="line">        &#123;</span><br><span class="line">            http_proxy:http://root:woshiniba@192.168.11.11:9999/</span><br><span class="line">            https_proxy:http://192.168.11.11:9999/</span><br><span class="line">        &#125;</span><br><span class="line">    方式二：使用自定义下载中间件</span><br><span class="line">    </span><br><span class="line">    def to_bytes(text, encoding=None, errors=&apos;strict&apos;):</span><br><span class="line">        if isinstance(text, bytes):</span><br><span class="line">            return text</span><br><span class="line">        if not isinstance(text, six.string_types):</span><br><span class="line">            raise TypeError(&apos;to_bytes must receive a unicode, str or bytes &apos;</span><br><span class="line">                            &apos;object, got %s&apos; % type(text).__name__)</span><br><span class="line">        if encoding is None:</span><br><span class="line">            encoding = &apos;utf-8&apos;</span><br><span class="line">        return text.encode(encoding, errors)</span><br><span class="line">        </span><br><span class="line">    class ProxyMiddleware(object):</span><br><span class="line">        def process_request(self, request, spider):</span><br><span class="line">            PROXIES = [</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;111.11.228.75:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;120.198.243.22:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;111.8.60.9:8123&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;101.71.27.120:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;122.96.59.104:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;122.224.249.122:8088&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">            ]</span><br><span class="line">            proxy = random.choice(PROXIES)</span><br><span class="line">            if proxy[&apos;user_pass&apos;] is not None:</span><br><span class="line">                request.meta[&apos;proxy&apos;] = to_bytes（&quot;http://%s&quot; % proxy[&apos;ip_port&apos;]）</span><br><span class="line">                encoded_user_pass = base64.encodestring(to_bytes(proxy[&apos;user_pass&apos;]))</span><br><span class="line">                request.headers[&apos;Proxy-Authorization&apos;] = to_bytes(&apos;Basic &apos; + encoded_user_pass)</span><br><span class="line">                print &quot;**************ProxyMiddleware have pass************&quot; + proxy[&apos;ip_port&apos;]</span><br><span class="line">            else:</span><br><span class="line">                print &quot;**************ProxyMiddleware no pass************&quot; + proxy[&apos;ip_port&apos;]</span><br><span class="line">                request.meta[&apos;proxy&apos;] = to_bytes(&quot;http://%s&quot; % proxy[&apos;ip_port&apos;])</span><br><span class="line">    </span><br><span class="line">    DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">       &apos;step8_king.middlewares.ProxyMiddleware&apos;: 500,</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">20. Https访问</span><br><span class="line">    Https访问时有两种情况：</span><br><span class="line">    1. 要爬取网站使用的可信任证书(默认支持)</span><br><span class="line">        DOWNLOADER_HTTPCLIENTFACTORY = &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot;</span><br><span class="line">        DOWNLOADER_CLIENTCONTEXTFACTORY = &quot;scrapy.core.downloader.contextfactory.ScrapyClientContextFactory&quot;</span><br><span class="line">        </span><br><span class="line">    2. 要爬取网站使用的自定义证书</span><br><span class="line">        DOWNLOADER_HTTPCLIENTFACTORY = &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot;</span><br><span class="line">        DOWNLOADER_CLIENTCONTEXTFACTORY = &quot;step8_king.https.MySSLFactory&quot;</span><br><span class="line">        </span><br><span class="line">        # https.py</span><br><span class="line">        from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory</span><br><span class="line">        from twisted.internet.ssl import (optionsForClientTLS, CertificateOptions, PrivateCertificate)</span><br><span class="line">        </span><br><span class="line">        class MySSLFactory(ScrapyClientContextFactory):</span><br><span class="line">            def getCertificateOptions(self):</span><br><span class="line">                from OpenSSL import crypto</span><br><span class="line">                v1 = crypto.load_privatekey(crypto.FILETYPE_PEM, open(&apos;/Users/wupeiqi/client.key.unsecure&apos;, mode=&apos;r&apos;).read())</span><br><span class="line">                v2 = crypto.load_certificate(crypto.FILETYPE_PEM, open(&apos;/Users/wupeiqi/client.pem&apos;, mode=&apos;r&apos;).read())</span><br><span class="line">                return CertificateOptions(</span><br><span class="line">                    privateKey=v1,  # pKey对象</span><br><span class="line">                    certificate=v2,  # X509对象</span><br><span class="line">                    verify=False,</span><br><span class="line">                    method=getattr(self, &apos;method&apos;, getattr(self, &apos;_ssl_method&apos;, None))</span><br><span class="line">                )</span><br><span class="line">    其他：</span><br><span class="line">        相关类</span><br><span class="line">            scrapy.core.downloader.handlers.http.HttpDownloadHandler</span><br><span class="line">            scrapy.core.downloader.webclient.ScrapyHTTPClientFactory</span><br><span class="line">            scrapy.core.downloader.contextfactory.ScrapyClientContextFactory</span><br><span class="line">        相关配置</span><br><span class="line">            DOWNLOADER_HTTPCLIENTFACTORY</span><br><span class="line">            DOWNLOADER_CLIENTCONTEXTFACTORY</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">21. 爬虫中间件</span><br><span class="line">    class SpiderMiddleware(object):</span><br><span class="line"></span><br><span class="line">        def process_spider_input(self,response, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            下载完成，执行，然后交给parse处理</span><br><span class="line">            :param response: </span><br><span class="line">            :param spider: </span><br><span class="line">            :return: </span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            pass</span><br><span class="line">    </span><br><span class="line">        def process_spider_output(self,response, result, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            spider处理完成，返回时调用</span><br><span class="line">            :param response:</span><br><span class="line">            :param result:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return result</span><br><span class="line">    </span><br><span class="line">        def process_spider_exception(self,response, exception, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            异常调用</span><br><span class="line">            :param response:</span><br><span class="line">            :param exception:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return None</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">        def process_start_requests(self,start_requests, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            爬虫启动时调用</span><br><span class="line">            :param start_requests:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return: 包含 Request 对象的可迭代对象</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return start_requests</span><br><span class="line">    </span><br><span class="line">    内置爬虫中间件：</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware&apos;: 50,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&apos;: 500,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.referer.RefererMiddleware&apos;: 700,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware&apos;: 800,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.depth.DepthMiddleware&apos;: 900,</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># from scrapy.contrib.spidermiddleware.referer import RefererMiddleware</span><br><span class="line"># Enable or disable spider middlewares</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">   # &apos;step8_king.middlewares.SpiderMiddleware&apos;: 543,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">22. 下载中间件</span><br><span class="line">    class DownMiddleware1(object):</span><br><span class="line">        def process_request(self, request, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            请求需要被下载时，经过所有下载器中间件的process_request调用</span><br><span class="line">            :param request:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return:</span><br><span class="line">                None,继续后续中间件去下载；</span><br><span class="line">                Response对象，停止process_request的执行，开始执行process_response</span><br><span class="line">                Request对象，停止中间件的执行，将Request重新调度器</span><br><span class="line">                raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            pass</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">        def process_response(self, request, response, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            spider处理完成，返回时调用</span><br><span class="line">            :param response:</span><br><span class="line">            :param result:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return:</span><br><span class="line">                Response 对象：转交给其他中间件process_response</span><br><span class="line">                Request 对象：停止中间件，request会被重新调度下载</span><br><span class="line">                raise IgnoreRequest 异常：调用Request.errback</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            print(&apos;response1&apos;)</span><br><span class="line">            return response</span><br><span class="line">    </span><br><span class="line">        def process_exception(self, request, exception, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span><br><span class="line">            :param response:</span><br><span class="line">            :param exception:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return:</span><br><span class="line">                None：继续交给后续中间件处理异常；</span><br><span class="line">                Response对象：停止后续process_exception方法</span><br><span class="line">                Request对象：停止中间件，request将会被重新调用下载</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return None</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    默认下载中间件</span><br><span class="line">    &#123;</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware&apos;: 100,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware&apos;: 300,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware&apos;: 350,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&apos;: 400,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.retry.RetryMiddleware&apos;: 500,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware&apos;: 550,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware&apos;: 580,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware&apos;: 590,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware&apos;: 600,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware&apos;: 700,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware&apos;: 750,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware&apos;: 830,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.stats.DownloaderStats&apos;: 850,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware&apos;: 900,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware</span><br><span class="line"># Enable or disable downloader middlewares</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span><br><span class="line"># DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">#    &apos;step8_king.middlewares.DownMiddleware1&apos;: 100,</span><br><span class="line">#    &apos;step8_king.middlewares.DownMiddleware2&apos;: 500,</span><br><span class="line"># &#125;</span><br></pre></td></tr></table></figure>

<p>八 项目代码<br><a href="https://files.cnblogs.com/files/yuanchenqi/Amazon.zip" target="_blank" rel="noopener">下载项目代码</a></p>
</div><div class="post-copyright"><blockquote><p>原文作者: 吴泰</p><p>原文链接: <a href="http://yoursite.com/2019/10/27/12-scrapy-框架基础知识/">http://yoursite.com/2019/10/27/12-scrapy-框架基础知识/</a></p><p>版权声明: 转载请注明出处(必须保留原文作者署名原文链接)</p></blockquote></div><div class="tags"><a href="/tags/爬虫/">爬虫</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/2019/10/27/13-Scrapy-爬取知乎用户信息/" class="pre">13 Scrapy 爬取知乎用户信息</a><a href="/2019/10/27/实战1-Requests-正则表达式抓取猫眼TOP100/" class="next">10 实战1 Requests+正则表达式抓取猫眼TOP100</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-scrapy框架简介"><span class="toc-text">12.1 scrapy框架简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-1-介绍"><span class="toc-text">12.1.1 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-2-安装"><span class="toc-text">12.1.2 安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-3-命令行工具"><span class="toc-text">12.1.3 命令行工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-4-创建、启动项目"><span class="toc-text">12.1.4 创建、启动项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-5-目录结构"><span class="toc-text">12.1.5 目录结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-Spider类"><span class="toc-text">12.2 Spider类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-选择器"><span class="toc-text">12.3 选择器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#进入shell"><span class="toc-text">进入shell</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#初步使用"><span class="toc-text">初步使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#xpath和css结合使用"><span class="toc-text">xpath和css结合使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#提取SelectorList-的data数据，转为文本"><span class="toc-text">提取SelectorList 的data数据，转为文本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#提取属性"><span class="toc-text">提取属性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#获取标签文本"><span class="toc-text">获取标签文本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#标签属性包含匹配"><span class="toc-text">标签属性包含匹配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用正则提取字符串"><span class="toc-text">使用正则提取字符串</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-Item-项目"><span class="toc-text">12.4 Item(项目)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-1-声明项目"><span class="toc-text">12.4.1 声明项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-2-项目字段"><span class="toc-text">12.4.2 项目字段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-3-使用项目"><span class="toc-text">12.4.3 使用项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-4-扩展项目"><span class="toc-text">12.4.4 扩展项目</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-Item-PipeLine"><span class="toc-text">12.5 Item PipeLine</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-1-编写自己的项目管道"><span class="toc-text">12.5.1 编写自己的项目管道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-2-项目管道示例"><span class="toc-text">12.5.2 项目管道示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-价格验证和丢弃物品没有价格"><span class="toc-text">(1) 价格验证和丢弃物品没有价格</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-将项目写入JSON文件"><span class="toc-text">(2) 将项目写入JSON文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-将项目写入数据库"><span class="toc-text">(3) 将项目写入数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-重复过滤"><span class="toc-text">(4) 重复过滤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-3-激活项目管道组件"><span class="toc-text">12.5.3 激活项目管道组件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-6-下载中间件"><span class="toc-text">12.6 下载中间件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-7-settings配置"><span class="toc-text">12.7 settings配置</span></a></li></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/11-正则解析html/">11 正则解析html</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/13-Scrapy-爬取知乎用户信息/">13 Scrapy 爬取知乎用户信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/12-scrapy-框架基础知识/"> 12 scrapy 框架基础知识</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/实战1-Requests-正则表达式抓取猫眼TOP100/">10 实战1 Requests+正则表达式抓取猫眼TOP100</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/09-selinux-模块/">09  selinux 模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/08 解析模块之Xpath 模块/">08 解析模块之Xpath 模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/07 PyQuery  模块的使用/">07 PyQuery  模块的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/06 BeautifulSoup 模块的使用/">06 BeautifulSoup 模块的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/05-基础模块之Request模块和Response响应/">05 基础模块之Request模块和Response响应</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/04-基础模块之-Urllib/">04 基础模块之 Urllib</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">13</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">吴泰.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/javascript" id="maid-script" src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js?v=2.0.4"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>